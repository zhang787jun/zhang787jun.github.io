<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <title>基于SQuAD数据训练对话系统--基于项目的BERT原理及实践 - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
</head>

<body>

    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="/Wiki/">Home</a>&nbsp;»&nbsp;<a href="/Wiki/#Data_Science">Data_Science</a>&nbsp;»&nbsp;<a href="/Wiki/#-Algorithm">Algorithm</a>&nbsp;»&nbsp;<a href="/Wiki/#-自然语言处理">自然语言处理</a>&nbsp;»&nbsp;<a href="/Wiki/#-语言模型及特征抽取">语言模型及特征抽取</a>&nbsp;»&nbsp;基于SQuAD数据训练对话系统--基于项目的BERT原理及实践</div>
</div>
<div class="clearfix"></div>
<div id="title">基于SQuAD数据训练对话系统--基于项目的BERT原理及实践</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">问答系统</a></li>
<li><a href="#squad">SQuAD数据集合</a></li>
<li><a href="#bert">什么是BERT</a></li>
<li><a href="#bert_1">预训练的BERT</a></li>
<li><a href="#bert-base-cased-12">BERT-Base, Cased 12层 解析</a></li>
<li><a href="#_2">参考资料</a></li>
</ul>
</div>
<h1 id="_1">问答系统</h1>
<h1 id="squad">SQuAD数据集合</h1>
<p>SQuAD（Stanford Question Answering Dataset）<a href="https://rajpurkar.github.io/SQuAD-explorer/">(官网)</a>是斯坦福大学提供的问答数据集，包含 10 万个（问题，原文，答案）三元组，原文来自于 536 篇维基百科文章，而问题和答案的构建主要是通过众包的方式，让标注人员提出最多 5 个基于文章内容的问题并提供正确答案，且答案出现在原文中。</p>
<p>SQuAD_train_v2.0.json 结构</p>
<p><img alt="" src="../../../../attach/images/2019-09-28-18-33-47.png" /></p>
<h1 id="bert">什么是BERT</h1>
<p>BERT, or Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.</p>
<p>BERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder，因为decoder是不能获要预测的信息的。模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。</p>
<p><img alt="" src="../../../../attach/images/2019-09-25-17-08-43.png" /></p>
<h1 id="bert_1">预训练的BERT</h1>
<p>由于从头开始(from scratch)训练需要巨大的计算资源，因此Google提供了预训练的模型(的checkpoint)，目前包括英语、汉语和多语言3类模型，而英语又包括4个版本：</p>
<p>BERT-Base, Uncased 12层，768个隐单元，12个Attention head，110M参数<br />
BERT-Large, Uncased 24层，1024个隐单元，16个head，340M参数<br />
BERT-Base, Cased 12层，768个隐单元，12个Attention head，110M参数<br />
BERT-Large, Uncased 24层，1024个隐单元，16个head，340M参数。<br />
Uncased的意思是保留大小写，而cased是在预处理的时候都变成了小写。</p>
<p>对于汉语只有一个版本：<br />
BERT-Base, Chinese: 包括简体和繁体汉字，共12层，768个隐单元，12个Attention head，110M参数。</p>
<p>另外一个多语言的版本是BERT-Base, Multilingual Cased (New, recommended)，它包括104种不同语言，12层，768个隐单元，12个Attention head，110M参数。它是用所有这104中语言的维基百科文章混在一起训练出来的模型。所有这些模型的下载地址都在这里。</p>
<p>这么多版本我们应该选择哪一个呢？如果我们处理的问题只包含英文，那么我们应该选择英语的版本(模型大效果好但是参数多训练慢而且需要更多内存/显存)。如果我们只处理中文，那么应该使用中文的版本。如果是其他语言就使用多语言的版本。</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup></p>
<p>对下载的压缩文件进行解压，可以看到文件里有五个文件，其中bert_model.ckpt开头的文件是负责模型变量载入的，而vocab.txt是训练时中文文本采用的字典，最后bert_config.json是BERT在训练时，可选调整的一些参数。</p>
<p>Transformer是谷歌在17年做机器翻译任务的“Attention is all you need”的论文中提出的，引起了相当大的反响。 每一位从事NLP研发的同仁都应该透彻搞明白Transformer，它的重要性毫无疑问，尤其是你在看完我这篇文章之后，我相信你的紧迫感会更迫切，我就是这么一位善于制造焦虑的能手。不过这里没打算重点介绍它，想要入门Transformer的可以参考以下三篇文章：一个是Jay Alammar可视化地介绍Transformer的博客文章The Illustrated Transformer ，非常容易理解整个机制，建议先从这篇看起， 这是中文翻译版本；第二篇是 Calvo的博客：Dissecting BERT Part 1: The Encoder ，尽管说是解析Bert，但是因为Bert的Encoder就是Transformer，所以其实它是在解析Transformer，里面举的例子很好；再然后可以进阶一下，参考哈佛大学NLP研究组写的“The Annotated Transformer. ”，代码原理双管齐下，讲得也很清楚。</p>
<h1 id="bert-base-cased-12">BERT-Base, Cased 12层 解析</h1>
<p>{<br />
"attention_probs_dropout_prob": 0.1, #乘法attention时，softmax后dropout概率 <br />
"hidden_act": "gelu", #激活函数 <br />
"hidden_dropout_prob": 0.1, #隐藏层dropout概率 <br />
"hidden_size": 768, #隐藏单元数 <br />
"initializer_range": 0.02, #初始化范围 <br />
"intermediate_size": 3072, #升维维度<br />
"max_position_embeddings": 512,#一个大于seq_length的参数，用于生成position_embedding "num_attention_heads": 12, #每个隐藏层中的attention head数 <br />
"num_hidden_layers": 12, #隐藏层数 <br />
"type_vocab_size": 2, #segment_ids类别 [0,1] <br />
"vocab_size": 30522 #词典中词数<br />
}</p>
<p>config,                            # BertConfig对象<br />
 3               is_training,                      <br />
 4               input_ids,                        # 【batch_size, seq_length】<br />
 5               input_mask=None,                    # 【batch_size, seq_length】<br />
 6               token_type_ids=None,                # 【batch_size, seq_length】<br />
 7               use_one_hot_embeddings=False,    # 是否使用one-hot；否则tf.gather()</p>
<p>主体结构<br />
<img alt="" src="../../../../attach/images/2019-09-26-11-14-34.png" /><br />
12层tansorfmore<br />
<img alt="" src="../../../../attach/images/2019-09-26-11-16-30.png" /></p>
<p>每一层里面的结构 <br />
<img alt="" src="../../../../attach/images/2019-09-26-11-18-19.png" /></p>
<h1 id="_2">参考资料</h1>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>BERT代码阅读 http://fancyerii.github.io/2019/03/09/bert-codes/#%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E6%A8%A1%E5%9E%8B&#160;<a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
</div>
<div id="renote">
  <HR style=" FILTER: alpha (opacity = 100, finishopacity =0 , style= 3 )" width="80%" color=#987 cb 9 SIZE=3>
  <p>如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多!</p>
  <img src="/Wiki/static/images/pay.jpg" width="25%">
</div>

    </div>
    <div id="footer">
        <span>
            Copyright © 2021 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>

    
</body>
<script>
    function changeImgurl(site_root_url) {
        var images = document.images;
        var site_root = site_root_url;
        for (i = 0, len = images.length; i < len; i++) {
            image = images[i];
            image_src = image.src;
            if (image_src.search("attach") >= 0) {
                re_image_src = image_src.slice(image_src.search("attach"));
                abs_image_src = (site_root.endsWith("/")) ? site_root + re_image_src : site_root + "/" +
                    re_image_src;
                image.src = abs_image_src;
            }
        }
    }
    var site_root_url = "/Wiki";
    changeImgurl(site_root_url);
    let isMathjaxConfig = false; // 防止重复调用Config，造成性能损耗
    const initMathjaxConfig = () => {
        if (!window.MathJax) {
            return;
        }
        window.MathJax.Hub.Config({
            showProcessingMessages: false, //关闭js加载过程信息
            messageStyle: "none", //不显示信息
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
                displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
                skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
            },
            "HTML-CSS": {
                availableFonts: ["STIX", "TeX"], //可选字体
                showMathMenu: false //关闭右击菜单显示
            }
        });
        isMathjaxConfig = true; //
    };
    if (isMathjaxConfig === false) {
        // 如果：没有配置MathJax
        initMathjaxConfig();
    };
</script>

</html>