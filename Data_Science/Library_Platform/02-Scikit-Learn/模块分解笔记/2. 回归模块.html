<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <title>2.回归模块 - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
</head>

<body>

    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="/Wiki/">Home</a>&nbsp;»&nbsp;<a href="/Wiki/#Data_Science">Data_Science</a>&nbsp;»&nbsp;<a href="/Wiki/#-Library_Platform">Library_Platform</a>&nbsp;»&nbsp;<a href="/Wiki/#-02-Scikit-Learn">02-Scikit-Learn</a>&nbsp;»&nbsp;<a href="/Wiki/#-模块分解笔记">模块分解笔记</a>&nbsp;»&nbsp;2.回归模块</div>
</div>
<div class="clearfix"></div>
<div id="title">2.回归模块</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#1-svm-svr">1. 基于支持向量机(SVM)的回归-SVR</a><ul>
<li><a href="#11-svr">1.1. SVR</a></li>
<li><a href="#12-linearsvr">1.2. LinearSVR</a><ul>
<li><a href="#l2">内核岭回归L2</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#2">2. 线性回归</a><ul>
<li><a href="#21">2.1. 普通线性回归</a></li>
<li><a href="#22-l2">2.2. 岭回归 L2</a></li>
<li><a href="#24-lasso-l1">2.4. LASSO回归 L1</a></li>
<li><a href="#25-elasticnet-l1l2">2.5. ElasticNet 弹性网回归 L1+L2</a></li>
</ul>
</li>
</ul>
</div>
<h1 id="1-svm-svr">1. 基于支持向量机(SVM)的回归-SVR</h1>
<p>Support Vector Regression(SVR)<br />
已知：<br />
$$y = wx + b$$<br />
求 W,b -- 使得回归的线性函数对两侧的支持向量有最小间距</p>
<p><img alt="" src="/attach/images/2020-03-21-23-27-27.png" /></p>
<h2 id="11-svr">1.1. SVR</h2>
<div class="hlcode"><pre><span class="n">svr</span><span class="o">=</span><span class="n">SVR</span><span class="p">(</span>
    <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
    <span class="n">cache_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    <span class="n">coef0</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> 
    <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
    <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">gamma</span><span class="o">=</span><span class="sb">`auto_deprecated`</span><span class="p">,</span> 
    <span class="n">kernel</span><span class="o">=</span><span class="sb">`rbf`</span><span class="p">,</span>
    <span class="n">max_iter</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> 
    <span class="n">shrinking</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">tol</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> 
    <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span>
    <span class="p">)</span>
</pre></div>


<p><strong>参数说明</strong> <br />
-<code>epsilon</code> 距离误差</p>
<h2 id="12-linearsvr">1.2. LinearSVR</h2>
<div class="hlcode"><pre><span class="n">LinearSVR</span><span class="p">(</span>
    <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
    <span class="n">dual</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
    <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> 
    <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">intercept_scaling</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="sb">`epsilon_insensitive`</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
    <span class="n">tol</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> 
    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>


<h3 id="l2">内核岭回归L2</h3>
<div class="hlcode"><pre><span class="n">sklearn</span><span class="o">.</span><span class="n">kernel_ridge</span><span class="o">.</span><span class="n">KernelRidge</span><span class="p">(</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
    <span class="n">kernel</span><span class="o">=</span><span class="sb">`linear`</span><span class="p">,</span> 
    <span class="n">gamma</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
    <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
    <span class="n">coef0</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
    <span class="n">kernel_params</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>


<p><strong>参数解释</strong><br />
1. alpha:<br />
2. kernel : string or callable, default=”linear”<br />
    Kernel mapping used internally. A callable should accept two arguments and the keyword arguments passed to this object as kernel_params, and should return a floating point number.<br />
3. gamma : float, default=None<br />
   - rbf、laplacian、多项式、指数chi2和sigmoid核的gamma参数。<br />
4. degree : float, default=3<br />
   - 只针对polynomial kernel核有效，对于其他核忽略<br />
5. coef0 : float, default=1<br />
    Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels.</p>
<p>内核岭回归（KRR）将岭回归（具有L2-范数正则化的线性最小二乘）与<code>核技巧</code>结合在一起。</p>
<p>KRR学习的模型的形式与支持向量回归（SVR）相同。但是，使用了不同的损失函数：KRR使用平方误差损失，而支持向量回归使用epsilon不敏感损失，两者均与12正则化结合。</p>
<p>与SVR相比，KRR模型的拟合可以封闭形式进行，对于中等规模的数据集通常更快。另一方面，学习的模型是非稀疏的，因此比SVR慢，后者在预测时学习epsilon&gt; 0的稀疏模型。</p>
<h1 id="2">2. 线性回归</h1>
<p><img alt="" src="/attach/images/2020-03-21-23-32-22.png" /><br />
对于:<br />
$$y^{hat}=wX+b$$<br />
求W，b, Loss function = MSE<br />
$$min L(y_i,y^{hat})=L(y_i,Wx+b)$$</p>
<h2 id="21">2.1. 普通线性回归</h2>
<p><code>linear_model.LinearRegression</code> 使用普通小二乘法<strong>OLS</strong>的线性回归</p>
<p>$$ Loss=(y - wX)^2 $$</p>
<p>$$ w=(X^TX)^{-1}X^Ty $$</p>
<p>权重w只与输入数据(x,y)相关，要求 X 不为全0。</p>
<div class="hlcode"><pre><span class="n">sklearn</span><span class="o">.</span><span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">copy_X</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="c"># fit_intercept 拟合截距</span>
<span class="c"># copy_X 在X.copy上操作</span>
</pre></div>


<p><strong>参数说明</strong></p>
<ul>
<li><code>fit_intercept</code>[defualt=True] 拟合截距</li>
<li><code>copy_X</code>[defualt=True]在X.copy上操作</li>
</ul>
<h2 id="22-l2">2.2. 岭回归 L2</h2>
<p>$$ Loss=(y - Xw)^2 + alpha \times w^2$$</p>
<div class="hlcode"><pre><span class="n">sklearn</span><span class="o">.</span><span class="n">linear_model</span><span class="o">.</span><span class="n">Ridge</span><span class="p">(</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
    <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
    <span class="n">copy_X</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
    <span class="n">max_iter</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
    <span class="n">tol</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> 
    <span class="n">solver</span><span class="o">=</span><span class="sb">`auto`</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>


<p><strong>参数解释</strong><br />
- <code>solver</code> [defualt=<code>"auto"</code>] <br />
   - <code>auto</code>:根据数据类型自动选择求解器<br />
   - <code>svd</code> :奇异值分解SVD Singular Value Decomposition<br />
   - <code>cholesky</code>: 使用标准的 <code>scipy.linalg.solve</code> 函数获得一个封闭解<br />
   - <code>sparse_cg</code>: 使用<code>scipy.sparse.linalg.cg</code>中的共轭梯度解算器作为一种迭代算法，这个解算器比cholesky更适合于大规模数据（设置tol和max_iter的可能性）。<br />
   -  <code>lsqr</code>:  使用 <code>scipy.sparse.linalg.lsqr</code> 求解，最快的迭代方法<br />
   -  <code>sag</code>: 使用随机平均梯度下降<br />
   -  <code>saga</code>: 使用其无偏版本随机平均梯度下降<br />
- <code>max_iter</code> 共轭梯度求解器的最大迭代次数 <br />
   - <code>sparse_cg</code>和<code>lsqr</code>求解器，默认值由scipy.sparse.linalg确定<br />
   - <code>sag</code>求解器，默认值为1000</p>
<p>linear_model.Ridge 岭回归，一种将L2作为正则化工具的线性小二乘回归<br />
linear_model.RidgeCV 带交叉验证的岭回归<br />
linear_model.RidgeClassiﬁer 岭回归的分类器<br />
linear_model.RidgeClassiﬁerCV 带交叉验证的岭回归的分类器<br />
linear_model.ridge_regression 【函数】用正太方程法求解岭回归</p>
<h2 id="24-lasso-l1">2.4. LASSO回归 L1</h2>
<p>$$Loss=\frac{1}{2 * n}  * (y - Xw)^2 + alpha * |w|$$<br />
在sklearn中，lasso回归使用的就是坐标下降法</p>
<div class="hlcode"><pre><span class="n">sklearn</span><span class="o">.</span><span class="n">linear_model</span><span class="o">.</span><span class="n">Lasso</span><span class="p">(</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
    <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
    <span class="n">precompute</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">copy_X</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
    <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">tol</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> 
    <span class="n">warm_start</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
    <span class="n">positive</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
    <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
    <span class="n">selection</span><span class="o">=</span><span class="sb">`cyclic`</span><span class="p">)</span>
</pre></div>


<p><strong>参数解释</strong><br />
1. precompute: <br />
   - 是否使用预先计算的 Gram 矩阵来加速计算。<br />
2. tol<br />
   - 数据解算精度。<br />
3. positive<br />
   - positive: 强制系数为正值。</p>
<ol>
<li>selection[defualt=<code>cyclic</code>]: <ul>
<li>cyclic: 循环每次迭代使用原来的seed</li>
<li>random: 每次迭代都会更新一个seed。设置random 通常会使得收敛更加迅速，特别是设置tol&gt;1e-4。</li>
</ul>
</li>
</ol>
<p>linear_model.Lasso Lasso，使用L1作为正则化工具来训练的线性回归模型<br />
linear_model.LassoCV 带交叉验证和正则化迭代路径的Lasso<br />
linear_model.LassoLars 使用小角度回归求解的Lasso<br />
linear_model.LassoLarsCV 带交叉验证的使用小角度回归求解的Lasso<br />
linear_model.LassoLarsIC<br />
使用BIC或AIC进行模型选择的，使用小角度回归求解的 Lasso<br />
linear_model.MultiTaskLasso 使用L1 / L2混合范数作为正则化工具训练的多标签Lasso<br />
linear_model.MultiTaskLassoCV<br />
使用L1 / L2混合范数作为正则化工具训练的，带交叉验证 的多标签Lasso<br />
linear_model.lasso_path 【函数】用坐标下降计算Lasso路径<br />
</p>
<h2 id="25-elasticnet-l1l2">2.5. ElasticNet 弹性网回归 L1+L2</h2>
<p>β=(λI+K)−1yβ=(λI+K)−1y</p>
<p>$$ Loss=\frac{1}{2n}(y-Xw)^2+alpha \times l1 \times ratio<em>|w|+ \frac{alpha}{2}(1 - l1 \times ratio) </em>w^2$$</p>
<div class="hlcode"><pre><span class="n">sklearn</span><span class="o">.</span><span class="n">linear_model</span><span class="o">.</span><span class="n">ElasticNet</span><span class="p">(</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
    <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> 
    <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
    <span class="n">normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
    <span class="n">precompute</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> 
    <span class="n">copy_X</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
    <span class="n">tol</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> 
    <span class="n">warm_start</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
    <span class="n">positive</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
    <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
    <span class="n">selection</span><span class="o">=</span><span class="sb">`cyclic`</span><span class="p">)</span>
</pre></div>


<p>linear_model.ElasticNet 弹性网，一种将L1和L2组合作为正则化工具的线性回归<br />
linear_model.ElasticNetCV 带交叉验证和正则化迭代路径的弹性网<br />
linear_model.MultiTaskElasticNet 多标签弹性网<br />
linear_model.MultiTaskElasticNetCV 带交叉验证的多标签弹性网<br />
linear_model.enet_path 【函数】用坐标下降法计算弹性网的路径</p>
</div>
<div id="renote">
  <HR style=" FILTER: alpha (opacity = 100, finishopacity =0 , style= 3 )" width="80%" color=#987 cb 9 SIZE=3>
  <p>如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多!</p>
  <img src="/Wiki/static/images/pay.jpg" width="25%">
</div>

    </div>
    <div id="footer">
        <span>
            Copyright © 2021 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>

    
</body>
<script>
    function changeImgurl(site_root_url) {
        var images = document.images;
        var site_root = site_root_url;
        for (i = 0, len = images.length; i < len; i++) {
            image = images[i];
            image_src = image.src;
            if (image_src.search("attach") >= 0) {
                re_image_src = image_src.slice(image_src.search("attach"));
                abs_image_src = (site_root.endsWith("/")) ? site_root + re_image_src : site_root + "/" +
                    re_image_src;
                image.src = abs_image_src;
            }
        }
    }
    var site_root_url = "/Wiki";
    changeImgurl(site_root_url);
    let isMathjaxConfig = false; // 防止重复调用Config，造成性能损耗
    const initMathjaxConfig = () => {
        if (!window.MathJax) {
            return;
        }
        window.MathJax.Hub.Config({
            showProcessingMessages: false, //关闭js加载过程信息
            messageStyle: "none", //不显示信息
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
                displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
                skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
            },
            "HTML-CSS": {
                availableFonts: ["STIX", "TeX"], //可选字体
                showMathMenu: false //关闭右击菜单显示
            }
        });
        isMathjaxConfig = true; //
    };
    if (isMathjaxConfig === false) {
        // 如果：没有配置MathJax
        initMathjaxConfig();
    };
</script>

</html>