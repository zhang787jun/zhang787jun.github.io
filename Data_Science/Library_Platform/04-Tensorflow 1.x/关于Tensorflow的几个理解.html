<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <title>[汇总] Tensorflow 的几个模块理解(core) - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
</head>

<body>

    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="/Wiki/">Home</a>&nbsp;»&nbsp;<a href="/Wiki/#Data_Science">Data_Science</a>&nbsp;»&nbsp;<a href="/Wiki/#-Library_Platform">Library_Platform</a>&nbsp;»&nbsp;<a href="/Wiki/#-04-Tensorflow 1.x">04-Tensorflow 1.x</a>&nbsp;»&nbsp;[汇总] Tensorflow 的几个模块理解(core)</div>
</div>
<div class="clearfix"></div>
<div id="title">[汇总] Tensorflow 的几个模块理解(core)</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#1tftrain">1.训练的管理（tf.train）</a><ul>
<li><a href="#1-tftrainsupervisor">1. tf.train.Supervisor</a></li>
<li><a href="#2-tftrainmonitorsession">2. tf.train.MonitorSession()</a><ul>
<li><a href="#1-monitoredsession-init">1. MonitoredSession init 初始化</a></li>
<li><a href="#2-monitoredsession-run">2. MonitoredSession run 运行</a></li>
<li><a href="#3-monitoredsession-close">3.  MonitoredSession close 关闭</a></li>
<li><a href="#4-1-global-step">4. 关键点1： global step</a></li>
<li><a href="#5-2-tftrainscaffold">5. 关键点2： tf.train.Scaffold</a></li>
<li><a href="#6-tftrainmonitoredtrainingsession">6. 常用子类： tf.train.MonitoredTrainingSession()</a></li>
<li><a href="#tensorflow-hook">tensorflow hook架构</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#2">2. 设备的管理</a><ul>
<li><a href="#21-tfdevice">2.1 tf.device</a></li>
<li><a href="#22">2.2 当设备为分布式设备时候</a></li>
<li><a href="#23">2.3 查看可用设备</a></li>
<li><a href="#41-tfrecord">4.1 TFRecord 格式</a><ul>
<li><a href="#411-tfrecord">4.1.1  TFRecord 是什么？</a></li>
<li><a href="#412-tfrecordqueuerunner">4.1.2  为什么要使用TFRecord和Queuerunner?</a></li>
<li><a href="#413-tfrecord-example">4.1.3 TFRecord 核心--Example</a><ul>
<li><a href="#1">1. 参数说明</a></li>
<li><a href="#2_1">2. 方法说明</a></li>
</ul>
</li>
<li><a href="#414-tfrecord">4.1.4 存储为 TFrecord格式文件</a></li>
<li><a href="#415-tfrecordtensorflow">4.1.5  TFrecord格式文件在tensorflow图中输出</a><ul>
<li><a href="#1_1">1. 导入</a></li>
<li><a href="#2_2">2. 序列化样本解析</a><ul>
<li><a href="#21-parse_function">2.1 parse_function 解析函数</a></li>
</ul>
</li>
<li><a href="#3">3. 创建迭代器</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#42-protocol-buffer">4.2. Protocol Buffer</a></li>
<li><a href="#_1">其他</a></li>
<li><a href="#parse_example">parse_example</a></li>
</ul>
</li>
<li><a href="#5">5.持久化（保存和恢复）</a><ul>
<li><a href="#1_2">1. 几个概念</a><ul>
<li><a href="#_2">序列化</a></li>
<li><a href="#_3">反序列化</a></li>
<li><a href="#protobuf">protobuf</a></li>
</ul>
</li>
<li><a href="#2_3">2. 需要保存什么</a><ul>
<li><a href="#1_3">1. 图信息</a><ul>
<li><a href="#1-node-operation">1. Node节点信息--operation</a></li>
<li><a href="#2-tensor">2. 边缘信息--Tensor</a></li>
</ul>
</li>
<li><a href="#2_4">2. 参数信息</a></li>
<li><a href="#3_1">3. 其他信息</a><ul>
<li><a href="#12-metagraph">1.2 MetaGraph 信息</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#3_2">3. 怎么保存</a><ul>
<li><a href="#1-metadata">1. 常规保存 Meta+data</a><ul>
<li><a href="#1-metagraphdef">1. MetaGraphDef 保存图信息和其他信息</a></li>
<li><a href="#2_5">2.参数信息的保存</a></li>
<li><a href="#_4">实践</a><ul>
<li><a href="#tfsaver">使用tf.saver</a></li>
<li><a href="#pd">.pd 格式</a></li>
<li><a href="#pd-ckpt">pd 格式/ckpt格式文件互换</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#2-savedmodel">2. 以 SavedModel形式保存上线模型</a></li>
<li><a href="#3_3">3. 保存部署在移动设备上的轻量级模型</a></li>
<li><a href="#_5">实践</a></li>
</ul>
</li>
<li><a href="#4">4. 怎么加载</a><ul>
<li><a href="#2-pd">2 pd 文件加载</a><ul>
<li><a href="#1_4">1.保存</a></li>
<li><a href="#2_6">2.加载</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#6-tfsummary">6. 可视化（tf.summary）</a><ul>
<li><a href="#1_5">1. 得到需要可视化的数据</a><ul>
<li><a href="#11">1.1 标量</a></li>
<li><a href="#12">1.2 图片</a></li>
<li><a href="#13">1.3 音频</a></li>
<li><a href="#14">1.4 文本</a></li>
<li><a href="#15">1.5 直方图数据</a></li>
<li><a href="#16">1.6 分布图数据</a></li>
<li><a href="#17-">1.7 快捷操作--所有可视化数据</a></li>
</ul>
</li>
<li><a href="#2_7">2  将输出的数据都保存到本地磁盘中</a></li>
</ul>
</li>
<li><a href="#7-tfmetrics">7. 评价模型的优劣（tf.metrics）</a><ul>
<li><a href="#71-tfmetrics">7.1 tf.metrics 类</a><ul>
<li><a href="#1tfmetrics">1.tf.metrics 类的通用特性</a></li>
<li><a href="#2-tfmetrics">2. tf.metrics 的执行过程</a></li>
<li><a href="#3-tfmetrics-tfloss">3. tf.metrics 与 tf.loss 的区别</a></li>
</ul>
</li>
<li><a href="#72">7.2 如何评价一个模型</a><ul>
<li><a href="#1-error-">1 误差 error--离散度的测量</a><ul>
<li><a href="#1-mae">1. 平均绝对误差 MAE</a></li>
<li><a href="#2-mre">2. 平均相对误差 MRE</a></li>
<li><a href="#3-mse">3. 平方差（方差）MSE</a></li>
<li><a href="#4-rmse">4. 均方差(标准差) RMSE</a></li>
<li><a href="#5_1">5. 余弦相似性</a></li>
</ul>
</li>
<li><a href="#2_8">2 混合矩阵</a><ul>
<li><a href="#1-accuracy">1. 准确率 (accuracy)</a></li>
<li><a href="#2-precision">2. 精确率 (precision)</a></li>
<li><a href="#3-recall">3. 召回率 (recall)</a></li>
<li><a href="#4-auc">4. AUC</a></li>
<li><a href="#5-sensitivity">5. 敏感度(sensitivity)</a></li>
<li><a href="#6-specificity">6. 特异度(specificity)</a></li>
<li><a href="#7">7. 各种数量</a></li>
</ul>
</li>
<li><a href="#3_4">3 基本统计量</a><ul>
<li><a href="#1_6">1. 平均数</a></li>
<li><a href="#2_9">2. 百分百</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#8-scope">8. 代码的可读性--作用域(scope)</a><ul>
<li><a href="#tfname_scope">tf.name_scope</a></li>
<li><a href="#tensorop">Tensor/op 改名</a></li>
</ul>
</li>
<li><a href="#9-tflosses">9. 损失函数 tf.losses</a><ul>
<li><a href="#91-0-1">9.1. 0-1函数</a></li>
<li><a href="#92">9.2 绝对值损失</a></li>
<li><a href="#93-log">9.3 log对数函数</a><ul>
<li><a href="#_6">对于二分类问题</a></li>
</ul>
</li>
<li><a href="#94">9.4 平方函数</a></li>
<li><a href="#95">9.5 指数损失函数</a></li>
<li><a href="#96-hinge">9.6 Hinge损失函数</a></li>
<li><a href="#97-huber">9.7 Huber损失函数</a></li>
<li><a href="#97-log-cosh">9.7 Log-Cosh损失</a></li>
<li><a href="#97-loss-tfnnloss">9.7. 神经网络里面的loss （tf.nn.loss）</a><ul>
<li><a href="#_7">熵</a></li>
<li><a href="#971-tfnnsigmoid_cross_entropy_with_logits">9.7.1  tf.nn.sigmoid_cross_entropy_with_logits</a></li>
<li><a href="#972-tfnnsoftmax_cross_entropy_with_logits">9.7.2 tf.nn.softmax_cross_entropy_with_logits</a></li>
<li><a href="#973-tfnnsparse_softmax_cross_entropy_with_logits">9.7.3 tf.nn.sparse_softmax_cross_entropy_with_logits</a></li>
<li><a href="#974-tfnnl2_loss">9.7.4 tf.nn.l2_loss</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#10">10. 图的管理</a><ul>
<li><a href="#101">10.1 图的管理</a><ul>
<li><a href="#1011">10.1.1 图的建立/加载</a></li>
<li><a href="#1012">10.1.2 图的管理操作</a><ul>
<li><a href="#1_7">1. 设置为默认图</a></li>
<li><a href="#1_8">1. 设置为默认图</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#102-edge">10.2 edge(边缘)操作</a></li>
<li><a href="#103-node">10.3 node(节点)操作</a></li>
</ul>
</li>
<li><a href="#11_1">11. 迁移学习</a></li>
<li><a href="#12_1">12. 正则化</a></li>
<li><a href="#13-tfcontribresampler">13. tf.contrib.resampler</a></li>
<li><a href="#14-session">14. Session 会话</a></li>
<li><a href="#15-tensorflow-tfvariable">15. TensorFlow 变量(tf.Variable)</a><ul>
<li><a href="#150">15.0 常量/变量/占位符/张量</a></li>
<li><a href="#151">15.1 变量的类型</a><ul>
<li><a href="#1511">15.1.1 全局变量</a></li>
<li><a href="#1512">15.1.2 本地变量</a></li>
<li><a href="#1513">15.1.3 可训练变量</a></li>
<li><a href="#1514">15.1.4 模型变量</a></li>
<li><a href="#1515">15.1.5 指数移动平均值变量</a></li>
</ul>
</li>
<li><a href="#152">15.2 变量的操作</a><ul>
<li><a href="#1521">15.2.1 变量初始化</a><ul>
<li><a href="#1_9">1. 确定值初始化</a></li>
<li><a href="#2_10">2. 随机初始化</a><ul>
<li><a href="#21">2.1 正态分布</a></li>
<li><a href="#22_1">2.2 均匀分布</a></li>
</ul>
</li>
<li><a href="#3_5">3. 特殊函数初始化</a><ul>
<li><a href="#31-xavier-initialization-glorot-initialization">3.1 Xavier initialization/ Glorot initialization</a></li>
<li><a href="#32-he-initialization">3.2 He initialization</a></li>
<li><a href="#33-lecun-initialization">3.3 Lecun initialization</a></li>
<li><a href="#33-orthogonal-initialization-">3.3 orthogonal initialization(正交初始化)-基于正态分布</a></li>
</ul>
</li>
<li><a href="#4_1">4. 变量初始化的影响</a><ul>
<li><a href="#41">4.1  没有初始化，变量报错</a></li>
<li><a href="#42">4.2 糟糕的初始化影响变量的数值更新</a></li>
</ul>
</li>
<li><a href="#4_2">4. 从磁盘中加载数据初始化</a></li>
</ul>
</li>
<li><a href="#1522">15.2.2 变量赋值</a></li>
<li><a href="#1523">15.2.3 变量恢复与保存</a><ul>
<li><a href="#1_10">1. 变量保存</a></li>
<li><a href="#2_11">2. 变量加载回复</a></li>
</ul>
</li>
<li><a href="#1524">15.2.4 获得变量</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#16-tfprofile">16. 程序性能评估(tf.profile)</a><ul>
<li><a href="#161">16.1 程序性能评估的过程</a></li>
<li><a href="#162">16.2 评估器实例</a><ul>
<li><a href="#1621">16.2.1 实例的建立</a></li>
<li><a href="#1622">16.2.2 实例操作</a></li>
<li><a href="#1622_1">16.2.2 评估运行并得到结果</a></li>
</ul>
</li>
<li><a href="#163-protobuf">16.3 protobuf格式的数据结构对象</a></li>
<li><a href="#164">16.4 评估器的选项设置</a><ul>
<li><a href="#1641">16.4.1 记录内容选项</a><ul>
<li><a href="#1_11">1. 记录时间和内存消耗</a></li>
<li><a href="#2_12">2. 记录浮点运算情况</a></li>
</ul>
</li>
<li><a href="#1642">16.4.2 输出选项</a></li>
<li><a href="#1643">16.4.3 限定选项</a><ul>
<li><a href="#1-profiler">1. 选择制定profiler节点</a></li>
<li><a href="#2-profiler">2. 选择消耗时间大于阈值的profiler节点</a></li>
<li><a href="#3-profiler">3. 选择消耗空间大于阈值的profiler节点</a></li>
<li><a href="#4-profiler">4. 选择运算大于阈值的profiler节点</a></li>
<li><a href="#5-profiler">5. 选择参数数量大于阈值的profiler节点</a></li>
</ul>
</li>
<li><a href="#1644-">16.4.4 评估器选项构建器-构建</a></li>
<li><a href="#1644">16.4.4 输出视图</a></li>
</ul>
</li>
<li><a href="#164_1">16.4 常用实例</a><ul>
<li><a href="#1-flop">1. 浮点运算次数 flop</a></li>
<li><a href="#2_13">2. 总参数量</a></li>
<li><a href="#3_6">3. 统计模型内存和耗时情况</a></li>
<li><a href="#4-profile">4. 给出使用profile工具给出建议</a></li>
</ul>
</li>
<li><a href="#165-profile">16.5 基于profile的建议</a></li>
<li><a href="#166-estimator-profile">16.6 在 estimator 中使用profile</a></li>
<li><a href="#167-keras">16.7 在keras 中使用</a></li>
</ul>
</li>
<li><a href="#17tfappflags">17.命令行传递参数(tf.app.flags)</a></li>
<li><a href="#18-tfnnbatch_normalization">18.归一化  tf.nn.batch_normalization</a></li>
<li><a href="#19-tfgfile">19. tf.gfile文件操作模块</a><ul>
<li><a href="#_8">复制</a></li>
<li><a href="#_9">删除</a></li>
<li><a href="#filename">判断路径filename是否存在</a></li>
<li><a href="#dirname">判断路径dirname是否为一目录</a></li>
</ul>
</li>
<li><a href="#20">20. 计算梯度</a></li>
<li><a href="#x1-tensorflow">X1. 基于Tensorflow 的模型训练 基本步骤</a></li>
<li><a href="#x2-tensorflow">X2. Tensorflow 的运行过程</a></li>
<li><a href="#_10">参考文献</a></li>
</ul>
</div>
<h1 id="1tftrain">1.训练的管理（tf.train）</h1>
<p>tensorflow 主要有2种方式进行训练监督<br />
1. tf.train.MonitorSession()<br />
2. tf.train.Supervisor</p>
<h3 id="1-tftrainsupervisor">1. tf.train.Supervisor</h3>
<p>tf.Supervisor 是一个比较简单的训练监督方法。</p>
<blockquote>
<ol>
<li>自动save_model</li>
<li>自动write_summary (tensorboard)</li>
<li>检查停止点</li>
<li>check_point 文件中恢复模型<br />
</li>
</ol>
</blockquote>
<ul>
<li>它每10分钟(默认save_model_secs=600)向 'logdir'内保存图内的 vars</li>
<li>并且它每2分钟（默认save_summaries_secs=120）自动运行 所有的 summary_ops，同时把 event file 存进 'logdir' 。</li>
<li>他还自动记录 steps ， 在它自己的线程里启动  tf.train.QueueRunner 。</li>
<li>也可以用它来检查停止点 tf.Supervisor.should_stop() ，要使用这个的话需要自己来设定 stop creterion ，然后当满足该条件时再用 tf.Supervisor.request_stop() 来触发 tf.Supervisor.should_stop() ，下一次检查 should_stop() 时就会停下。</li>
</ul>
<blockquote>
<div class="hlcode"><pre>    <span class="n">sv</span><span class="p">.</span><span class="n">prepare_or_wait_for_session</span><span class="p">(</span><span class="n">server</span><span class="p">.</span><span class="n">target</span><span class="p">)</span>
    <span class="n">merged_summary_op</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">merge_all_summaries</span><span class="p">()</span>
    <span class="n">sv</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">Supervisor</span><span class="p">(</span><span class="n">logdir</span><span class="o">=</span><span class="s">&quot;/home/keith/tmp/&quot;</span><span class="p">,</span><span class="n">init_op</span><span class="o">=</span><span class="n">init_op</span><span class="p">)</span> 
    <span class="err">#</span><span class="n">logdir</span><span class="err">用来保存</span><span class="n">checkpoint</span><span class="err">和</span><span class="n">summary</span>
    <span class="n">saver</span><span class="o">=</span><span class="n">sv</span><span class="p">.</span><span class="n">saver</span>
</pre></div>


</blockquote>
<p>从上面代码可以看出，Supervisor帮助我们处理一些事情：<br />
1. 自动去checkpoint加载数据或初始化数据<br />
2. 自身有一个Saver，可以用来保存checkpoint<br />
3. 有一个summary_computed用来保存Summary</p>
<p>所以，我们就不需要：<br />
1. 手动初始化或从checkpoint中加载数据<br />
2. 不需要创建Saver，使用sv内部的就可以<br />
3. 不需要创建summary writer</p>
<div class="hlcode"><pre>    <span class="n">__init__</span><span class="p">(</span>
    <span class="n">graph</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>
    <span class="n">ready_op</span><span class="o">=</span><span class="n">USE_DEFAULT</span><span class="p">,</span>
    <span class="n">ready_for_local_init_op</span><span class="o">=</span><span class="n">USE_DEFAULT</span><span class="p">,</span>
    <span class="n">is_chief</span><span class="o">=</span><span class="n">True</span><span class="p">,</span>
    <span class="n">init_op</span><span class="o">=</span><span class="n">USE_DEFAULT</span><span class="p">,</span>
    <span class="n">init_feed_dict</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>
    <span class="n">local_init_op</span><span class="o">=</span><span class="n">USE_DEFAULT</span><span class="p">,</span>
    <span class="n">logdir</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>
    <span class="n">summary_op</span><span class="o">=</span><span class="n">USE_DEFAULT</span><span class="p">,</span>
    <span class="n">saver</span><span class="o">=</span><span class="n">USE_DEFAULT</span><span class="p">,</span>
    <span class="n">global_step</span><span class="o">=</span><span class="n">USE_DEFAULT</span><span class="p">,</span>
    <span class="n">save_summaries_secs</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span>
    <span class="n">save_model_secs</span><span class="o">=</span><span class="mi">600</span><span class="p">,</span>
    <span class="n">recovery_wait_secs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
    <span class="n">stop_grace_secs</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span>
    <span class="n">checkpoint_basename</span><span class="o">=</span><span class="err">&#39;</span><span class="n">model</span><span class="p">.</span><span class="n">ckpt</span><span class="err">&#39;</span><span class="p">,</span>
    <span class="n">session_manager</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>
    <span class="n">summary_writer</span><span class="o">=</span><span class="n">USE_DEFAULT</span><span class="p">,</span>
    <span class="n">init_fn</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>
    <span class="n">local_init_run_options</span><span class="o">=</span><span class="n">None</span>
    <span class="p">)</span>
</pre></div>


<h3 id="2-tftrainmonitorsession">2. tf.train.MonitorSession()</h3>
<p>处理以下操作：<br />
1. 参数初始化<br />
2. 使用钩子hook<br />
3. 从错误中恢复会话的运行</p>
<p>执行步骤:</p>
<blockquote>
<ol>
<li>MonitoredSession init</li>
<li>MonitoredSession run</li>
<li>MonitoredSession close</li>
</ol>
</blockquote>
<h4 id="1-monitoredsession-init">1. MonitoredSession init 初始化</h4>
<blockquote>
<ol>
<li>调用 hook.begin()</li>
<li>调用scaffold.finalize() <br />
初始化计算图<blockquote>
<p>如果没有指定scaffold,使用 ops.GraphKeys.SUMMARY_OP 构建图，所有， tf.train.MonitorSession() 要指定图。tf.1.12 的init 中</p>
</blockquote>
</li>
<li>Creatsession.<br />
sess=tf.Seession as<br />
   创建会话 </li>
<li>init Scaffold的操作(op)<br />
sess.run(Scaffold.init_op) 初始化模型<blockquote>
<p>Scaffold.init_op 如果没有，</p>
</blockquote>
</li>
</ol>
</blockquote>
<ol>
<li>如果checkpoint存在的话，restore模型的参数 launches queue runners</li>
<li>调用hook.after_create_session()</li>
</ol>
<h4 id="2-monitoredsession-run">2. MonitoredSession run 运行</h4>
<blockquote>
<ol>
<li>调用hook.before_run()</li>
<li>调用TensorFlow的 session.run()</li>
<li>调用hook.after_run()</li>
<li>返回用户需要的session.run()的结果</li>
</ol>
</blockquote>
<p>如果发生了AbortedError或者UnavailableError，则在再次执行run()之前恢复或者重新初始化会话</p>
<h4 id="3-monitoredsession-close">3.  MonitoredSession close 关闭</h4>
<blockquote>
<ol>
<li>调用hook.end()</li>
<li>关闭队列和会话</li>
<li>阻止OutOfRange错误<br />
</li>
</ol>
</blockquote>
<h4 id="4-1-global-step">4. 关键点1： global step</h4>
<p>global_step在训练中是计数的作用，每训练一个batch就加1</p>
<p>在初始化训练的监控程序之前，一个用于跟踪训练步数的张量 global step 必须 添加到图中</p>
<ol>
<li>初始化时，global step必须被设定:<br />
    global_step = tf.train.get_global_step /tf.train.get_or_create_global_step</li>
<li>在训练过程中，step可以被获取:<br />
    step = tf.train.global_step(sess, global_step)</li>
<li>step 通过传递到min op 中自增加:<br />
    tf.train.AdamOptimizer(self.learning rt).minimize(self.loss, global step=self.step)</li>
</ol>
<hr />
<p><strong>Arg</strong>:</p>
<p><strong>session_creator</strong>: 制定用于创建回话的ChiefSessionCreator.<br />
<strong>hooks</strong>: An iterable of `SessionRunHook' objects.</p>
<blockquote>
<p>主要有<br />
1. LoggingTensorHook  每 N step/每N 秒 输出指定 tensors<br />
2. SummarySaverHook 每N steps 保存Summary<br />
3. CheckpointSaverHook 每N steps 保存Checkpoint<br />
4. NanTensorHook 监测loss 函数，当loss==NaN 时停止训练<br />
5. FeedFnHook 运行 feed_fn 函数 通知是的 feed_dict 字典匹配 <br />
6. GlobalStepWaiterHook 分布式使用GlobalStep时 <br />
7. ProfilerHook 在分布式计算过程中用于逐步启动worker<br />
8. tf.train.StopAtStepHook()设置停止训练的条件</p>
</blockquote>
<h4 id="5-2-tftrainscaffold">5. 关键点2： tf.train.Scaffold</h4>
<p>当你建立以用于训练的模型的时候，你通常需要 初始化操作、一个保存检查点checkpoint 的操作，一个用于tensorboard 可视化的summary 操作等，Scaffold类 将帮助你创建并添加到一个集合里面</p>
<p>可以省略 init_op \ tensorboard  tf.summary.merge_all()<br />
或者 在tf.train.Scaffold中定义 init_op、summary_op、ready_op</p>
<div class="hlcode"><pre><span class="n">__init__</span><span class="p">(</span>
    <span class="n">init_op</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">init_feed_dict</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">init_fn</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">ready_op</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">ready_for_local_init_op</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">local_init_op</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">summary_op</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">saver</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">copy_from_scaffold</span><span class="o">=</span><span class="bp">None</span>
<span class="p">)</span>
</pre></div>


<h4 id="6-tftrainmonitoredtrainingsession">6. 常用子类： tf.train.MonitoredTrainingSession()</h4>
<p>属于.MonitorSession() 是其子类 ,返回.MonitorSession()<br />
主要功能：</p>
<ol>
<li>自动保存检查点checkpoint saver_hook = CheckpointSaverHook(...)</li>
<li>自动运行保存summary（tensorboard） summary_hook = SummarySaverHook(...)</li>
<li>方便在多设备上运行tensorflow  session_creator</li>
</ol>
<blockquote>
<p>注意上述的全部功能其实都是程序设定了默认值运行的。如 ，自动保存检查点checkpoint。里面靠寻找name为“global_step”的变量，来寻找global_step,所以需要在图中定义名称为global_step的变量。tf.train.MonitoredTrainingSession()作为sess, sess 里面主要是train—_op ，才能定义global_step</p>
</blockquote>
<div class="hlcode"><pre>    <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">MonitoredTrainingSession</span><span class="p">(</span>
    <span class="n">master</span><span class="o">=</span><span class="err">&#39;&#39;</span><span class="p">,</span>
    <span class="n">is_chief</span><span class="o">=</span><span class="n">True</span><span class="p">,</span>
    <span class="n">checkpoint_dir</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>
    <span class="n">scaffold</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>
    <span class="n">hooks</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>
    <span class="n">chief_only_hooks</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>
    <span class="n">save_checkpoint_secs</span><span class="o">=</span><span class="n">USE_DEFAULT</span><span class="p">,</span>
    <span class="n">save_summaries_steps</span><span class="o">=</span><span class="n">USE_DEFAULT</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
    <span class="n">save_summaries_secs</span><span class="o">=</span><span class="n">USE_DEFAULT</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="n">None</span><span class="p">,</span>
    <span class="n">stop_grace_period_secs</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span>
    <span class="n">log_step_count_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">max_wait_secs</span><span class="o">=</span><span class="mi">7200</span><span class="p">,</span>
    <span class="n">save_checkpoint_steps</span><span class="o">=</span><span class="n">USE_DEFAULT</span><span class="p">,</span>
    <span class="n">summary_dir</span><span class="o">=</span><span class="n">None</span>
    <span class="p">)</span>
</pre></div>


<blockquote>
<p><strong>Args:</strong><br />
<strong>master</strong> 字符串 the TensorFlow master to use.<br />
<strong>is_chief</strong>：用于分布式系统中，用于判断该系统是否是chief，如果为True，它将负责初始化并恢复底层TensorFlow会话。如果为False，它将等待chief初始化或恢复TensorFlow会话。<br />
<strong>checkpoint_dir</strong>：字符串。指定一个用于恢复变量的checkpoint文件路径。<br />
<strong>scaffold</strong> ：(脚手架) 用于集合或建立op 。如果未指定，则会创建默认一个默认的scaffold。它用于完成图<br />
<strong>hooks</strong>：一个SessionRunHook对象的列表。<br />
<strong>chief_only_hooks</strong>：SessionRunHook对象列表。如果is_chief== True，则激活这些挂钩，否则忽略。<br />
<strong>save_checkpoint_secs</strong>：用默认的checkpoint saver保存checkpoint的频率（以秒为单位）。如果save_checkpoint_secs设置为None，不保存checkpoint。<br />
<strong>save_summaries_steps</strong>：使用默认summaries saver将摘要写入磁盘的频率（以全局步数表示）。如果save_summaries_steps和<strong>save_summaries_secs</strong> 都设置为None，则不使用默认的summaries saver保存summaries。默认为100<br />
<strong>save_summaries_secs</strong>：使用默认summaries saver将摘要写入磁盘的频率（以秒为单位）。如果save_summaries_steps和save_summaries_secs都设置为None，则不使用默认的摘要保存。默认未启用。<br />
<strong>config</strong>：用于配置会话的tf.ConfigProtoproto的实例。它是tf.Session的构造函数的config参数。<br />
<strong>stop_grace_period_secs</strong>：调用close（）后线程停止的秒数。<br />
<strong>log_step_count_steps</strong>：记录全局步/秒的全局步数的频率</p>
<p><strong>Return :</strong><br />
一个MonitoredSession（） 实例。</p>
</blockquote>
<h4 id="tensorflow-hook">tensorflow hook架构</h4>
<p>所有的hook都继承自SessionRunHook，定义在session_run_hook.py 文件里。其包含五个通用接口：</p>
<div class="hlcode"><pre>    <span class="n">def</span> <span class="n">begin</span><span class="p">(</span><span class="n">self</span><span class="p">)</span>

    <span class="n">def</span> <span class="n">after_create_session</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">session</span><span class="p">,</span> <span class="n">coord</span><span class="p">)</span>

    <span class="n">def</span> <span class="n">before_run</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">run_context</span><span class="p">)</span>

    <span class="n">def</span> <span class="n">after_run</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">run_context</span><span class="p">,</span> <span class="n">run_values</span><span class="p">)</span>

    <span class="n">def</span> <span class="n">end</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">session</span><span class="p">)</span>
</pre></div>


<h1 id="2">2. 设备的管理</h1>
<h3 id="21-tfdevice">2.1 tf.device</h3>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_name_or_function</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">&quot;/cpu:0&quot;</span><span class="p">):</span>
  <span class="o">...</span>
</pre></div>


<p><code>device_name_or_function</code> 可以是<br />
1. 设备名称字符串 <br />
 /job:<JOB_NAME>/task:<TASK_INDEX>/device:<DEVICE_TYPE>:<DEVICE_INDEX><br />
2. 返回设备字符串的函数func</p>
<p><strong>其中：</strong></p>
<ul>
<li><strong><JOB_NAME></strong> 是一个字母数字字符串，并且不以数字开头。</li>
<li>
<p><strong><DEVICE_TYPE></strong> 是一种注册设备类型（例如 GPU 或 CPU）。</p>
</li>
<li>
<p><strong><TASK_INDEX></strong> 是一个非负整数，表示名为 </p>
</li>
<li><strong><JOB_NAME></strong> 的作业中的任务的索引。请参阅 tf.train.ClusterSpec 了解作业和任务的说明。</li>
<li><strong><DEVICE_INDEX></strong> 是一个非负整数，表示设备索引，例如用于区分同一进程中使用的不同 GPU 设备。</li>
</ul>
<h3 id="22">2.2 当设备为分布式设备时候</h3>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">replica_device_setter</span><span class="p">(</span><span class="n">cluster</span><span class="o">=</span><span class="n">cluster</span><span class="p">)</span>
</pre></div>


<p>intra_op_parallelism_threads 控制运算符op内部的并行(一个op内部的多个并行)</p>
<p>https://blog.csdn.net/rockingdingo/article/details/55652662</p>
<hr />
<h3 id="23">2.3 查看可用设备</h3>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">tensorflow.python.client</span> <span class="kn">import</span> <span class="n">device_lib</span>

<span class="k">print</span> <span class="p">(</span><span class="s">&quot;可用计算硬件情况：{}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">device_lib</span><span class="o">.</span><span class="n">list_local_devices</span><span class="p">()))</span>

<span class="o">&gt;&gt;&gt;</span><span class="err">可用计算硬件情况：</span><span class="p">[[</span><span class="n">name</span><span class="p">:</span> <span class="s">&quot;/device:CPU:0&quot;</span>
<span class="n">device_type</span><span class="p">:</span> <span class="s">&quot;CPU&quot;</span>
<span class="n">memory_limit</span><span class="p">:</span> <span class="mi">268435456</span>
<span class="n">locality</span> <span class="p">{</span>
<span class="p">}</span>
<span class="n">incarnation</span><span class="p">:</span> <span class="mi">16732882166560050894</span>
<span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="s">&quot;/device:XLA_CPU:0&quot;</span>
<span class="n">device_type</span><span class="p">:</span> <span class="s">&quot;XLA_CPU&quot;</span>
<span class="n">memory_limit</span><span class="p">:</span> <span class="mi">17179869184</span>
<span class="n">locality</span> <span class="p">{</span>
<span class="p">}</span>
<span class="n">incarnation</span><span class="p">:</span> <span class="mi">799772212799803613</span>
<span class="n">physical_device_desc</span><span class="p">:</span> <span class="s">&quot;device: XLA_CPU device&quot;</span>
<span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="s">&quot;/device:XLA_GPU:0&quot;</span>
<span class="n">device_type</span><span class="p">:</span> <span class="s">&quot;XLA_GPU&quot;</span>
<span class="n">memory_limit</span><span class="p">:</span> <span class="mi">17179869184</span>
<span class="n">locality</span> <span class="p">{</span>
<span class="p">}</span>
<span class="n">incarnation</span><span class="p">:</span> <span class="mi">2714282392667418740</span>
<span class="n">physical_device_desc</span><span class="p">:</span> <span class="s">&quot;device: XLA_GPU device&quot;</span>
<span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="s">&quot;/device:GPU:0&quot;</span>
<span class="n">device_type</span><span class="p">:</span> <span class="s">&quot;GPU&quot;</span>
<span class="n">memory_limit</span><span class="p">:</span> <span class="mi">14892338381</span>
<span class="n">locality</span> <span class="p">{</span>
  <span class="n">bus_id</span><span class="p">:</span> <span class="mi">1</span>
  <span class="n">links</span> <span class="p">{</span>
  <span class="p">}</span>
<span class="p">}</span>
<span class="n">incarnation</span><span class="p">:</span> <span class="mi">10914707427568231652</span>
<span class="n">physical_device_desc</span><span class="p">:</span> <span class="s">&quot;device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5&quot;</span>
<span class="p">]]</span>
</pre></div>


<p>Tensorflow把数据输送到计算图的方式主要有三种：</p>
<table>
<thead>
<tr>
<th align="right">编号</th>
<th align="right">方式</th>
<th align="right">说明</th>
<th align="left">优缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">1</td>
<td align="right"><strong>Constant</strong></td>
<td align="right">把Dataset中的数据以const的形式存放在tensorflow的计算图中，主要使用的是tf.constant 函数。</td>
<td align="left">这种形式主要适用于小数据集，由于数据固化到了计算图中，所以它的数据读取速度是最快的。</td>
</tr>
<tr>
<td align="right">2</td>
<td align="right"><strong>Feeding</strong></td>
<td align="right"><p align="left">在每次session.run 时，把numpy形式的数据输入到feed_dict参数中。这种方式主要包括两种存在的状态。<br>1：数据一次性全部load到内存中。自己维护一个DataProvider 类，每次都会获取一部分训练数据。需要注意的是training的时候最好把数据集shuffle，test的时最好不shuffle。<br/> <br>2：训练数据无法一次性全部load到内存中时，分批次load数据。自己维护的DataProvider 类要做好队列的管理。这种形式一个小的trick是每次载入的数据使用多次进行训练，这样可以减少重复地读取数据。</p></td>
<td align="left">优点：<br>    1. 当数据集较小的时候，数据可以全部载入到内存，这时数据的处理速度就会比较快。<br>    2. 训练和测试几乎可以共用一套代码，仅需要把反馈网络去掉，不使用参数更新即可。<br>    3. 在预测的时候，一般数据不会是文件的形式，所以只能使用feeding方式。<br>缺点：<br>    1.自己维护DataProvider类相对麻烦，而且自己写的类原生是不支持多进程的（主要是Python API的问题）<br>    2.单进程读取数据较慢，很多时间花费在数据读取上，所以训练时间相对较长。</td>
</tr>
<tr>
<td align="right">3</td>
<td align="right"><strong>Queue</strong></td>
<td align="right">从文件中读取数据，使用QueueRunner形式从文件中读取。tensorflow以一种黑箱的方式读取数据,必要的时候会启动多进程（需要设置，这些代码是用c++封装的，多线程支持的效果比较好）。</td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p><strong>加载数据的选择</strong><br />
取大量小文件会显著影响 I/O 性能。<br />
实现最大 I/O 吞吐量的一种方法是将输入数据预处理为更大（约 100MB）的 TFRecord 文件。<br />
1. 对于较小的数据集 (200MB-1GB)，最好的方法通常是将整个数据集加载到内存中<br />
2. 对于较大的数据集。 将输入数据预处理为更大（约 100MB）的 TFRecord 文件。 <br />
i. 下载并转换为 TFRecord 格式文档包含用于创建 TFRecord 的信息和脚本，此脚本会将 CIFAR-10 数据集转换为 TFRecord。</p>
<p>tensorflow gfile文件操作详解</p>
<p>翻译过来就是 <strong>无线程锁的文件I/O操作包装器</strong></p>
<ol>
<li>提供了一种类似于python文件 I/O操作的API；</li>
<li>提供了一种操作tensorflow C++文件系统的API；</li>
<li>tensorflow c++文件操作接口支持多个文件系统实现，包括本地文件、谷歌云存储(以gs://开头)和HDFS(以HDFS://开头)，tensorflow封装这些接口到tf.gfile，以便我们可以使用这些接口来存储和加载检查点文件、将tensorboard log信息写到文本里以及访问训练数据(在其他用途里)。但是，如果所有文件都放在本地，那么我们直接使用python提供的常规文本操作接口也是一样效果且毫无问题的</li>
</ol>
<p>https://zhuanlan.zhihu.com/p/31536538</p>
<h3 id="41-tfrecord">4.1 TFRecord 格式</h3>
<h4 id="411-tfrecord">4.1.1  TFRecord 是什么？</h4>
<blockquote>
<p>TFRecord 是谷歌推荐的一种二进制<code>文件格式</code>，理论上它可以保存任何格式的信息。</p>
</blockquote>
<p>TFRecord文件将数据存储为 &lt;<strong>二进制</strong>&gt; &lt;<strong>字符串</strong>&gt; &lt;<strong>序列</strong>&gt; .</p>
<p>将数据写入TFRecord文件之<strong>前</strong>需要<strong>指定数据的结构</strong>，<br />
Tensorflow为此提供了两个组件：tf.train.Example 和 tf.train.SequenceExample。必须将每个数据样本存储在其中一个组件中，然后对其进行序列化并使用tf.python_io.TFRecordWriter把它写到磁盘上。</p>
<h4 id="412-tfrecordqueuerunner">4.1.2  为什么要使用TFRecord和Queuerunner?</h4>
<ol>
<li>Python不支持多线程（伪多线程，虽可以启用multi-thread，但所有启用线程的处理能力加起来等于一个核的处理能力）</li>
<li>
<p>Python多进程如果是任务可分，不用和主线程交互的情况下是可用的，但对于tensorflow的训练来说，肯定需要使用多（线程、进程）与主（线程、进程）交互。</p>
</li>
<li>
<p>Tensorflow使用黑箱的方式为数据的解析提供支持，相比于自己写multi-process然后共享变量来说在代码实现上更加友好。</p>
</li>
</ol>
<h4 id="413-tfrecord-example">4.1.3 TFRecord 核心--Example</h4>
<p>TFRecord文件本身是 &lt;<strong>二进制</strong>&gt; &lt;<strong>字符串</strong>&gt; &lt;<strong>序列</strong>&gt;，还原数据原本面目的<strong>核心在于说明数据的结构</strong></p>
<h5 id="1">1. 参数说明</h5>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Example</span><span class="p">(</span><span class="n">features</span><span class="p">:</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Features</span><span class="o">=</span><span class="n">xxx</span><span class="p">)</span>
 <span class="o">|</span><span class="n">__tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Features</span><span class="p">(</span><span class="n">feature</span><span class="p">:</span><span class="nb">dict</span><span class="o">=</span><span class="n">feature_dict</span><span class="p">)</span>
   <span class="o">|</span><span class="n">__feature_dict</span><span class="o">=</span><span class="p">{</span><span class="n">key</span><span class="p">:</span><span class="n">value</span><span class="p">}</span>
    <span class="o">|</span><span class="n">__tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Feature</span><span class="p">(</span><span class="n">Int64List</span><span class="o">=</span><span class="n">int_a</span><span class="p">)</span>  <span class="o">|</span><span class="n">__</span> <span class="n">string</span>
        <span class="o">|</span><span class="n">__tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">BytesList</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="p">[])</span>
</pre></div>


<p>tf.train.Example 是protocol buffer  协议下的<strong>消息体(message)</strong>,不是普通的python类</p>
<p>一个 Example <strong>消息体</strong>包含了 <strong>一系列的feature 属性</strong></p>
<div class="hlcode"><pre><span class="n">example</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Example</span><span class="p">(</span><span class="n">features</span><span class="p">:</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Features</span><span class="o">=</span><span class="n">xxx</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span><span class="nb">type</span><span class="p">(</span><span class="n">xxx</span><span class="p">)</span> 
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Features</span>
</pre></div>


<p>tf.train.Features是命名特征的集合。它有一个形参 feature ，类型为一个字典，其中key是特征的名称，value是tf.train.Feature</p>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Features</span><span class="p">(</span><span class="n">feature</span><span class="p">:</span><span class="nb">dict</span><span class="o">=</span><span class="n">feature_dict</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">type</span><span class="p">(</span><span class="n">feature_dict</span><span class="p">)</span>
<span class="nb">dict</span>
</pre></div>


<div class="hlcode"><pre><span class="n">feature_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span><span class="n">value</span><span class="p">,}</span>
<span class="o">&gt;&gt;&gt;</span><span class="nb">type</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="n">string</span>
<span class="o">&gt;&gt;&gt;</span><span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Feature</span>  <span class="c">## 注意没有s</span>

<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Feature</span><span class="p">(</span><span class="n">Int64List</span><span class="p">:</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">BytesList</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">FloatList</span><span class="p">:</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">FloatList</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">BytesList</span><span class="p">:</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Int64List</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Feature</span><span class="p">(</span><span class="n">Int64List</span><span class="o">=</span><span class="n">int_a</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Feature</span><span class="p">(</span><span class="n">FloatList</span><span class="o">=</span><span class="n">float_b</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Feature</span><span class="p">(</span><span class="n">BytesList</span><span class="o">=</span><span class="n">bytes_c</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span><span class="nb">type</span><span class="p">(</span><span class="n">int_a</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">BytesList</span>
<span class="o">&gt;&gt;&gt;</span><span class="nb">type</span><span class="p">(</span><span class="n">float_b</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">FloatList</span>
<span class="o">&gt;&gt;&gt;</span><span class="nb">type</span><span class="p">(</span><span class="n">bytes_c</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Int64List</span>

<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">BytesList</span><span class="p">(</span><span class="n">value</span><span class="p">:</span><span class="nb">list</span><span class="o">=</span><span class="p">[])</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">FloatList</span><span class="p">(</span><span class="n">value</span><span class="p">:</span><span class="nb">list</span><span class="o">=</span><span class="p">[])</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Int64List</span><span class="p">(</span><span class="n">value</span><span class="p">:</span><span class="nb">list</span><span class="o">=</span><span class="p">[])</span>

<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">BytesList</span><span class="p">(</span><span class="n">value</span><span class="p">:</span><span class="nb">list</span><span class="o">=</span><span class="p">[</span><span class="n">b</span><span class="s">&quot;hello world&quot;</span><span class="p">])</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">FloatList</span><span class="p">(</span><span class="n">value</span><span class="p">:</span><span class="nb">list</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.2</span><span class="p">])</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Int64List</span><span class="p">(</span><span class="n">value</span><span class="p">:</span><span class="nb">list</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
</pre></div>


<h5 id="2_1">2. 方法说明</h5>
<p>tf.train.Example和tf.train.SequenceExample提供SerializeToString方法,进行结构化数据首先需要序列化</p>
<p>example.SerializeToString()</p>
<h4 id="414-tfrecord">4.1.4 存储为 TFrecord格式文件</h4>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">python_io</span><span class="o">.</span><span class="n">TFRecordWriter</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="n">float_list</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Int64List</span><span class="p">(</span><span class="n">value</span><span class="p">:</span><span class="nb">list</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="n">feature_value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Feature</span><span class="p">(</span><span class="n">FloatList</span><span class="o">=</span><span class="n">float_list</span><span class="p">)</span>
<span class="n">feature_dict</span><span class="o">=</span><span class="p">{</span><span class="s">&quot;feature_key&quot;</span><span class="p">,</span><span class="n">feature_value</span><span class="p">}</span>
<span class="n">features</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Features</span><span class="p">(</span><span class="n">feature</span><span class="p">:</span><span class="nb">dict</span><span class="o">=</span><span class="n">feature_dict</span><span class="p">)</span>
<span class="n">example</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Example</span><span class="p">(</span><span class="n">features</span><span class="p">:</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Features</span><span class="o">=</span><span class="n">features</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">python_io</span><span class="o">.</span><span class="n">TFRecordWriter</span><span class="p">(</span><span class="s">&#39;customer_1.tfrecord&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
    <span class="n">writer</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">example</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">())</span>
</pre></div>


<h4 id="415-tfrecordtensorflow">4.1.5  TFrecord格式文件在tensorflow图中输出</h4>
<h5 id="1_1">1. 导入</h5>
<p>从多个tfrecord文件中导入数据到Dataset类</p>
<div class="hlcode"><pre><span class="n">filenames</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;test.tfrecord&quot;</span><span class="p">,</span> <span class="s">&quot;test2.tfrecord&quot;</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TFRecordDataset</span><span class="p">(</span><span class="n">filenames</span><span class="p">)</span>
</pre></div>


<h5 id="2_2">2. 序列化样本解析</h5>
<p>tfrecord文件是序列化样本，所以我们需要对每一个样本进行解析。 具体实现 通过dataset 的map方法，map 解析函数，如下：</p>
<div class="hlcode"><pre><span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">parse_function</span><span class="p">)</span>
<span class="c"># parse_function 解析函数</span>
</pre></div>


<h6 id="21-parse_function">2.1 parse_function 解析函数</h6>
<p><strong>1. 输入输出</strong><br />
输入：example_proto  也就是序列化后的样本tf_serialized<br />
输出： parsed_example </p>
<div class="hlcode"><pre><span class="k">def</span> <span class="nf">parse_function</span><span class="p">(</span><span class="n">example_proto</span><span class="p">):</span>
    <span class="c"># 只接受一个形参：example_proto，也就是序列化后的样本tf_serialized</span>
    <span class="n">parsed_example</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">parse_single_example</span><span class="p">(</span><span class="n">example_proto</span><span class="p">,</span> <span class="n">feature_dicts</span><span class="p">)</span>
    <span class="c"># feature_dicts 解析字典---feature的解析方式</span>
    <span class="c"># feature_dicts={key:value}  key为feature名，value为feature的解析方式</span>

    <span class="c"># 返回所有feature</span>
    <span class="k">return</span> <span class="n">parsed_example</span>
</pre></div>


<p><strong>2. feature_dicts 解析字典</strong><br />
feature_dicts={key:value}  key为feature名，value为feature的解析方式</p>
<p><strong>3. feature的解析方式</strong></p>
<p>feature的解析方式:</p>
<table>
<thead>
<tr>
<th align="right">编号</th>
<th align="left">解析方式</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">1</td>
<td align="left">定长特征解析</td>
</tr>
<tr>
<td align="right">2</td>
<td align="left">不定长特征解析</td>
</tr>
</tbody>
</table>
<ol>
<li>定长特征解析：<code>tf.FixedLenFeature(shape, dtype, default_value)</code></li>
</ol>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">FixedLenFeature</span><span class="p">(</span><span class="n">shape</span><span class="p">:</span><span class="nb">tuple</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">default_value</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>


<table>
<thead>
<tr>
<th align="right">形参</th>
<th align="left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">shape</td>
<td align="left">1. 可当reshape来用，如vector的shape从(3,)改动成了(1,3)。<br>2. 如果写入的feature使用了.tostring() 其shape就是()</td>
</tr>
<tr>
<td align="right">dtype</td>
<td align="left">必须是tf.float32， tf.int64， tf.string中的一种。</td>
</tr>
<tr>
<td align="right">default_value</td>
<td align="left">feature值缺失时所指定的值。</td>
</tr>
</tbody>
</table>
<p>注：</p>
<ol>
<li>不定长特征解析：<code>tf.VarLenFeature(dtype)</code></li>
</ol>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">VarLenFeature</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="c">#注：可以不明确指定shape，但得到的tensor是SparseTensor。</span>
</pre></div>


<p><strong>4. 特殊 feature的 转变特征</strong></p>
<p>得到的parsed_example也是一个字典，其中每个key是对应feature的名字，value是相应的feature解析值。如果使用了下面两种情况，则还需要对这些值进行转变。其他情况则不用。</p>
<p>string类型：tf.decode_raw(parsed_feature, type) 来解码<br />
注：这里type必须要和当初.tostring()化前的一致。如tensor转变前是tf.uint8，这里就需是tf.uint8；转变前是tf.float32，则tf.float32</p>
<p>VarLen解析：由于得到的是SparseTensor，所以视情况需要用tf.sparse_tensor_to_dense(SparseTensor)来转变成DenseTensor</p>
<div class="hlcode"><pre><span class="c"># 解码字符</span>
<span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;tensor&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">decode_raw</span><span class="p">(</span><span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;tensor&#39;</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="c"># 稀疏表示 转为 密集表示</span>
<span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;matrix&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sparse_tensor_to_dense</span><span class="p">(</span><span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;matrix&#39;</span><span class="p">])</span>
</pre></div>


<p><strong>5. 改变形状</strong><br />
到此为止得到的特征都是向量，需要根据之前存储的shape信息对每个feature进行reshape。</p>
<div class="hlcode"><pre><span class="c"># 转变matrix形状</span>
<span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;matrix&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;matrix&#39;</span><span class="p">],</span> <span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;matrix_shape&#39;</span><span class="p">])</span>

<span class="c"># 转变tensor形状</span>
<span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;tensor&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;tensor&#39;</span><span class="p">],</span> <span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;tensor_shape&#39;</span><span class="p">])</span>
</pre></div>


<p>2.1.3. 执行解析函数</p>
<p>创建好解析函数后，将创建的parse_function送入dataset.map()得到新的数据集</p>
<p>new_dataset = dataset.map(parse_function)</p>
<h5 id="3">3. 创建迭代器</h5>
<p>后续与其他dataset 处理一致<br />
iterator = dataset.make_one_shot_iterator()</p>
<h3 id="42-protocol-buffer">4.2. Protocol Buffer</h3>
<p>Protocol Buffer 是谷歌开放的处理结构化数据的工具<br />
特性：<br />
1. 处理结构化数据的工具<br />
2. 二进制流<br />
3. 需要先定义数据格式（schema）,才能还原<br />
4. 序列化数据比xml 小3~10倍，解析快20~100倍<br />
5. 文件格式 <code>.proto</code><br />
6. 每一个message代表了一类结构体和的数据</p>
<h2 id="_1">其他</h2>
<h2 id="parse_example">parse_example</h2>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">parse_example</span><span class="p">(</span>
    <span class="n">serialized</span><span class="p">,</span>
    <span class="n">features_spec</span><span class="p">,</span>
    <span class="n">example_names</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="bp">None</span>
<span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">parse_example</span><span class="p">()</span>
<span class="n">tf</span><span class="o">.</span><span class="n">parse_example</span><span class="p">()</span>

<span class="c"># serialized:一个batch的序列化的example </span>
<span class="c"># features_spec:解析example的规则 </span>
<span class="c"># name：当前操作的名字 </span>
<span class="c"># example_name:当前解析example的proto名称</span>
</pre></div>


<p>这里重点要说的是第二个参数，也就是features，features是把serialized的example中按照键值映射到三种tensor: 1,VarlenFeature 2, SparseFeature 3,FixedLenFeature <br />
下面对这三种映射方式做一个简要的叙述：</p>
<p>VarlenFeature</p>
<p>是按照键值把example的value映射到SpareTensor对象，假设我们有如下的serialized数据：</p>
<p>serialized = [<br />
    features<br />
      { feature { key: "ft" value { float_list { value: [1.0, 2.0] } } } },<br />
    features<br />
      { feature []},<br />
    features<br />
      { feature { key: "ft" value { float_list { value: [3.0] } } }<br />
  ]</p>
<p>使用VarLenFeatures方法：</p>
<p>features={<br />
    "ft":tf.VarLenFeature(tf.float32)<br />
}<br />
1<br />
2<br />
3<br />
那么我们将得到的是：</p>
<p>{"ft": SparseTensor(indices=[[0, 0], [0, 1], [2, 0]],<br />
                      values=[1.0, 2.0, 3.0],<br />
                      dense_shape=(3, 2)) }<br />
1<br />
2<br />
3<br />
可见，显示的indices是ft值的索引，values是值，dense_shape是indices的shape</p>
<p>FixedLenFeature</p>
<p>而FixedLenFeature是按照键值对将features映射到大小为[serilized.size(),df.shape]的矩阵，这里的FixLenFeature指的是每个键值对应的feature的size是一样的。对于上面的例子，如果使用：</p>
<p>features: {<br />
      "ft": FixedLenFeature([2], dtype=tf.float32, default_value=-1),<br />
  }<br />
1<br />
2<br />
3<br />
那么我们将得到：</p>
<p>{"ft": [[1.0, 2.0], [3.0, -1.0]]}<br />
1<br />
可见返回的值是一个[2,2]的矩阵，如果返回的长度不足给定的长度，那么将会使用默认值去填充。 <br />
【注意：】</p>
<h1 id="5">5.持久化（保存和恢复）</h1>
<p>参考：https://www.tensorflow.org/guide/saved_model</p>
<h2 id="1_2">1. 几个概念</h2>
<h3 id="_2">序列化</h3>
<blockquote>
<blockquote>
<ol>
<li><strong>数据序列化</strong>就是将对象或者数据结构转化成特定的格式，使其可在网络中传输，或者可存储在内存或者文件中。<br />
廖雪峰：变量从内存中变成可存储或传输的过程称之为序列化<br />
对象序列化后的数据格式可以是二进制，可以是XML，也可以是JSON等任何格式。</li>
</ol>
</blockquote>
</blockquote>
<h3 id="_3">反序列化</h3>
<blockquote>
<blockquote>
<ol>
<li><strong>反序列化</strong>是相反的操作，将对象从序列化数据中还原出来。<br />
廖雪峰：把变量内容从序列化的对象重新读到内存里称之为反序列化</li>
</ol>
</blockquote>
</blockquote>
<h3 id="protobuf">protobuf</h3>
<blockquote>
<blockquote>
<ol>
<li><strong>protobuf</strong> <br />
 Google Protocol Buffers 简称 Protobuf，类似json的一种数据格式，但不同的是他是二进制格式，性能好、效率高<br />
文件以.proto结尾 <br />
https://tensorflow.juejin.im/extend/tool_developers/index.html<br />
ProtoBuf 实际上支持两种不同的文件保存格式。<br />
1.TextFormat 是一种人眼可读的文本形式，这在调试和编辑时是很方便的，但它在存储数值数据时会变得很大，比如我们常见的权重数据。文件名为 xxx.pbtxt。<br />
2.二进制格式的文件会小得多，缺点就是它不易读。文件名为 xxx.pb</li>
</ol>
</blockquote>
</blockquote>
<h2 id="2_3">2. 需要保存什么</h2>
<p>主要是：</p>
<table>
<thead>
<tr>
<th>编号</th>
<th>项目</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>图信息</td>
</tr>
<tr>
<td>2</td>
<td>变量信息</td>
</tr>
<tr>
<td>3</td>
<td>其他信息（服务器信息）</td>
</tr>
</tbody>
</table>
<h4 id="1_3">1. 图信息</h4>
<p>图被定义为 “一些 Operation（Node节点） 和 Tensor（Edge边缘） 的集合”。</p>
<p><strong>图信息的主要内容包括： 1.Node节点信息;2.边信息（Tensor）</strong></p>
<h5 id="1-node-operation">1. Node节点信息--operation</h5>
<p>图中的节点又称为算子，它代表一个操作（operation，OP），一般用来表示施加的数学运算，也可以表示数据输入（feed in）的起点以及输出（push out）的终点，或者是读取/写入持久 变量（persistent variable）的终点。</p>
<p>Operation包含OpDef和NodeDef两个主要成员变量。<br />
1. OpDef描述了op的静态属性信息，例如op入参列表，出参列表等。<br />
2. NodeDef则描述op的动态属性信息，例如op运行的设备信息，用户给op设置的name等。包括placeholder,placeholder 是tensor </p>
<div class="hlcode"><pre><span class="p">[</span><span class="n">op</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_operations</span><span class="p">()]</span>
</pre></div>


<h5 id="2-tensor">2. 边缘信息--Tensor</h5>
<p>边用来表示计算的数据，它经过上游节点计算后得到，然后传递给下游节点进行运算。</p>
<p>Tensor中主要包含两类信息：<br />
1. 是Graph结构信息，如边的源节点和目标节点(有向图)。<br />
2. 它所保存的数据信息，例如数据类型，shape等（tensor.shape/tensor.dytpe）。</p>
<h4 id="2_4">2. 参数信息</h4>
<p>主要为tf.Variables类的节点信息即变量的初始值、具体数值、shape等。</p>
<h4 id="3_1">3. 其他信息</h4>
<ol>
<li>服务器信息</li>
<li>集群信息</li>
<li>Checkpoint： 用于保存模型的权重，主要用于模型训练过程中参数的备份和模型训练热启动。model_checkpoint_path;all_model_checkpoint_path</li>
<li>版本和其他用户信息</li>
</ol>
<h5 id="12-metagraph">1.2 MetaGraph 信息</h5>
<h2 id="3_2">3. 怎么保存</h2>
<h3 id="1-metadata">1. 常规保存 Meta+data</h3>
<h4 id="1-metagraphdef">1. MetaGraphDef 保存图信息和其他信息</h4>
<p>MetaGraphDef 是 MetaGraph信息的序列化文件，同样是由 Protocol Buffer来定义的一个包含<strong>更多图信息</strong>的序列化文件。其中包括：</p>
<table>
<thead>
<tr>
<th align="right">组成</th>
<th align="right">内容</th>
<th align="right">例如</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">MetaInfoDef</td>
<td align="right">存一些元信息</td>
<td align="right">版本和其他用户信息</td>
</tr>
<tr>
<td align="right">GraphDef</td>
<td align="right">Graph的序列化信息，MetaGraph的核心内容之一 ,不包含模型权重变量信息</td>
<td align="right">Node(Placeholder + op)</td>
</tr>
<tr>
<td align="right">SaverDef</td>
<td align="right">图的Saver信息</td>
<td align="right">最多同时保存的check-point数量；需保存的Tensor名字等，但并不保存Tensor中的实际内容</td>
</tr>
<tr>
<td align="right">CollectionDef</td>
<td align="right">任何需要特殊注意的 Python 对象，需要特殊的标注以方便import_meta_graph 后取回。</td>
<td align="right">“train_op”,"prediction"等等</td>
</tr>
</tbody>
</table>
<div class="hlcode"><pre><span class="n">graph</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
<span class="n">graph_def</span><span class="o">=</span><span class="n">graph</span><span class="o">.</span><span class="n">as_graph_def</span><span class="p">()</span>
<span class="nb">type</span><span class="p">(</span><span class="n">graph_def</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tensorflow</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">framework</span><span class="o">.</span><span class="n">graph_pb2</span><span class="o">.</span><span class="n">GraphDef</span>

<span class="n">graph_def</span>
<span class="o">&gt;&gt;&gt;</span>
  <span class="n">node</span> <span class="p">{</span>
    <span class="n">name</span><span class="p">:</span> <span class="s">&quot;start&quot;</span>
    <span class="n">op</span><span class="p">:</span> <span class="s">&quot;Const&quot;</span>
    <span class="n">attr</span> <span class="p">{</span>
      <span class="n">key</span><span class="p">:</span> <span class="s">&quot;dtype&quot;</span>
      <span class="n">value</span> <span class="p">{</span>
        <span class="nb">type</span><span class="p">:</span> <span class="n">DT_INT64</span>
      <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">attr</span> <span class="p">{</span>
      <span class="n">key</span><span class="p">:</span> <span class="s">&quot;value&quot;</span>
      <span class="n">value</span> <span class="p">{</span>
        <span class="n">tensor</span> <span class="p">{</span>
          <span class="n">dtype</span><span class="p">:</span> <span class="n">DT_INT64</span>
          <span class="n">tensor_shape</span> <span class="p">{</span>
          <span class="p">}</span>
          <span class="n">int64_val</span><span class="p">:</span> <span class="mi">0</span>
        <span class="p">}</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="n">library</span> <span class="p">{</span>
    <span class="n">function</span> <span class="p">{</span>
      <span class="n">signature</span> <span class="p">{</span>
        <span class="n">name</span><span class="p">:</span> <span class="s">&quot;_make_dataset_YADYq1MF2s4&quot;</span>
        <span class="n">output_arg</span> <span class="p">{</span>
          <span class="n">name</span><span class="p">:</span> <span class="s">&quot;modeldataset&quot;</span>
          <span class="nb">type</span><span class="p">:</span> <span class="n">DT_VARIANT</span>
        <span class="p">}</span>
        <span class="n">description</span><span class="p">:</span> <span class="s">&quot;Factory function for a dataset.&quot;</span>
        <span class="n">is_stateful</span><span class="p">:</span> <span class="n">true</span>
      <span class="p">}</span>
      <span class="n">node_def</span> <span class="p">{</span>
        <span class="n">name</span><span class="p">:</span> <span class="s">&quot;RangeDataset/start&quot;</span>
        <span class="n">op</span><span class="p">:</span> <span class="s">&quot;Const&quot;</span>
        <span class="n">attr</span> <span class="p">{</span>
          <span class="n">key</span><span class="p">:</span> <span class="s">&quot;dtype&quot;</span>
          <span class="n">value</span> <span class="p">{</span>
            <span class="nb">type</span><span class="p">:</span> <span class="n">DT_INT64</span>
          <span class="p">}</span>
        <span class="p">}</span>
        <span class="n">attr</span> <span class="p">{</span>
          <span class="n">key</span><span class="p">:</span> <span class="s">&quot;value&quot;</span>
          <span class="n">value</span> <span class="p">{</span>
            <span class="n">tensor</span> <span class="p">{</span>
              <span class="n">dtype</span><span class="p">:</span> <span class="n">DT_INT64</span>
              <span class="n">tensor_shape</span> <span class="p">{</span>
              <span class="p">}</span>
              <span class="n">int64_val</span><span class="p">:</span> <span class="mi">0</span>
            <span class="p">}</span>
          <span class="p">}</span>
        <span class="p">}</span>
      <span class="p">}</span>
      <span class="n">ret</span> <span class="p">{</span>
        <span class="n">key</span><span class="p">:</span> <span class="s">&quot;modeldataset&quot;</span>
        <span class="n">value</span><span class="p">:</span> <span class="s">&quot;ModelDataset:handle:0&quot;</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="n">versions</span> <span class="p">{</span>
    <span class="n">producer</span><span class="p">:</span> <span class="mi">27</span>
    <span class="n">min_consumer</span><span class="p">:</span> <span class="mi">12</span>
  <span class="p">}</span>
</pre></div>


<p>在实际操作中，很少单独持久化Graph信息，一般都是直接保存MetaGraph信息，保存MetaGraph信息的文件格式主要有</p>
<ol>
<li>
<p>pd 格式<br />
形如<code>xxx_name.pd</code></p>
</li>
<li>
<p>meta 格式<br />
形如<code>xxx_name.meta</code></p>
</li>
</ol>
<h4 id="2_5">2.参数信息的保存</h4>
<p>参数信息持久化时保存为(1)索引和(2)数据 两部分。其中：<br />
1. 索引命名为: <code>xxx_name.index</code><br />
2. 数据命名为: <code>xxx_name.data</code></p>
<table>
<thead>
<tr>
<th>filename</th>
<th>类型</th>
<th>内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>model.ckpt-20.index</td>
<td>二进制文件</td>
<td>数据 index</td>
</tr>
<tr>
<td>model.ckpt-20.data-00000-of-00002</td>
<td>二进制文件</td>
<td>数据</td>
</tr>
</tbody>
</table>
<h4 id="_4">实践</h4>
<h5 id="tfsaver">使用tf.saver</h5>
<p>.meta 格式 保存 MetaGraph，</p>
<p>保存的文件包括如下</p>
<table>
<thead>
<tr>
<th align="right">文件名</th>
<th align="right">文件类型</th>
<th align="right">描述</th>
<th align="right"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">checkpoint</td>
<td align="right">文本文件</td>
<td align="right">可直接记事本打开，记录检查点信息</td>
<td align="right">model_checkpoint_path;all_model_checkpoint_path</td>
</tr>
<tr>
<td align="right">model.ckpt-0.meta</td>
<td align="right">二进制文件</td>
<td align="right">图结构</td>
<td align="right"></td>
</tr>
<tr>
<td align="right">model.ckpt-20.index</td>
<td align="right">二进制文件</td>
<td align="right">数据 index</td>
<td align="right"></td>
</tr>
<tr>
<td align="right">model.ckpt-20.data-00000-of-00002</td>
<td align="right">二进制文件</td>
<td align="right">数据</td>
<td align="right"></td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>文件的命名</strong> <br />
<strong>model.ckpt-20</strong><br />
mode：可变的文件名<br />
ckpt：文件格式<br />
-20： 代表 global_step =20 <br />
saver.save(sess, 'my-model', global_step=0) ==&gt; filename: 'my-model-0'<br />
saver.save(sess, 'my-model.ckpt', global_step=0) ==&gt; filename: 'my-model.ckpt-0'<br />
...<br />
saver.save(sess, 'my-model', global_step=1000) ==&gt; filename: 'my-model-1000'</p>
</blockquote>
<p>同时保存 <code>xxx.meta</code>, <code>xxx.index</code>,<code>xxx.data-yyy-of-ttt</code></p>
<p>其主要操作为<code>tf.saver</code></p>
<div class="hlcode"><pre><span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</pre></div>


<p>saver是一个tensorflow.python.training.saver.Saver 类 </p>
<p>saver的建立主要通过 tf.train 子类建立，有以下方式:</p>
<blockquote>
<ol>
<li>saver = tf.train.Saver()</li>
<li>saver = tf.train.import_meta_graph()<br />
返回MetaGraphDef里面的saver_def 或者None</li>
</ol>
</blockquote>
<p>参数</p>
<div class="hlcode"><pre><span class="k">class</span> <span class="nc">Saver</span><span class="p">()</span>
        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
                <span class="n">var_list</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">reshape</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                <span class="n">sharded</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                <span class="n">max_to_keep</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                <span class="n">keep_checkpoint_every_n_hours</span><span class="o">=</span><span class="mf">10000.0</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">restore_sequentially</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                <span class="n">saver_def</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">builder</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">defer_build</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                <span class="n">allow_empty</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                <span class="n">write_version</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">SaverDef</span><span class="o">.</span><span class="n">V2</span><span class="p">,</span>
                <span class="n">pad_step_number</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                <span class="n">save_relative_paths</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                <span class="n">filename</span><span class="o">=</span><span class="bp">None</span>
        <span class="p">)</span>
</pre></div>


<p>方法</p>
<div class="hlcode"><pre><span class="mf">1.</span> <span class="n">as_saver_def</span>
         <span class="err">返回一个</span> <span class="n">SaverDef</span> <span class="n">proto</span>
<span class="mf">2.</span> <span class="n">build</span> 
<span class="mf">3.</span> <span class="n">export_meta_graph</span>  <span class="err">返回一个</span><span class="n">MetaGraphDef</span>
<span class="mf">4.</span> <span class="n">from_proto</span>   <span class="err">从</span>  <span class="n">saver_def</span> <span class="err">返回以一个</span><span class="n">Saver</span> <span class="n">built</span>
<span class="mf">5.</span> <span class="n">recover_last_checkpoints</span> <span class="err">从错误中恢复</span> <span class="n">saver</span><span class="err">的初始状态</span>
<span class="mf">6.</span> <span class="n">restore</span>  <span class="o">--</span><span class="err">恢复先前保存的参数变量</span>
<span class="mf">7.</span> <span class="n">save</span> <span class="o">--</span> <span class="err">保存参数变量</span>

<span class="n">save</span><span class="p">(</span>
    <span class="n">sess</span><span class="p">,</span>
    <span class="n">save_path</span><span class="p">,</span>
    <span class="n">global_step</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">latest_filename</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">meta_graph_suffix</span><span class="o">=</span><span class="s">&#39;meta&#39;</span><span class="p">,</span>
    <span class="n">write_meta_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">write_state</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">strip_default_attrs</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>

<span class="mf">8.</span> <span class="n">set_last_checkpoints</span> <span class="o">--</span>
<span class="mf">9.</span> <span class="n">set_last_checkpoints_with_time</span>
<span class="mf">10.</span> <span class="n">to_proto</span><span class="err">。将</span>  <span class="n">Saver</span> <span class="err">转化成</span> <span class="n">a</span> <span class="n">SaverDef</span> <span class="n">protocol</span> <span class="nb">buffer</span><span class="o">.</span>
</pre></div>


<h5 id="pd">.pd 格式</h5>
<ol>
<li>通过 get_default_graph().as_graph_def() 得到当前图的计算节点（Node）信息</li>
</ol>
<div class="hlcode"><pre><span class="n">graph</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
<span class="n">graph_def</span><span class="o">=</span><span class="n">graph</span><span class="o">.</span><span class="n">as_graph_def</span><span class="p">()</span>
</pre></div>


<ol>
<li>通过 graph_util.convert_variables_to_constants 将相关节点的values固定值</li>
</ol>
<div class="hlcode"><pre><span class="n">var_list</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">()</span>
<span class="n">constant_graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">graph_util</span><span class="o">.</span><span class="n">convert_variables_to_constants</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">sess</span><span class="o">.</span><span class="n">graph_def</span><span class="p">,</span> <span class="p">[</span><span class="n">var_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">name</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">var_list</span><span class="p">))])</span>
</pre></div>


<ol>
<li>通过 tf.gfile.GFile 进行模型持久化.保存trainable variables到.pb文件：</li>
</ol>
<div class="hlcode"><pre>        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">FastGFile</span><span class="p">(</span><span class="n">pb_path</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">constant_graph</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">())</span>
</pre></div>


<h5 id="pd-ckpt">pd 格式/ckpt格式文件互换</h5>
<div class="hlcode"><pre><span class="k">def</span> <span class="nf">freeze_graph</span><span class="p">(</span><span class="n">input_checkpoint</span><span class="p">,</span><span class="n">output_graph</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    :param input_checkpoint:</span>
<span class="sd">    :param output_graph: PB模型保存路径</span>
<span class="sd">    :return:</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c"># checkpoint = tf.train.get_checkpoint_state(model_folder) #检查目录下ckpt文件状态是否可用</span>
    <span class="c"># input_checkpoint = checkpoint.model_checkpoint_path #得ckpt文件路径</span>

    <span class="c"># 指定输出的节点名称,该节点名称必须是原模型中存在的节点</span>
    <span class="n">output_node_names</span> <span class="o">=</span> <span class="s">&quot;InceptionV3/Logits/SpatialSqueeze&quot;</span>
    <span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">import_meta_graph</span><span class="p">(</span><span class="n">input_checkpoint</span> <span class="o">+</span> <span class="s">&#39;.meta&#39;</span><span class="p">,</span> <span class="n">clear_devices</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
        <span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">input_checkpoint</span><span class="p">)</span> <span class="c">#恢复图并得到数据</span>
        <span class="n">output_graph_def</span> <span class="o">=</span> <span class="n">graph_util</span><span class="o">.</span><span class="n">convert_variables_to_constants</span><span class="p">(</span>  <span class="c"># 模型持久化，将变量值固定</span>
            <span class="n">sess</span><span class="o">=</span><span class="n">sess</span><span class="p">,</span>
            <span class="n">input_graph_def</span><span class="o">=</span><span class="n">sess</span><span class="o">.</span><span class="n">graph_def</span><span class="p">,</span><span class="c"># 等于:sess.graph_def</span>
            <span class="n">output_node_names</span><span class="o">=</span><span class="n">output_node_names</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&quot;,&quot;</span><span class="p">))</span><span class="c"># 如果有多个输出节点，以逗号隔开</span>

        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">GFile</span><span class="p">(</span><span class="n">output_graph</span><span class="p">,</span> <span class="s">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span> <span class="c">#保存模型</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">output_graph_def</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">())</span> <span class="c">#序列化输出</span>
        <span class="k">print</span><span class="p">(</span><span class="s">&quot;</span><span class="si">%d</span><span class="s"> ops in the final graph.&quot;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_graph_def</span><span class="o">.</span><span class="n">node</span><span class="p">))</span> <span class="c">#得到</span>
</pre></div>


<h3 id="2-savedmodel">2. 以 SavedModel形式保存上线模型</h3>
<p>SavedModel：使用saved_model接口导出的模型文件，包含模型Graph和权限可直接用于上线，TensorFlow和Keras模型推荐使用这种模型格式。</p>
<p><img alt="" src="/attach/images/2019-07-15-17-16-14.png" /></p>
<p>例如，图显示了一个包含三个 MetaGraphDef 的 SavedModel，它们三个都共享同一组检查点和资源：</p>
<p>SavedModel 是一种独立于语言且可恢复的神秘序列化格式，使较高级别的系统和工具可以创建、使用和转换 TensorFlow 模型。</p>
<div class="hlcode"><pre><span class="err">构建</span><span class="n">builder</span><span class="err">方法如下：</span>
<span class="n">builder</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">saved_model</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">SavedModelBuilder</span><span class="p">(</span><span class="s">&quot;/home/xsr-ai/study/mnist/saved-model&quot;</span><span class="p">)</span>
<span class="err">在训练完后，我们调用如下命令保存模型：</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_meta_graph_and_variables</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">saved_model</span><span class="o">.</span><span class="n">tag_constants</span><span class="o">.</span><span class="n">TRAINING</span><span class="p">],</span> <span class="n">signature_def_map</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">assets_collection</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">add_meta_graph_and_variables</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span><span class="n">tags</span><span class="p">,</span><span class="n">signature_def_map</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">assets_collection</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">legacy_init_op</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">clear_devices</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">main_op</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>


<span class="n">sess</span><span class="err">：用于执行添加元图和变量功能的会话；</span>
<span class="n">tags</span><span class="err">：用于保存元图的标签；</span>
<span class="n">signature_def_map</span><span class="err">：用于保存元图的签名；</span>
<span class="n">assets_collection</span><span class="err">：使用</span><span class="n">SavedModel</span><span class="err">保存的资源集合；</span>
<span class="n">legacy_init_op</span><span class="err">：在恢复模型操作后，对</span><span class="n">Op</span><span class="err">和</span><span class="n">Ops</span><span class="err">组的遗留支持；</span>
<span class="n">clear_devices</span><span class="err">：如果默认图形上的设备信息应该被清除，则应该设置为</span><span class="n">true</span><span class="err">；</span>
<span class="n">main_op</span><span class="err">：在加载图时执行</span><span class="n">Op</span><span class="err">或</span><span class="n">Ops</span><span class="err">组的操作。请注意，当</span><span class="n">main_op</span><span class="err">被指定时，它将在加载恢复</span><span class="n">op</span><span class="err">后运行；</span>
</pre></div>


<h3 id="3_3">3. 保存部署在移动设备上的轻量级模型</h3>
<p>FrozenGraph：使用freeze_graph.py对checkpoint和GraphDef进行整合和优化，可以直接部署到Android、iOS等移动设备上。<br />
TFLite：基于flatbuf对模型进行优化，可以直接部署到Android、iOS等移动设备上，使用接口和FrozenGraph有些差异</p>
<h3 id="_5">实践</h3>
<h2 id="4">4. 怎么加载</h2>
<h4 id="2-pd">2 pd 文件加载</h4>
<ol>
<li>
<p>tf.io.gfile.GFile 打开pb文件</p>
<div class="hlcode"><pre><span class="n">model_f</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">io</span><span class="p">.</span><span class="n">gfile</span><span class="p">.</span><span class="n">GFile</span><span class="p">(</span><span class="s">&quot;./Test/model.pb&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="err">&#39;</span><span class="n">rb</span><span class="err">&#39;</span><span class="p">)</span>
<span class="cp"># 无线程锁定的 I/O 处理</span>
<span class="n">tensorflow</span> <span class="o">-&gt;</span><span class="n">i</span><span class="o">/</span><span class="n">o</span><span class="o">-&gt;</span>
</pre></div>


</li>
<li>
<p>将pb文件输入到graph_def类中</p>
</li>
</ol>
<p>2.1 创建一个空的graph_def 类</p>
<div class="hlcode"><pre>    <span class="n">graph_def</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">GraphDef</span><span class="p">()</span>
</pre></div>


<p>2.2 将 MetaGraph的二进制文件调入 空的graph_def，加载节点(Node)信息</p>
<div class="hlcode"><pre>    <span class="n">graph_def</span><span class="p">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">model_f</span><span class="p">.</span><span class="n">read</span><span class="p">())</span>  <span class="err">##二进制调用实际上是</span> <span class="n">ParseFromString</span>
</pre></div>


<ol>
<li>
<p>将graph_def调入到现在的graph 中</p>
<div class="hlcode"><pre><span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">graph_util</span><span class="p">.</span><span class="n">import_graph_def</span><span class="p">(</span><span class="n">graph_def</span><span class="p">,</span> <span class="n">return_elements</span><span class="o">=</span><span class="p">[</span><span class="s">&quot;add2:0&quot;</span><span class="p">])</span>
<span class="err">通过</span><span class="n">return_elements</span> <span class="err">确定返回的</span> <span class="n">op</span> <span class="o">/</span><span class="n">tensor</span>
</pre></div>


</li>
<li>
<p>获得op/tensor ，可以运行</p>
<div class="hlcode"><pre><span class="n">print</span><span class="p">(</span><span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
</pre></div>


</li>
</ol>
<h5 id="1_4">1.保存</h5>
<div class="hlcode"><pre><span class="c">#1. 创建saver</span>

<span class="n">saver</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>

<span class="c"># 2. 保存参数变量</span>
<span class="n">saver_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">save_path</span><span class="o">=</span><span class="s">&quot;./path/model.ckpt&quot;</span> <span class="p">,</span><span class="n">global_step</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="c">#1 sess 必须提前加载，同时参数没有初始化</span>
<span class="c">#2 save_path : 返回 str </span>
</pre></div>


<h5 id="2_6">2.加载</h5>
<ol>
<li>加载持久化图<div class="hlcode"><pre><span class="n">saver</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">import_meta_graph</span><span class="p">(</span><span class="err">“</span><span class="n">save</span><span class="o">/</span><span class="n">model</span><span class="p">.</span><span class="n">ckpt</span><span class="p">.</span><span class="n">meta</span><span class="err">”</span><span class="p">)</span>
</pre></div>


</li>
</ol>
<p>注意：<br />
如果图中使用了来自 tf.contrib.的operation 需要在加载图前 使用 tf.contrib.resampler</p>
<div class="hlcode"><pre>    <span class="n">import</span> <span class="n">tensorflow</span> <span class="n">as</span> <span class="n">tf</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">resampler</span>
    <span class="n">saver</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">import_meta_graph</span><span class="p">(</span><span class="err">“</span><span class="n">save</span><span class="o">/</span><span class="n">model</span><span class="p">.</span><span class="n">ckpt</span><span class="p">.</span><span class="n">meta</span><span class="err">”</span><span class="p">)</span>
</pre></div>


<ol>
<li>加载保存的参数</li>
</ol>
<div class="hlcode"><pre><span class="n">dir_path</span><span class="o">=</span><span class="s">&quot;./model&quot;</span> <span class="c"># 文件夹地址</span>
<span class="n">model_file</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">(</span><span class="n">dir_path</span><span class="p">)</span>
<span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span><span class="n">model_file</span><span class="p">)</span>

<span class="c"># sess 必须提前加载，同时参数没有初始化，因为restore 方法本身就是一个初始化的过</span>
</pre></div>


<h1 id="6-tfsummary">6. 可视化（tf.summary）</h1>
<p>TensorBoard--tensorflow的可视化结构管理工具</p>
<h3 id="1_5">1. 得到需要可视化的数据</h3>
<p>使用tf.summary类下的子类机以实现如下功能：<br />
1. 将输入的Tensor信息 生成Tensor.proto格式的二进制文件<br />
2. 返回dtype=tf.string,shape=() ,内容为Tensor.proto格式的二进制文件信息（Summary protobuf）的Tensor</p>
<h4 id="11">1.1 标量</h4>
<div class="hlcode"><pre><span class="c"># 1. 记录 标量</span>
<span class="n">scalar_summary</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">collections</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="c"># tensor: 标量 /一个数 A real numeric Tensor containing a single value</span>
<span class="nb">type</span><span class="p">(</span><span class="n">scalar_summary</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tensorflow</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">framework</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span>
<span class="n">scalar_summary</span><span class="o">.</span><span class="n">dtype</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">string</span>
<span class="n">scalar_summary</span><span class="o">.</span><span class="n">shape</span>
<span class="o">&gt;&gt;&gt;</span><span class="p">()</span>
</pre></div>


<h4 id="12">1.2 图片</h4>
<div class="hlcode"><pre><span class="c"># 2. 记录 图片</span>
<span class="n">image_summary</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">image</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">max_outputs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">collections</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="c"># tensor：tensor dtype=`uint8` or `float32`;shape=[batch_size, height, width, channels],其中`channels` 只能是 1, 3, or 4.</span>
</pre></div>


<h4 id="13">1.3 音频</h4>
<div class="hlcode"><pre><span class="n">audio_summary</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">audio</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">collections</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span> 
<span class="c"># tensor: A 3-D `float32` `Tensor` of shape `[batch_size, frames, channels]`or a 2-D `float32` `Tensor` of shape `[batch_size, frames]`</span>
</pre></div>


<h4 id="14">1.4 文本</h4>
<div class="hlcode"><pre><span class="n">text_summary</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">collections</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="c"># tensor: a string-type Tensor to summarize.</span>
</pre></div>


<h4 id="15">1.5 直方图数据</h4>
<div class="hlcode"><pre><span class="n">histogram_summary</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">collections</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="c">#values: 数字类型的Tensor </span>
<span class="c"># A real numeric `Tensor`. Any shape. Values to use to build the histogram.</span>
</pre></div>


<h4 id="16">1.6 分布图数据</h4>
<p>(未确定)</p>
<div class="hlcode"><pre><span class="n">distribution_summary</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">distribution</span> <span class="p">(</span><span class="err">记录</span> <span class="err">数据的分布图</span><span class="p">)</span>
</pre></div>


<h4 id="17-">1.7 快捷操作--所有可视化数据</h4>
<p>上面的每一个op 都是 构建图的一部分，没有会话的执行sess.run 都不会计算</p>
<p>为了会话计算方便，可以把上面所有在默认图中的 summaries op 合并为一个</p>
<div class="hlcode"><pre><span class="n">summary_op</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">merge_all</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="nb">type</span><span class="p">(</span><span class="n">summary_op</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">framework</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span>
</pre></div>


<h3 id="2_7">2  将输出的数据都保存到本地磁盘中</h3>
<p>这是一个命令 不需要 sess run </p>
<div class="hlcode"><pre><span class="n">filewriter</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">FileWriter</span><span class="p">(</span><span class="n">logdir</span><span class="p">,</span> <span class="n">graph</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">max_queue</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">flush_secs</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="n">graph_def</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">filename_suffix</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span><span class="nb">type</span><span class="p">(</span><span class="n">filewriter</span><span class="p">)</span>
<span class="n">tensorflow</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">FileWriter</span>
</pre></div>


<h1 id="7-tfmetrics">7. 评价模型的优劣（tf.metrics）</h1>
<h3 id="71-tfmetrics">7.1 tf.metrics 类</h3>
<h4 id="1tfmetrics">1.tf.metrics 类的通用特性</h4>
<ol>
<li><strong>需要初始化 LOCAL_VARIABLES</strong>。</li>
</ol>
<p>计算时会创建几个变量，变量会加入tf.GraphKeys.LOCAL_VARIABLES集合中，所以需要初始化 LOCAL_VARIABLES。<br />
   如果不重新初始化这些 LOCAL_VARIABLES，计算的tf.metrics的几个本地变量将持续更新，不随着一个batch的结束而结束。</p>
<div class="hlcode"><pre><span class="n">init_local_variables</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initialize_local_variables</span><span class="p">()</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span> <span class="p">(</span><span class="n">init_local_variables</span><span class="p">)</span>
</pre></div>


<ol>
<li>返回两个tensorflow op。</li>
</ol>
<div class="hlcode"><pre><span class="c"># accuracy（相当于calculate_accuracy()）</span>
<span class="c"># update_op（相当于update_running_variables()）</span>
<span class="n">accuracy</span><span class="p">,</span> <span class="n">update_op</span> <span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">XXX</span>
</pre></div>


<h4 id="2-tfmetrics">2. tf.metrics 的执行过程</h4>
<p>以 tf.metrics.accuracy(predictions,labels )为例：<br />
1. 构建2个本地变量 LOCAL_VARIABLES: total 和 count<br />
2. 本地变量 total 和 count 初始化设置为 0<br />
3. 当执行 accuracy_op 的时候，只是运行除法 0/0 ==0<br />
4. 当执行 accuracy_update_op 的时候, 根据输入的predictions,labels的情况更新total 和count变化，并运行除法</p>
<h4 id="3-tfmetrics-tfloss">3. tf.metrics 与 tf.loss 的区别</h4>
<ol>
<li>tf.loss 及其子类用于在反向传播期间更新模型。 tf.metrics用于评估模型。</li>
<li>tf.loss 是直接针对输入到 tf.loss.xxx(predictions,labels) 的predictions,labels进行计算； tf.metrics是根据输入到tf.metrics.xxx(predictions,labels)的predictions,labels情况，将其添加到 从上一个本地变量初始化开始的数据集，根据更新的数据集进行计算。</li>
</ol>
<h3 id="72">7.2 如何评价一个模型</h3>
<h4 id="1-error-">1 误差 error--离散度的测量</h4>
<h5 id="1-mae">1. 平均绝对误差 MAE</h5>
<p>平均绝对误差 MAE,也称L1损失</p>
<p>$$AE(y_ ,y)=|y_ -y|$$</p>
<p>$$MAE(y_ ,y)=\frac{1}{N}\sum\ |y_ -y|$$</p>
<p><img alt="" src="../../../../attach/images/2019-08-29-10-36-45.png" /></p>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

<span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_absolute_error</span><span class="p">(</span>
    <span class="n">labels</span><span class="p">,</span>
    <span class="n">predictions</span><span class="p">,</span>
    <span class="n">weights</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">metrics_collections</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">updates_collections</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="bp">None</span>
<span class="p">)</span>
</pre></div>


<p>Computes the mean absolute error between the labels and predictions.</p>
<h5 id="2-mre">2. 平均相对误差 MRE</h5>
<p>相对误差 Relative Error (RE)<br />
$$RE(y_ ,y)=\frac{|y_ -y|}{y}$$</p>
<p>平均相对误差 Mean Relative Error (MRE)，也称为L2损失<br />
$$MRE(y_ ,y)=\frac{1}{N}\sum\ RE(y_ ,y)=\frac{1}{N}\sum\frac{|y_ -y|}{y}$$</p>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_relative_error</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>


<p>Computes the mean relative error by normalizing with the given values.</p>
<h5 id="3-mse">3. 平方差（方差）MSE</h5>
<p>平方误差 squared_error  (SE)<br />
$$SE(y_ ,y)=(y_ -y)^2$$</p>
<p>平均相对误差 Mean Relative Error (MRE)，也称为L2损失<br />
$$MRE(y_ ,y)=\frac{1}{N}\sum\ SE(y_ ,y)=\frac{1}{N}\sum(y_ -y)^2$$</p>
<p><img alt="" src="../../../../attach/images/2019-08-29-10-37-13.png" /></p>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
</pre></div>


<p>Computes the mean squared error between the labels and predictions.</p>
<h5 id="4-rmse">4. 均方差(标准差) RMSE</h5>
<p>$$MRE(y_ ,y)=\frac{1}{N}\sqrt{\sum(y_ -y)^2} $$</p>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">root_mean_squared_error</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
</pre></div>


<p>Computes the root mean squared error between the labels and predictions.</p>
<h5 id="5_1">5. 余弦相似性</h5>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_cosine_distance</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
</pre></div>


<p>Computes the cosine distance between the labels and predictions.</p>
<h4 id="2_8">2 混合矩阵</h4>
<p><img alt="avatar" src="https://upload-images.jianshu.io/upload_images/7252179-dbad746fab87dc42.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/646/format/webp" /></p>
<h5 id="1-accuracy">1. 准确率 (accuracy)</h5>
<p>$$ accuracy =\frac{正确预测的数量}{样本总数} $$</p>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy</span><span class="p">()</span>


<span class="n">real_label</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">]])</span> <span class="c"># [1] [1] [1]</span>
<span class="n">predict_label</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span> <span class="c"># [1] [1] [1]</span>

<span class="n">accuracy_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">real_label</span><span class="p">,</span> <span class="n">predictions</span><span class="o">=</span><span class="n">predict_label</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&quot;accuracy_op&quot;</span><span class="p">)</span>
<span class="n">ini_op</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">(),</span><span class="n">tf</span><span class="o">.</span><span class="n">local_variables_initializer</span><span class="p">()]</span>
<span class="n">sess</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">ini_op</span><span class="p">)</span>
<span class="n">accuracy</span><span class="p">,</span><span class="n">update_op</span><span class="o">=</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">accuracy_op</span><span class="p">)</span>
<span class="c"># (1.0, 1.0)</span>
<span class="c"># accuracy：A Tensor，表示准确性，值total除以count。</span>
<span class="c"># update_op ：适当增加total和count变量并且使其值匹配accuracy的操作。</span>
</pre></div>


<h5 id="2-precision">2. 精确率 (precision)</h5>
<p>模型正确预测正类别的频率<br />
   你认为的正样本，有多少猜对了（猜的准确性如何）</p>
<blockquote>
<p>正类别：在二元分类中，两种可能的类别分别被标记为正类别和负类别。正类别结果是我们要测试的对象。例如，在医学检查中，正类别可以是“肿瘤”</p>
</blockquote>
<p>$$precision=\frac{正例数}{(正例数+假正例数)}$$</p>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">precision</span><span class="p">()</span>
</pre></div>


<h5 id="3-recall">3. 召回率 (recall)</h5>
<div class="hlcode"><pre><span class="err">正样本有多少被找出来了（召回了多少）。</span>
</pre></div>


<p>在所有可能的正类别标签中，模型正确地识别出了多少个？即<br />
   $$recall=\frac{正例数}{(正例数+假负例数)}$$</p>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">recall</span><span class="p">()</span>
</pre></div>


<h5 id="4-auc">4. AUC</h5>
<p>AUC：ROC 曲线下面积 (AUC, Area under the ROC Curve)</p>
<blockquote>
<p>受试者工作特征曲线（receiver operating characteristic，简称 ROC 曲线）</p>
</blockquote>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">auc</span><span class="p">()</span>
<span class="c"># Computes the approximate AUC via a Riemann sum.</span>
</pre></div>


<h5 id="5-sensitivity">5. 敏感度(sensitivity)</h5>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_iou</span><span class="p">()</span>
</pre></div>


<h5 id="6-specificity">6. 特异度(specificity)</h5>
<p>sensitivity_at_specificity(...): Computes the specificity at a given sensitivity.</p>
<h5 id="7">7. 各种数量</h5>
<p>false_negatives(...):<br />
 Computes the total number of false negatives.<br />
false_positives(...): <br />
Sum the weights of false positives.<br />
true_negatives(...): <br />
Sum the weights of true_negatives.<br />
true_positives(...): </p>
<p>Sum the weights of true_positives.</p>
<h4 id="3_4">3 基本统计量</h4>
<p>mean_iou(...):</p>
<p>Calculate per-step mean Intersection-Over-Union (mIOU).</p>
<h5 id="1_6">1. 平均数</h5>
<p>mean(...): Computes the (weighted) mean of the given values.</p>
<h5 id="2_9">2. 百分百</h5>
<p>percentage_below(...): Computes the percentage of values less than the given threshold.</p>
<h1 id="8-scope">8. 代码的可读性--作用域(scope)</h1>
<h2 id="tfname_scope">tf.name_scope</h2>
<p>-(tf.name_scope/tf.variable_scope)</p>
<p>TensorFlow doesn’t know what nodes should be grouped together, unless you tell it to<br />
TensorFlow 并不知道那个node 需要本整合<br />
为了解决这个问题，我们引入了 <code>name_scope</code> 和 <code>variable_scope，</code> 二者又分别承担着不同的责任：</p>
<ul>
<li>name_scope: * 为了更好地管理变量的命名空间而提出的。比如在 tensorboard 中，因为引入了 name_scope， 我们的 Graph 看起来才井然有序,name_scope主要是给variable_name加前缀，也可以给op_name 加前缀；。</li>
<li>variable_scope: * 大大大部分情况下，跟 tf.get_variable() 配合使用，实现变量共享的功能,，name_scope 是给 op_name 加前缀。</li>
</ul>
<p>这两个函数在大部分情况下是等价的, 唯一的区别是在使用tf.get_variable函数时. </p>
<div class="hlcode"><pre><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">&quot;foo&quot;</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">&quot;bar&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">print</span> <span class="n">a</span><span class="o">.</span><span class="n">name</span>    <span class="c"># 输出 foo/bar: 0</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">&quot;bar&quot;</span><span class="p">):</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">&quot;bar&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">print</span> <span class="n">b</span><span class="o">.</span><span class="n">name</span>     <span class="c"># 输出 bar/bar: 0</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">&quot;a&quot;</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">print</span> <span class="n">a</span><span class="o">.</span><span class="n">name</span>     <span class="c"># 输出 a/Va        　　    a = tf.Variable(&quot;b&quot;, [1]):</span>
　　    <span class="k">print</span> <span class="n">a</span><span class="o">.</span><span class="n">name</span>　<span class="c"># 输出 b: 0</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">&quot;b&quot;</span><span class="p">):</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">&quot;b&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span>        <span class="c"># Error</span>
</pre></div>


<h2 id="tensorop">Tensor/op 改名</h2>
<div class="hlcode"><pre><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">&quot;abc&quot;</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&quot;z&quot;</span><span class="p">)</span>
</pre></div>


<div class="hlcode"><pre><span class="n">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">&quot;abc&quot;</span><span class="p">)</span> <span class="n">as</span> <span class="n">scope</span><span class="o">:</span>
    <span class="err">#</span> <span class="n">z</span> <span class="n">will</span> <span class="n">get</span> <span class="n">the</span> <span class="n">name</span> <span class="s">&quot;abc&quot;</span><span class="p">.</span> <span class="n">x</span> <span class="n">and</span> <span class="n">y</span> <span class="n">will</span> <span class="n">have</span> <span class="n">names</span> <span class="n">in</span> <span class="s">&quot;abc/...&quot;</span> <span class="k">if</span> <span class="n">they</span>
    <span class="err">#</span> <span class="n">are</span> <span class="n">converted</span> <span class="n">to</span> <span class="n">tensors</span><span class="p">.</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>
</pre></div>


<h1 id="9-tflosses">9. 损失函数 tf.losses</h1>
<p><strong>损失函数</strong>是衡量由特征值x经过模型f得到的预测值y_=f(x)与真实值y的差距，是衡量预测错误程度的指标</p>
<p>函数的问题最终还是要归结于 任务类型，是处理 predict_label 和 real_label 的问题</p>
<p>单个样本：成本函数（Loss Function）<br />
多个样本：成本函数（Cost Function）</p>
<p>目标：在有约束条件下的最小化成本函数（Cost Function）</p>
<ol>
<li>离散数据</li>
<li>分类<ol>
<li>二分类</li>
<li>多分类</li>
</ol>
</li>
<li>连续数据</li>
<li>回归</li>
</ol>
<p>tf.loss 类计算的都是平均值</p>
<p><img alt="" src="../../../../attach/images/2019-09-24-11-01-59.png" /></p>
<h3 id="91-0-1">9.1. 0-1函数</h3>
<p>$$<br />
Loss(y_ ,y)=\begin{cases}<br />
1   (y_!=y)\<br />
0   (y_==y)\<br />
\end{cases}<br />
$$<br />
<strong>手动：</strong></p>
<div class="hlcode"><pre><span class="c"># real_label shape =(N,···)</span>
<span class="c"># predict_label shape =(N,···)</span>
<span class="c"># predict_label.shape==real_label.shape</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">real_label</span><span class="p">,</span> <span class="n">predict_label</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="c"># loss shape =(N,1)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="c"># cost shape= ()  is a  val 数字</span>
</pre></div>


<p><strong>封装：</strong></p>
<div class="hlcode"><pre><span class="c"># 注意正确率的定义中 相等以及不等 相反,函数为不正确率 </span>
<span class="n">cost</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">-</span><span class="n">accuracy</span><span class="p">)</span>
</pre></div>


<p><strong>应用：</strong><br />
感知机</p>
<p>但是由于0-1损失函数相等这个条件太过严格，因此我们可以放宽条件，即满足时认为相等</p>
<p>$$Loss(y_ ,y)=\begin{cases}<br />
1 (|y_ -y|&gt;=error)\<br />
0 (|y_ -y|&lt;error)\<br />
\end{cases}<br />
$$</p>
<h3 id="92">9.2 绝对值损失</h3>
<p>$$Loss(y_ ,y)=|y_ -y|$$</p>
<p><strong>手动：</strong></p>
<div class="hlcode"><pre><span class="c"># real_label shape =(N,···)</span>
<span class="c"># predict_label shape =(N,···)</span>
<span class="c"># predict_label.shape==real_label.shape</span>
<span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">real_label</span><span class="p">,</span> <span class="n">predict_label</span><span class="p">))</span>
<span class="c"># loss shape =(N,···)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">keep_dims</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">reduction_indices</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="c"># cost shape= ()  is a  val 数字</span>
</pre></div>


<p><strong>封装：</strong></p>
<div class="hlcode"><pre><span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">absolute_difference</span><span class="p">(</span>
    <span class="n">labels</span><span class="p">,</span>
    <span class="n">predictions</span><span class="p">,</span>
    <span class="n">weights</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">scope</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">loss_collection</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">LOSSES</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="n">Reduction</span><span class="o">.</span><span class="n">SUM_BY_NONZERO_WEIGHTS</span>
<span class="p">)</span>
</pre></div>


<h3 id="93-log">9.3 log对数函数</h3>
<p><strong>log对数函数</strong>，又称为 <strong>对数似然函数</strong> ，是对<strong>单个样本</strong>的描述，表示为<br />
$$Loss(y,P(y|x))$$</p>
<p>是关于<strong>实际值y</strong>，与 特征值x下的<strong>后验概率P(y|x)</strong> 的函数，<strong>log对数</strong>等于<strong>后验概率</strong>的对数。这个函数的值通过下面的<strong>log函数的标准形式</strong>计算：</p>
<p>$$Loss(y,P(y|x))=-\log{P(y|x)}$$</p>
<p>$$Coss(y,P(y|x))=-\frac{1}{N} \sum{log{P(y|x)}}$$</p>
<p>“似然性”与“或然性”或“概率”意思相近，都是指某种事件发生的可能性，但是在统计学中，“似然性”和“或然性”或“概率”又有明确的区分。</p>
<p><strong>概率</strong>用于在已知一些参数$x$的情况下，预测接下来的观测所得到的结果y;  $P(y|x)$</p>
<p><strong>似然性</strong>则是用于在已知某些观测所得到的结果y时，对有关事物的性质的参数x进行估计。$L(x|y)$</p>
<p>给定输出y时，关于参数的x似然函数$L(x|y)$（在数值上）等于给定参数x后变量y的概率$P(y|x)$：<br />
$$ L(x|y) = P(y|x)$$</p>
<p><strong>关于后验概率P(y|x)</strong></p>
<p>在神经网络模型中，通常将经过激活函数的(0,1)范围的输出层值作为作为<strong>后验概率P(y|x)</strong></p>
<p>$$P(y|x)=sigmod(Wx+b)$$</p>
<h4 id="_6">对于二分类问题</h4>
<p>二分类问题的后验概率P(label=0|x)（当输入为x时，label=0的概率）,可知</p>
<p>$$P(label=0|x)=1-P(label=1|x)$$<br />
若令$y=P(label=0|x)$,则 正样本（label=0）的<strong>对数机率</strong> z<br />
$$ln\frac{y}{1-y}=ln\frac{P(label=0|x)}{P(label=1|x)}=z$$</p>
<p>依据输入特征x，确定该特征对应的label=0的概率为$P(label=0|x)$</p>
<p>$$P(label=0|x)=\frac{1}{1+e^{-z}}=\frac{e^z}{1+e^{z}}$$</p>
<p>依据输入特征x，确定该特征对应的label=1的概率为$P(label=1|x)$，<br />
$$P(label=1|x)=1-P(label=0|x)=\frac{1}{1+e^{z}}$$</p>
<p>即 <br />
$$P(label∣x)=\begin{cases}<br />
\frac{e^z}{1+e^{z}}=\frac{e^{wx+b}}{1+e^{wx+b}}, label=0\<br />
\frac{1}{1+e^{z}}=\frac{1}{1+e^{wx+b}}, label=1\<br />
\end{cases}<br />
$$</p>
<p>$$Loss(y,P(y|x))=-\log {P(y|x)}=\begin{cases}<br />
-\log{\frac{e^z}{1+e^{z}}}, y=0\<br />
-\log{\frac{1}{1+e^{z}}}, y=1\<br />
\end{cases}<br />
$$</p>
<p><strong>手动：</strong></p>
<div class="hlcode"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c"># y_true ==labels</span>
<span class="c"># y_pred ==predictions</span>

<span class="k">def</span> <span class="nf">logcoss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-15</span><span class="p">):</span>
    <span class="c"># Prepare numpy array data</span>
    <span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span> 
    <span class="k">assert</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">))</span>
    <span class="c"># Clip y_pred between eps and 1-eps</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">eps</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span> <span class="n">y_true</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">))</span>
    <span class="n">cost</span><span class="o">=</span><span class="n">loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cost</span>
</pre></div>


<p><strong>封装：</strong></p>
<p>$$logloss=weights\times(labels\times \log{(predictions+epsilon)} + (1-labels)* \log{(1-predictions+epsilon)}) $$</p>
<p>$$logloss=W\times(L\times \log{(P+e)} + (1-L)\times\log{(1-P+e)}) $$</p>
<div class="hlcode"><pre><span class="n">log_loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">log_loss</span><span class="p">(</span>
    <span class="n">labels</span><span class="p">,</span>
    <span class="n">predictions</span><span class="p">,</span> 
    <span class="n">weights</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-07</span><span class="p">,</span> 
    <span class="n">scope</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">loss_collection</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">LOSSES</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="n">Reduction</span><span class="o">.</span><span class="n">SUM_BY_NONZERO_WEIGHTS</span>
<span class="p">)</span>
<span class="c"># 注意</span>
<span class="n">If</span> <span class="n">reduction</span> <span class="ow">is</span> <span class="n">NONE</span><span class="p">,</span> <span class="n">this</span> <span class="n">has</span> <span class="n">the</span> <span class="n">same</span> <span class="n">shape</span> <span class="k">as</span> <span class="n">labels</span><span class="p">;</span> <span class="n">otherwise</span><span class="p">,</span> <span class="n">it</span> <span class="ow">is</span> <span class="n">scalar</span><span class="o">.</span>


<span class="n">log_loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">log_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span><span class="n">y_predict</span><span class="p">,</span><span class="n">reduction</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="nb">type</span><span class="p">(</span><span class="n">log_loss</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Tensor</span> 
<span class="n">log_loss_results</span><span class="o">=</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">log_loss</span><span class="p">)</span>
<span class="c"># log_loss_results.shape =y_true.shape </span>
<span class="n">log_cost</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">log_loss</span><span class="p">)</span>


<span class="n">log_loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">log_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span><span class="n">y_predict</span><span class="p">)</span>
<span class="nb">type</span><span class="p">(</span><span class="n">log_loss</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Tensor</span> 
<span class="n">log_loss_results</span><span class="o">=</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">log_loss</span><span class="p">)</span>
<span class="c"># log_loss_results 标量</span>

<span class="nb">type</span><span class="p">(</span><span class="n">log_cost</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="err">标量</span>
</pre></div>


<p><strong>应用</strong><br />
Logistic回归</p>
<h3 id="94">9.4 平方函数</h3>
<p>$$loss(y_,y)=\frac{1}{N}\sum_{i=1}^{N}{(y_-y)^2}$$</p>
<p><strong>手动：</strong></p>
<div class="hlcode"><pre><span class="c"># real_label shape =(N,···)</span>
<span class="c"># predict_label shape =(N,···)</span>
<span class="c"># predict_label.shape==real_label.shape</span>
<span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">real_label</span><span class="p">,</span> <span class="n">predict_label</span><span class="p">))</span>
<span class="c"># loss shape =(N,···)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">keep_dims</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">reduction_indices</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="c"># cost shape= ()  is a  val 数字</span>
</pre></div>


<p><strong>封装：</strong></p>
<div class="hlcode"><pre><span class="n">mean_squared_error</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span>
    <span class="n">labels</span><span class="p">,</span>
    <span class="n">predictions</span><span class="p">,</span>
    <span class="n">weights</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">scope</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">loss_collection</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">LOSSES</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="n">Reduction</span><span class="o">.</span><span class="n">SUM_BY_NONZERO_WEIGHTS</span>
<span class="p">)</span>
<span class="c"># mean_squared_error shape =() is a value</span>

<span class="c"># 平方函数实际为 均方差（MSE）</span>
<span class="n">mean_squared_error</span><span class="p">,</span><span class="n">update_op</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span>
    <span class="n">labels</span><span class="p">,</span>
    <span class="n">predictions</span><span class="p">,</span> <span class="c">#　predictions　为predict＿label</span>
    <span class="n">weights</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">metrics_collections</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">updates_collections</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="bp">None</span>
<span class="p">)</span>
<span class="c"># mean_squared_error shape =() is a value</span>
</pre></div>


<p><strong>示例</strong></p>
<div class="hlcode"><pre><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]])</span>

<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>     
<span class="n">mse_op</span><span class="p">,</span> <span class="n">mse_update_op</span><span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&quot;mse&quot;</span><span class="p">)</span>
<span class="n">init_op</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initialize_all_variables</span><span class="p">()</span>
<span class="n">init_op2</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initialize_local_variables</span><span class="p">()</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init_op2</span><span class="p">)</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init_op</span><span class="p">)</span>

<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span> <span class="c">#4.6666665</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">mse_update_op</span><span class="p">)</span> <span class="c">#4.6666665</span>
</pre></div>


<p><strong>应用：</strong><br />
最小二乘法通常用欧式距离进行距离的度量,使用平方损失函数</p>
<h3 id="95">9.5 指数损失函数</h3>
<p>$$loss(y_,y)=e^{-y_·y}=\frac{e^y} {e^{y_}}$$</p>
<p>$$coss(y_,y)=\frac{1}{N}\sum_{i=1}^{N}{e^{-y_·y}}$$</p>
<p>Tensorflow中没有指数损失函数的封装包，可以自定义</p>
<div class="hlcode"><pre><span class="c"># real_label.shape ==predict_label.shape==(counts,····)</span>
<span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="o">-</span><span class="n">real_label</span><span class="p">,</span><span class="n">predict_label</span><span class="p">))</span>
<span class="c"># loss shape =(counts,····)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">keep_dims</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">reduction_indices</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="c"># cost shape= ()  is a  val 数字</span>
</pre></div>


<p><strong>应用</strong><br />
AdaBoost使用指数损失函数。</p>
<h3 id="96-hinge">9.6 Hinge损失函数</h3>
<p>$$loss(y_,y)=max(0,1-y_ · y)$$</p>
<p><strong>手动：</strong></p>
<div class="hlcode"><pre><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">real_label</span><span class="p">,</span> <span class="n">predict_label</span><span class="p">))</span>
<span class="c"># loss shape =real_label.shape==predict_label.shape</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">keep_dims</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">reduction_indices</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="c"># cost shape= ()  is a  val 数字</span>
</pre></div>


<p><strong>封装：</strong></p>
<div class="hlcode"><pre><span class="n">hinge_loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">hinge_loss</span><span class="p">(</span>
    <span class="n">labels</span><span class="p">,</span>
    <span class="n">logits</span><span class="p">,</span>
    <span class="n">weights</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">scope</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">loss_collection</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">LOSSES</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="n">Reduction</span><span class="o">.</span><span class="n">SUM_BY_NONZERO_WEIGHTS</span>
<span class="p">)</span>
</pre></div>


<p><strong>应用</strong><br />
Hinge loss用于最大间隔（maximum-margin）分类，其中最有代表性的就是支持向量机SVM。</p>
<h3 id="97-huber">9.7 Huber损失函数</h3>
<p>Huber损失函数，平滑平均绝对误差 相比平方误差损失，Huber损失对于数据中异常值的敏感性要差一些。在值为0时，它也是可微分的。它基本上是绝对值，在误差很小时会变为平方值。误差使其平方值的大小如何取决于一个超参数δ，该参数可以调整。当δ~ 0时，Huber损失会趋向于MAE；当δ~ ∞（很大的数字），Huber损失会趋向于MSE。</p>
<div class="hlcode"><pre><span class="c"># huber 损失</span>
<span class="k">def</span> <span class="nf">huber</span><span class="p">(</span><span class="n">true</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">delta</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">true</span><span class="o">-</span><span class="n">pred</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">delta</span> <span class="p">,</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">((</span><span class="n">true</span><span class="o">-</span><span class="n">pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="n">delta</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">true</span> <span class="o">-</span> <span class="n">pred</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">delta</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>


<p><img alt="" src="../../../../attach/images/2019-09-24-10-59-57.png" /></p>
<h3 id="97-log-cosh">9.7 Log-Cosh损失</h3>
<h3 id="97-loss-tfnnloss">9.7. 神经网络里面的loss （tf.nn.loss）</h3>
<p>参考：https://www.tensorflow.org/api_docs/python/tf/losses</p>
<blockquote>
<p>关于<strong>概率的讨论</strong></p>
</blockquote>
<p><strong>概率（Probability） P</strong> 描述的是某事件A出现的次数与所有事件出现的次数之比:</p>
<p>$$P(A)=\frac{发生事件A的次数}{发送所有事件的次数}$$</p>
<p>$$P(A) \subset[0,1]$$</p>
<p>如果针对二分类问题，将P视为正样本的概率，则1-P为负样本的可能性，形如下面的则称为 事件为正样本的<strong>几率 （Odds）</strong>：<br />
$$Odds(A)=\frac{P(A)}{1-P(A)}=\frac{事件A发生的概率}{事件A不发生的概率}$$<br />
$$Odds(A) \subset[0,+\infty)$$</p>
<p><strong>Logit变换</strong> 是指log it(它)，Logit Odds 就是对Odds 进行log(Odds)计算</p>
<p>对上式进行<strong>Logit变换</strong>,形如下面的则称为正样本的<strong>对数机率</strong> z<br />
$$z=ln(Odds(A))=ln\frac{P(A)}{1-P(A)}$$<br />
$$z \subset(-\infty,+\infty)$$</p>
<p><img alt="" src="/attach/images/2019-07-10-16-24-57.png" /></p>
<blockquote>
<p>数学上<br />
$$ln\frac{P(A)}{1-P(A)}=z$$<br />
可以推出，概率P(A) 形式如下：<br />
$$P(A)=f(z)=\frac{1}{1+e^{-z}}=\frac{e^z}{1+e^{z}} $$</p>
</blockquote>
<p>$f(z)=\frac{1}{1+e^{-z}}$称为<strong>对数机率函数</strong> ，其数形式与Sigmoid 函数相同</p>
<h4 id="_7">熵</h4>
<p><strong>熵--当只有一个变量分布</strong></p>
<p>熵是对于给定分布 $q(x)$ 的不确定性的度量， 当取自有限的样本时，熵的公式可以表示为。</p>
<p>$$H(q(x))=-\sum{q(x) \log{q(x)} }$$</p>
<p>如果我们已知 所有的点都是绿色的，单一的？ 那个分布的不确定性是0，熵为0。<br />
如果我们已知，数据点服从q（x） 分布，我们可以依据上式计算该分布的熵</p>
<p><strong>交叉熵--当有2个变量分布</strong></p>
<p>在信息论中，基于相同事件测度的两个概率分布 P(x)和 q(x)的<strong>交叉熵</strong>是指，<br />
$$H(P(x),q(x))=-\sum{P(x) \log{q(x)} }$$</p>
<p>交叉熵是用来描述p分布和q分布的距离</p>
<blockquote>
<p><strong>现实情况中</strong></p>
</blockquote>
<p>现实情况中，多数情况式我们不知道数据的的真实分布。假设，数据真实分布为q(y)，我们推测其分为P（y_）。如果我们像这样计算熵，我们实际上是在计算两个分布之间的交叉熵：</p>
<p>$$H(q(y),P(y_))=-\sum{q(y) * \log{ P(y_ ) } }$$</p>
<p>模型训练的目的就是使 预测分布P(x) 逼近 q(x)，他们之间距离越小，函数越小。</p>
<h4 id="971-tfnnsigmoid_cross_entropy_with_logits">9.7.1  tf.nn.sigmoid_cross_entropy_with_logits</h4>
<p><code>predict_label=sigmoid(logits)</code></p>
<div class="hlcode"><pre><span class="n">cross_entropy</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span>
    <span class="n">_sentinel</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">labels</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">logits</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="bp">None</span>
<span class="p">)</span>
<span class="c"># labels</span>
<span class="c"># predict_label=sigmoid(logits)</span>
<span class="c"># cross_entropy的shape ：cross_entropy.shape=predict_label.shape=labels.shape</span>
</pre></div>


<h4 id="972-tfnnsoftmax_cross_entropy_with_logits">9.7.2 tf.nn.softmax_cross_entropy_with_logits</h4>
<p><code>predict_label=softmax(logits)</code></p>
<div class="hlcode"><pre><span class="n">cross_entropy</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span>
    <span class="n">_sentinel</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">labels</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">logits</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="bp">None</span>
<span class="p">)</span>
</pre></div>


<h4 id="973-tfnnsparse_softmax_cross_entropy_with_logits">9.7.3 tf.nn.sparse_softmax_cross_entropy_with_logits</h4>
<p>是<code>tf.nn.softmax_cross_entropy_with_logits</code>的易用版本。<br />
label 为<strong>未进行one hot编码</strong>的数据，labels.shape==[counts,1]<br />
logits不变，logits.shape==[counts,···]</p>
<p>如果已经对label进行了one hot编码，则可以直接使用 <code>tf.nn.softmax_cross_entropy_with_logits</code></p>
<p>predict_label=softmax(logits)<br />
real_label=tf.one_hot(label)</p>
<div class="hlcode"><pre><span class="n">cross_entropy</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span>
    <span class="n">_sentinel</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">labels</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">logits</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="bp">None</span>
<span class="p">)</span>

<span class="c"># labels.shape==[counts,1]   [1,2,3,4,5]</span>
<span class="c"># logits.shape==[counts,···] </span>
<span class="c"># cross_entropy.shape====[counts,···]</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">label1</span><span class="p">,</span><span class="n">CLASS</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
</pre></div>


<h4 id="974-tfnnl2_loss">9.7.4 tf.nn.l2_loss</h4>
<p>output = sum(t ** 2) / 2</p>
<div class="hlcode"><pre><span class="n">l2_loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_loss</span><span class="p">(</span>
    <span class="n">t</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="bp">None</span>
<span class="p">)</span>

<span class="nb">type</span><span class="p">(</span><span class="n">l2_loss</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span><span class="n">tensor</span>

<span class="n">t</span><span class="p">:</span> <span class="n">A</span> <span class="n">Tensor</span><span class="o">.</span> <span class="n">Must</span> <span class="n">be</span> <span class="n">one</span> <span class="n">of</span> <span class="n">the</span> <span class="n">following</span> <span class="n">types</span><span class="p">:</span> <span class="n">half</span><span class="p">,</span> <span class="n">bfloat16</span><span class="p">,</span> <span class="n">float32</span><span class="p">,</span> <span class="n">float64</span><span class="o">.</span> <span class="n">Typically</span> <span class="mi">2</span><span class="o">-</span><span class="n">D</span><span class="p">,</span> <span class="n">but</span> <span class="n">may</span> <span class="n">have</span> <span class="nb">any</span> <span class="n">dimensions</span><span class="o">.</span>
</pre></div>


<h1 id="10">10. 图的管理</h1>
<blockquote>
<p>图=edge(边缘)+node(节点)</p>
</blockquote>
<h3 id="101">10.1 图的管理</h3>
<h4 id="1011">10.1.1 图的建立/加载</h4>
<div class="hlcode"><pre><span class="n">graph</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="n">graph</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span> 
</pre></div>


<h4 id="1012">10.1.2 图的管理操作</h4>
<h5 id="1_7">1. 设置为默认图</h5>
<div class="hlcode"><pre><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">()</span>
<span class="n">graph</span><span class="o">.</span><span class="n">as_graph_def</span><span class="p">(</span><span class="n">from_version</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">add_shapes</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>


<h5 id="1_8">1. 设置为默认图</h5>
<p>tf中可以定义多个计算图，<strong>不同计算图上的张量和运算是相互独立的</strong>，不会共享。计算图可以用来隔离张量和计算，同时提供了管理张量和计算的机制。<strong>计算图可以通过Graph.device函数来指定运行计算的设备</strong>，为TensorFlow充分利用GPU/CPU提供了机制。</p>
<ol>
<li>使用 g = tf.Graph()函数创建新的计算图;</li>
<li><strong>在with g.as_default()语句下</strong>定义属于计算图g的张量和操作</li>
<li>
<p><strong>在with tf.Session()中</strong>通过参数 graph = xxx指定当前会话所运行的计算图;</p>
</li>
<li>
<p>如果没有显式指定张量和操作所属的计算图，则这些张量和操作属于默认计算图;</p>
</li>
<li>一个图可以在多个sess中运行，一个sess也能运行多个图</li>
</ol>
<h3 id="102-edge">10.2 edge(边缘)操作</h3>
<div class="hlcode"><pre><span class="n">op</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="o">.</span><span class="n">get_operation_by_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>    <span class="c">#根据名称返回操作节点 </span>
<span class="n">tensor</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>       <span class="c">#根据名称返回tensor数据 </span>
</pre></div>


<table>
<thead>
<tr>
<th>tf.Graph.as_graph_element(obj, allow_tensor=True, allow_operation=True)</th>
<th>返回一个图中与obj相关联的对象，为一个操作节点或者tensor数据</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td>tf.Graph.gradient_override_map(op_type_map)</td>
<td>用于覆盖梯度函数的上下文管理器</td>
</tr>
</tbody>
</table>
<h3 id="103-node">10.3 node(节点)操作</h3>
<div class="hlcode"><pre><span class="n">op_list</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="o">.</span><span class="n">get_operations</span><span class="p">()</span>  
<span class="c">#返回图中的操作节点列表</span>
</pre></div>


<p>tf.Graph</p>
<table>
<thead>
<tr>
<th align="left">操作</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">class tf.Graph</td>
<td align="left">tensorflow中的计算以图数据流的方式表示,一个图包含一系列表示计算单元的操作对象以及在图中流动的数据单元以tensor对象表现</td>
</tr>
<tr>
<td align="left"><code>tf.Graph.__init__()</code></td>
<td align="left">建立一个空图</td>
</tr>
<tr>
<td align="left">tf.Graph.as_default()</td>
<td align="left">一个将某图设置为默认图，并返回一个上下文管理器，如果不显式添加一个默认图，系统会自动设置一个全局的默认图。所设置的默认图，在模块范围内所定义的节点都将默认加入默认图中</td>
</tr>
<tr>
<td align="left">tf.Graph.as_graph_def(from_version=None, add_shapes=False)</td>
<td align="left">返回一个图的序列化的GraphDef表示，序列化的GraphDef可以导入至另一个图中(使用 import_graph_def())或者使用C++ Session API</td>
</tr>
<tr>
<td align="left">tf.Graph.finalize()</td>
<td align="left">完成图的构建，即将其设置为只读模式</td>
</tr>
<tr>
<td align="left">tf.Graph.finalized</td>
<td align="left">返回True，如果图被完成</td>
</tr>
<tr>
<td align="left">tf.Graph.control_dependencies(control_inputs)</td>
<td align="left">定义一个控制依赖，并返回一个上下文管理器</td>
</tr>
<tr>
<td align="left">with g.control_dependencies([a, b, c])</td>
<td align="left"># <code>d</code> 和 <code>e</code> 将在 <code>a</code>, <code>b</code>, 和<code>c</code>执行完之后运行.d = …e = …</td>
</tr>
<tr>
<td align="left">tf.Graph.device(device_name_or_function)</td>
<td align="left">定义运行图所使用的设备，并返回一个上下文管理器，with g.device('/gpu:0'): 、...with g.device('/cpu:0'): ...</td>
</tr>
<tr>
<td align="left">tf.Graph.name_scope(name)</td>
<td align="left">为节点创建层次化的名称，并返回一个上下文管理器</td>
</tr>
<tr>
<td align="left">tf.Graph.add_to_collection(name, value)</td>
<td align="left">将value以name的名称存储在收集器(collection)中</td>
</tr>
<tr>
<td align="left">tf.Graph.get_collection(name, scope=None)</td>
<td align="left">根据name返回一个收集器中所收集的值的列表</td>
</tr>
</tbody>
</table>
<h1 id="11_1">11. 迁移学习</h1>
<p>基本步骤：<br />
1. Get input, output , saver and graph"""#从导入图中获取需要的东西（）<br />
2. 构造新的variables用于后面的finetuning<br />
3. 构造损失<br />
4. 构造 op<br />
5. 开始新的训练</p>
<p>https://blog.csdn.net/ying86615791/article/details/76215363</p>
<h1 id="12_1">12. 正则化</h1>
<p>正则化 的实现 </p>
<div class="hlcode"><pre><span class="n">layer2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">filters</span><span class="p">,</span><span class="n">kernel_size</span><span class="p">,</span><span class="n">kernel_regularizer</span><span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">l2_regularizer</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span> 

<span class="o">...</span> 
<span class="n">l2_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">get_regularization_loss</span><span class="p">()</span> 
<span class="n">loss</span> <span class="o">+=</span> <span class="n">l2_loss</span> 
</pre></div>


<h1 id="13-tfcontribresampler">13. tf.contrib.resampler</h1>
<p>包含了 CPU 和 GPU 运算的图像可微重采样（differentiable resampling）</p>
<h1 id="14-session">14. Session 会话</h1>
<p>Session 分为tf.Session()，with tf.Session() as sess:和tf.InteractivateSession()：</p>
<p>1.tf.Session()和with tf.Session() as sess:区别：</p>
<p>如果用tf.Session()，使用完毕需要关闭会话如： </p>
<div class="hlcode"><pre><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="c">### 任务完成, 关闭会话.</span>
<span class="n">sess</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
 <span class="n">session</span><span class="err">对象在使用完后需要关闭以释放资源</span><span class="o">.</span> <span class="err">除了显式调用</span> <span class="n">close</span> <span class="err">外</span><span class="p">,</span> <span class="err">也可以使用</span> <span class="s">&quot;with&quot;</span> <span class="err">代码块</span> <span class="err">来自动完成关闭动作</span><span class="o">.</span>
</pre></div>


<p>with tf.Session() as sess:<br />
所以一般使用with tf.Session() as sess:</p>
<ol>
<li>with tf.Session() as sess:和tf.InteractivateSession()： 区别</li>
</ol>
<p>with tf.Session() as sess:是先构建计算图，然后通过sess启动计算图。</p>
<p>通过sess.run(Operation)或者sess.run(Tensor)启动计算</p>
<p>tf.InteractivateSession()：是交互式使用，如在Ipython环境中，可以在运行计算图的时候插入计算图</p>
<p>通过Tensor.eval()和Operation.run()计算</p>
<p>即吧张量还有操作放在外面了</p>
<h1 id="15-tensorflow-tfvariable">15. TensorFlow 变量(tf.Variable)</h1>
<p>-Tensorflow<br />
--GraphKeys<br />
---VARIABLES<br />
Variable被收集在名为tf.GraphKeys.VARIABLES的colletion中</p>
<h3 id="150">15.0 常量/变量/占位符/张量</h3>
<table>
<thead>
<tr>
<th align="left">项目</th>
<th align="left">常量tf.Constant</th>
<th align="left">变量tf.Variable</th>
<th align="left">占位符tf.placehold</th>
<th align="left">张量tf.tensor</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">数值</td>
<td align="left">不可变</td>
<td align="left">可变</td>
<td align="left">需要feed</td>
<td align="left">可变，并没有保存数字，通过sess.run(tensor) 输出后才确定</td>
</tr>
<tr>
<td align="left">shape/dtype</td>
<td align="left">不可变</td>
<td align="left">不可变</td>
<td align="left">不可变</td>
<td align="left">通过op 计算的</td>
</tr>
<tr>
<td align="left">初始化</td>
<td align="left">不需要</td>
<td align="left">需要</td>
<td align="left">需要feed 内存数据</td>
<td align="left">不需要</td>
</tr>
<tr>
<td align="left">用途</td>
<td align="left">储存超参数或其他结构信息的变量</td>
<td align="left">储存权重和其他信息的矩阵</td>
<td align="left">feed</td>
<td align="left">程序运行中间出现</td>
</tr>
</tbody>
</table>
<ol>
<li>constant</li>
</ol>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
    <span class="n">value</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">shape</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="s">&#39;Const&#39;</span><span class="p">,</span>
    <span class="n">verify_shape</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>
</pre></div>


<ol>
<li>Variable</li>
</ol>
<div class="hlcode"><pre><span class="c"># tensorflow/python/ops/variables.py</span>
<span class="nd">@tf_export</span><span class="p">(</span><span class="s">&quot;Variable&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Variable</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">initial_value</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">trainable</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
               <span class="n">collections</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">validate_shape</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
               <span class="n">caching_device</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">variable_def</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">expected_shape</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">import_scope</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
               <span class="n">constraint</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
<span class="c"># initial_value: 初始值，为一个tensor，或者可以被包装为tensor的值</span>
<span class="c"># trainable：是否可以训练，如果为false，则训练时不会改变</span>
<span class="c"># collections：变量要加入哪个集合中，有全局变量集合、本地变量集合、可训练变量集合等。默认加入全局变量集合中</span>
<span class="c"># dtype：变量的类型</span>
</pre></div>


<ol>
<li>Tensor</li>
</ol>
<div class="hlcode"><pre><span class="c"># tensorflow/python/framework/ops.py</span>

<span class="nd">@tf_export</span><span class="p">(</span><span class="s">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Tensor</span><span class="p">(</span><span class="n">_TensorLike</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">value_index</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
</pre></div>


<p>关于变量性质的理解<br />
1. 变量必须初始化<br />
2. 变量可变<br />
3. 变量类型/shape不能变<br />
4. 变量在sess run 前没有往内存申请与变量数值有关的内存</p>
<p>关于张量性质的理解<br />
1. 张量不需要初始化<br />
2. 变量在sess run 前没有往内存申请与变量数值有关的内存</p>
<div class="hlcode"><pre><span class="c"># -------默认图下</span>
<span class="n">val_a</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span> 
<span class="c"># initial_value=[1,2,4] trainable=True,dtype=tf.int </span>
<span class="nb">type</span><span class="p">(</span><span class="n">val_a</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tensorflow</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">variables</span><span class="o">.</span><span class="n">RefVariable</span>

<span class="n">init_value</span><span class="o">=</span><span class="n">val_a</span><span class="o">.</span><span class="n">initial_value</span>
<span class="nb">type</span><span class="p">(</span><span class="n">init_value</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">python</span><span class="o">.</span><span class="n">framework</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span>
<span class="n">new_val_a</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">val_a</span><span class="p">,[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>

<span class="n">init_op</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="c">#-------</span>
<span class="c"># 1. 变量必须初始化</span>
<span class="n">sess</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">val_a</span><span class="p">)</span> <span class="c"># Error </span>

<span class="n">sess</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init_op</span><span class="p">)</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">val_a</span><span class="p">)</span> <span class="c"># [1,2,4]</span>


<span class="c"># 2. 变量数值可变</span>
<span class="n">sess</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init_op</span><span class="p">)</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">val_a</span><span class="p">)</span> <span class="c"># [1,2,4]</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">new_val_a</span><span class="p">)</span> <span class="c"># [2,2,2]</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">val_a</span><span class="p">)</span> <span class="c"># [2,2,2]</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init_value</span><span class="p">)</span> <span class="c"># [1,2,4]</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">val_a</span><span class="o">.</span><span class="n">initial_value</span><span class="p">)</span> <span class="c"># [1,2,4]</span>

<span class="c"># 3. 变量类型/shape不能变</span>
<span class="c"># 在Graph中</span>
<span class="n">new_val_a</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">val_a</span><span class="p">,[</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">])</span> <span class="c">#Error</span>
<span class="n">new_val_a</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">val_a</span><span class="p">,[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span> <span class="c">#Error</span>

<span class="c"># 4 变量内存占用</span>
<span class="n">val_a</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span> 
<span class="n">val_b</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="mi">10</span><span class="p">)</span> 
<span class="n">sys</span><span class="o">.</span><span class="n">getsizeof</span><span class="p">(</span><span class="n">val_a</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span><span class="mi">56</span>
<span class="n">sys</span><span class="o">.</span><span class="n">getsizeof</span><span class="p">(</span><span class="n">val_b</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span><span class="mi">56</span>

<span class="c"># 5. 张量不需要初始化</span>
<span class="n">sess</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init_value</span><span class="p">)</span> <span class="c"># [1,2,4]</span>
</pre></div>


<h3 id="151">15.1 变量的类型</h3>
<p>Variable被划分到不同的集合中，方便后续操作。主要通过collections设置划分，默认加入全局变量集合中</p>
<h4 id="1511">15.1.1 全局变量</h4>
<p>全局变量：全局变量可以在不同进程中共享，可运用在分布式环境中。变量默认会加入到全局变量集合中。</p>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">global_variables</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="nd">@tf_export</span><span class="p">(</span><span class="s">&quot;global_variables&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">global_variables</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">GLOBAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="p">)</span>
  <span class="c">#可以查询全局变量集合。其op标示为GraphKeys.GLOBAL_VARIABLES</span>
</pre></div>


<h4 id="1512">15.1.2 本地变量</h4>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">local_variables</span><span class="p">()</span>

<span class="nd">@tf_export</span><span class="p">(</span><span class="s">&quot;local_variables&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">local_variables</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">LOCAL_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="p">)</span>
<span class="c"># 其op标示为GraphKeys.LOCAL_VARIABLES</span>
</pre></div>


<h4 id="1513">15.1.3 可训练变量</h4>
<p>可训练变量：一般模型参数会放到可训练变量集合中，训练时，做这些变量会得到改变。不在这个集合中的变量则不会得到改变。默认会放到此集合中。</p>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">()</span> 
<span class="nd">@tf_export</span><span class="p">(</span><span class="s">&quot;trainable_variables&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">trainable_variables</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">TRAINABLE_VARIABLES</span><span class="p">,</span> <span class="n">scope</span><span class="p">)</span>
<span class="c"># 其op标示为GraphKeys.TRAINABLE_VARIABLES</span>
</pre></div>


<h4 id="1514">15.1.4 模型变量</h4>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">model_variables</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>


<h4 id="1515">15.1.5 指数移动平均值变量</h4>
<p>如果创建了指数移动平均值对象，并且在变量列表上调用了apply（）方法，则这些变量将添加到graphkeys.moving_average_variables集合中。此便利函数返回该集合的内容。</p>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">moving_average_variables</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>


<h3 id="152">15.2 变量的操作</h3>
<h4 id="1521">15.2.1 变量初始化</h4>
<p>变量只有初始化后才能使用。即<code>__init__ (initial_value)</code> 必须有数值</p>
<p>initial_value的赋值有两种方式：<br />
1. 在build graph 时直接赋值，从内存读取一个树，生成一个tensor，initial_value=tensor,赋值给变量</p>
<div class="hlcode"><pre><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mf">5.3</span><span class="p">])</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">initialization</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
    <span class="k">print</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>


<ol>
<li>通过op 在session中生成tensor，initial_value=tensor,赋值给变量</li>
</ol>
<div class="hlcode"><pre><span class="n">Initializer_instance</span><span class="o">=</span><span class="err">···</span>
<span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span>
</pre></div>


<p>tf.local_variables_initializer<br />
tf.variables_initializer<br />
例如：tf.global_variables_initializer() ,就是通过变量的赋值op assign 数值给所有global_variables变量</p>
<h5 id="1_9">1. 确定值初始化</h5>
<ol>
<li>常标量<br />
$$W=1$$</li>
</ol>
<div class="hlcode"><pre><span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">Initializer_instance</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>


<ol>
<li>常张量</li>
</ol>
<p>$$W=\left[<br />
 \begin{matrix}<br />
   1 &amp; 1 &amp; 1 \<br />
   1 &amp; 1 &amp; 1 \<br />
   1 &amp; 1 &amp; 1  \<br />
  \end{matrix} <br />
  \right]<br />
$$</p>
<p>$$W=\left[<br />
 \begin{matrix}<br />
   0 &amp; 0 &amp; 0 \<br />
   0 &amp; 0 &amp; 0 \<br />
   0 &amp; 0 &amp; 0  \<br />
  \end{matrix} <br />
  \right]<br />
$$</p>
<p>$$W=\left[<br />
 \begin{matrix}<br />
   1 &amp; 0 &amp; 0 \<br />
   0 &amp; 1 &amp; 0 \<br />
   0 &amp; 0 &amp; 1  \<br />
  \end{matrix} <br />
  \right]<br />
$$</p>
<p>正交矩阵 A 有 $A^TA=AA^T=I$</p>
<div class="hlcode"><pre><span class="c"># 全1矩阵</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">node_in</span><span class="p">,</span> <span class="n">node_out</span><span class="p">]))</span>
<span class="n">Initializer_instance</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">ones</span><span class="p">()</span>

<span class="c">## 0矩阵</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">node_in</span><span class="p">,</span> <span class="n">node_out</span><span class="p">]))</span>
<span class="n">Initializer_instance</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">zeros</span><span class="p">()</span>

<span class="c">## 单位矩阵</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="n">gain</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
<span class="n">Initializer_instance</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="c"># gain: 用于单位矩阵的乘法因子。 </span>




<span class="k">class</span> <span class="nc">uniform_unit_scaling</span><span class="err">：生成张量而不缩放方差的初始化程序</span><span class="o">.</span>
</pre></div>


<h5 id="2_10">2. 随机初始化</h5>
<h6 id="21">2.1 正态分布</h6>
<ol>
<li>标准正态分布</li>
</ol>
<p>$$W \sim N[0,1]$$</p>
<div class="hlcode"><pre><span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">node_in</span><span class="p">,</span> <span class="n">node_out</span><span class="p">)))</span>
<span class="n">Initializer_instance</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">random_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>


<ol>
<li>截断正态分布</li>
</ol>
<p>在该分布下，产生的值$W_i$ 满足<br />
$$W \sim N[mean=0,stddev=1]$$</p>
<p>$$|W_i-mean|&lt;2* stddev$$</p>
<div class="hlcode"><pre><span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">node_in</span><span class="p">,</span> <span class="n">node_out</span><span class="p">)))</span>

<span class="n">Initializer_instance</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="err">：生成截断正态分布的初始化程序</span><span class="o">.</span>
</pre></div>


<ol>
<li>依据输入权重shape大小调整的方差的正态分布</li>
</ol>
<p>$$n=W_i.shape[0]  ,or, n=W_{i-1}.shape[0] $$</p>
<p>$$  stddev = \sqrt{\frac{scale}  {n}} $$</p>
<p>$$W \sim N[mean=0,stddev]$$</p>
<p>𝑤[𝑙] = 𝑛𝑝.𝑟𝑎𝑛𝑑𝑜𝑚.𝑟𝑎𝑛𝑑𝑛(shape)∗ np.sqrt(1/𝑛[𝑙−1])，</p>
<p>如果mode='fan_in'，则n为输入权重的数量shape[0]<br />
如果mode='fan_out'，则n为输出权重的数量shape[0]<br />
如果mode='fan_avg'，则n为输入输出权重的数量的平均值 (W[i].shape[0]+W[i-1].shape[0])/2</p>
<div class="hlcode"><pre><span class="n">Initializer_instance</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">variance_scaling</span><span class="p">(</span>
    <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="s">&#39;fan_in&#39;</span><span class="p">,</span>
    <span class="n">distribution</span><span class="o">=</span><span class="s">&#39;untruncated_normal&#39;</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="bp">None</span>
<span class="p">)</span>
</pre></div>


<ol>
<li>依据输入权重shape大小调整的方差的截断正态分布</li>
</ol>
<p>$$n=W_i.shape[0]  ,or, n=W_{i-1}.shape[0] $$</p>
<p>$$stddev = \sqrt{\frac{scale} {n}}$$</p>
<p>$$W \sim N[mean=0,stddev]$$</p>
<p>$$|W_i-mean|&lt;2* stddev$$</p>
<p>如果mode='fan_in'，则n为输入权重的数量shape[0]<br />
如果mode='fan_out'，则n为输出权重的数量shape[0]<br />
如果mode='fan_avg'，则n为输入输出权重的数量的平均值 (W[i].shape[0]+W[i-1].shape[0])/2</p>
<div class="hlcode"><pre><span class="n">Initializer_instance</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">variance_scaling</span><span class="p">(</span>
    <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="s">&#39;fan_in&#39;</span><span class="p">,</span>
    <span class="n">distribution</span><span class="o">=</span><span class="s">&#39;truncated_normal&#39;</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="bp">None</span>
<span class="p">)</span>
</pre></div>


<h6 id="22_1">2.2 均匀分布</h6>
<ol>
<li>标准均匀分布<br />
$$W \sim U(0,1)$$</li>
</ol>
<div class="hlcode"><pre><span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">node_in</span><span class="p">,</span> <span class="n">node_out</span><span class="p">)))</span>
<span class="n">Initializer_instance</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">(</span><span class="n">minval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">maxval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>


<ol>
<li>依据输入权重shape大小调整 的均匀分布<br />
$$n=W_i.shape[0]  ,or, n=W_{i-1}.shape[0] $$</li>
</ol>
<p>$$W \sim U(-\sqrt{\frac{3<em>scale}{n}},\sqrt{\frac{3</em>scale}{n}})$$</p>
<div class="hlcode"><pre><span class="n">Initializer_instance</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">variance_scaling</span><span class="p">(</span>
    <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="s">&#39;fan_in&#39;</span><span class="p">,</span>
    <span class="n">distribution</span><span class="o">=</span><span class="s">&#39;uniform&#39;</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="bp">None</span>
<span class="p">)</span>
</pre></div>


<p>如果mode='fan_in'，则n为输入权重的数量shape[0]<br />
如果mode='fan_out'，则n为输出权重的数量shape[0]<br />
如果mode='fan_avg'，则n为输入输出权重的数量的平均值 (W[i].shape[0]+W[i-1].shape[0])/2</p>
<h5 id="3_5">3. 特殊函数初始化</h5>
<p>对于深度神经网络,一旦随机分布选择不当，就会导致网络优化陷入困境。</p>
<h6 id="31-xavier-initialization-glorot-initialization">3.1 Xavier initialization/ Glorot initialization</h6>
<p>Xavier Glorot 在《Understanding the difficulty of training deep feedforward neural network》 http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf 提出的针对神经网络权重参数初始化方法。</p>
<p>适合<strong>Sigmoid或者Tanh作为激活函数</strong></p>
<div class="hlcode"><pre><span class="n">Initializer_instance</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">glorot_normal</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">Initializer_instance</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">glorot_uniform</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>


<ol>
<li>正态分布</li>
</ol>
<p>$$n=\frac{W_i.shape[0]+W_{i-1}.shape[0]}{2} ,scale=1.0$$</p>
<p>$$stddev = \sqrt{\frac{scale} {n}}$$</p>
<p>$$W \sim N[mean=0,stddev]$$</p>
<p>$$|W_i-mean|&lt;2* stddev$$</p>
<ol>
<li>均方分布</li>
</ol>
<p>$$n=\frac{W_i.shape[0]+W_{i-1}.shape[0]}{2} ,scale=1.0$$</p>
<p>$$W \sim U(-\sqrt{\frac{3<em>scale}{n}},\sqrt{\frac{3</em>scale}{n}})= U(-\sqrt{\frac{6}{W_i.shape[0]+W_{i-1}.shape[0]}},\sqrt{\frac{6}{W_i.shape[0]+W_{i-1}.shape[0]}})$$</p>
<h6 id="32-he-initialization">3.2 He initialization</h6>
<p>He initialization的思想是：<strong>在ReLU网络中</strong>，假定每一层有一半的神经元被激活，另一半为0，所以，要保持variance不变，只需要在Xavier的基础上再除以2：<br />
1. 正态分布</p>
<p>$$n=W_i.shape[0] ,scale=2.0$$</p>
<p>$$stddev = \sqrt{\frac{scale} {n}}=\sqrt{\frac{2} {n}}$$</p>
<p>$$W \sim N[mean=0,stddev]$$</p>
<p>$$|W_i-mean|&lt;2* stddev$$</p>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">he_uniform</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>


<ol>
<li>均匀分布</li>
</ol>
<p>$$n=W_i.shape[0]+W_{i-1},scale=2.0$$</p>
<p>$$W \sim U(-\sqrt{\frac{3<em>scale}{n}},\sqrt{\frac{3</em>scale}{n}})= U(-\sqrt{\frac{6}{W_i.shape[0]}},\sqrt{\frac{6}{W_i.shape[0]}})$$</p>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">he_uniform</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>


<h6 id="33-lecun-initialization">3.3 Lecun initialization</h6>
<ol>
<li>正态分布</li>
</ol>
<p>$$n=W_i.shape[0] ,scale=1.0$$</p>
<p>$$stddev = \sqrt{\frac{scale} {n}}=\sqrt{\frac{1} {n}}$$</p>
<p>$$W \sim N[mean=0,stddev]$$</p>
<p>$$|W_i-mean|&lt;2* stddev$$</p>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">lecun_normal</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>


<ol>
<li>均匀分布</li>
</ol>
<p>$$n=W_i.shape[0]+W_{i-1},scale=1.0$$</p>
<p>$$W \sim U(-\sqrt{\frac{3<em>scale}{n}},\sqrt{\frac{3</em>scale}{n}})= U(-\sqrt{\frac{3}{W_i.shape[0]}},\sqrt{\frac{3}{W_i.shape[0]}})$$</p>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">lecun_uniform</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>


<h6 id="33-orthogonal-initialization-">3.3 orthogonal initialization(正交初始化)-基于正态分布</h6>
<p>适合RNN<br />
基于正态分布</p>
<div class="hlcode"><pre><span class="k">def</span> <span class="nf">orthogonal</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
    <span class="n">flat_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">flat_shape</span><span class="p">)</span>
    <span class="n">u</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">u</span> <span class="k">if</span> <span class="n">u</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">flat_shape</span> <span class="k">else</span> <span class="n">v</span>
    <span class="k">return</span> <span class="n">q</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="n">orthogonal</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>

<span class="n">Initializer_instance</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">orthogonal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="c"># gain 放大范围</span>
</pre></div>


<h5 id="4_1">4. 变量初始化的影响</h5>
<h6 id="41">4.1  没有初始化，变量报错</h6>
<p>变量没有初始化，无法运行</p>
<h6 id="42">4.2 糟糕的初始化影响变量的数值更新</h6>
<p>针对神经网络中的权重变量初始化，若权重w没有得到好的初始化，则bp反馈中影响权重的更新。相关解决方案有<br />
1. 更有神经网络的结构特点，选择好的初始化方式</p>
<ol>
<li>
<p>使用与训练的初始化值</p>
</li>
<li>
<p>Batch Normalization Layer</p>
</li>
</ol>
<p>我们想要的是在非线性activation之前，输出值应该有比较好的分布（例如高斯分布），以便于back propagation时计算gradient，更新weight。Batch Normalization将输出值强行做一次Gaussian Normalization和线性变换：</p>
<p>Batch Normalization中所有的操作都是平滑可导，这使得back propagation可以有效运行并学到相应的参数$\gamma$，$\beta$。需要注意的一点是Batch Normalization在training和testing时行为有所差别。Training时$\mu_\mathcal{B}$和$\sigma_\mathcal{B}$由当前batch计算得出；在Testing时$\mu_\mathcal{B}$和$\sigma_\mathcal{B}$应使用Training时保存的均值或类似的经过处理的值，而不是由当前batch计算。</p>
<p>随机初始化，无Batch Normalization：</p>
<div class="hlcode"><pre><span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">node_in</span><span class="p">,</span> <span class="n">node_out</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="o">......</span>
<span class="n">fc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">fc</span><span class="p">)</span>
</pre></div>


<p>随机初始化，有Batch Normalization：</p>
<div class="hlcode"><pre><span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">node_in</span><span class="p">,</span> <span class="n">node_out</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.01</span>

<span class="n">fc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">(</span><span class="n">fc</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                  <span class="n">is_training</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">fc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">fc</span><span class="p">)</span>
</pre></div>


<h5 id="4_2">4. 从磁盘中加载数据初始化</h5>
<p>详细见 [15.2.3 变量恢复与保存]</p>
<h4 id="1522">15.2.2 变量赋值</h4>
<p>tf.assign(ref, value, validate_shape=None, use_locking=None, name=None) <br />
tf.assign_add<br />
tf.assign_sub</p>
<h4 id="1523">15.2.3 变量恢复与保存</h4>
<p>变量信息保存在图中，在持久后具体存在meta_graph里 </p>
<h5 id="1_10">1. 变量保存</h5>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">export_meta_graph</span><span class="p">(</span>
    <span class="n">filename</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">meta_info_def</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">graph_def</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">saver_def</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">collection_list</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">as_text</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">graph</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">export_scope</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">clear_devices</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">clear_extraneous_savers</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">strip_default_attrs</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</pre></div>


<h5 id="2_11">2. 变量加载回复</h5>
<div class="hlcode"><pre><span class="n">saver</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">import_meta_graph</span><span class="p">(</span><span class="err">“</span><span class="n">save</span><span class="o">/</span><span class="n">model</span><span class="o">.</span><span class="n">ckpt</span><span class="o">.</span><span class="n">meta</span><span class="err">”</span><span class="p">)</span>
<span class="n">dir_path</span><span class="o">=</span><span class="s">&quot;./model&quot;</span> <span class="c"># 文件夹地址</span>
<span class="n">model_file</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">(</span><span class="n">dir_path</span><span class="p">)</span>
<span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span><span class="n">model_file</span><span class="p">)</span>
</pre></div>


<h4 id="1524">15.2.4 获得变量</h4>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">is_variable_initialized</span>
<span class="n">tf</span><span class="o">.</span><span class="n">report_uninitialized_variables</span>
<span class="n">tf</span><span class="o">.</span><span class="n">assert_variables_initialized</span>
<span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span>
<span class="n">tf</span><span class="o">.</span><span class="n">get_local_variable</span>

<span class="c">#只读查询表</span>
<span class="n">tf</span><span class="o">.</span><span class="n">initialize_all_tables</span>
<span class="n">tf</span><span class="o">.</span><span class="n">tables_initializer</span>
</pre></div>


<h1 id="16-tfprofile">16. 程序性能评估(tf.profile)</h1>
<p>和其他程序算法一样，基于tensorflow框架的运行算法同样需要注重算法效率：<br />
1. 时间消耗<br />
2. 内存空间消耗<br />
3. 时间/空间复杂度</p>
<p>参考：<br />
1. TensorFlow Profiler and Advisor： https://github.com/tensorflow/tensorflow/tree/r1.3/tensorflow/core/profiler<br />
2. TensorFlow Profiler UI：https://github.com/tensorflow/profiler-ui</p>
<h3 id="161">16.1 程序性能评估的过程</h3>
<p>分为4步：<br />
1. 创建评估器 tf.profiler.Profiler 实例 </p>
<div class="hlcode"><pre><span class="n">profiler</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">Profiler</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">sess</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span><span class="n">op_log</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="c"># profiler 实例</span>
<span class="c"># op_log: optional. tensorflow::tfprof::OpLogProto proto. Used to define extra op types.</span>
</pre></div>


<ol>
<li>创建protobuf格式的数据结构对象</li>
</ol>
<div class="hlcode"><pre><span class="c">#2.1. 保存运行数据的run_metadata数据结构对象，以记录运行时候的每个OP 的运行时间和内存占用数据</span>
<span class="n">run_metadata</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">RunMetadata</span><span class="p">()</span>
<span class="nb">type</span><span class="p">(</span><span class="n">run_metadata</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">protobuf</span><span class="o">.</span><span class="n">config_pb2</span><span class="o">.</span><span class="n">RunMetadata</span>

<span class="c">#2.2. 保存评估器运行参数的run_options数据结构对象 </span>
<span class="n">run_options</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">RunOptions</span><span class="p">(</span><span class="n">trace_level</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">RunOptions</span><span class="o">.</span><span class="n">FULL_TRACE</span><span class="p">)</span>
<span class="nb">type</span><span class="p">(</span><span class="n">run_options</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">protobuf</span><span class="o">.</span><span class="n">config_pb2</span><span class="o">.</span><span class="n">RunOptions</span>
</pre></div>


<p>RunOptions 设置评估器的运行参数选项，包括:全记录，记录硬件信息，记录软件信息，不记录。</p>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">RunOptions</span> <span class="err">类</span>
<span class="err">类成员</span>
<span class="n">tf</span><span class="o">.</span><span class="n">RunOptions</span><span class="o">.</span><span class="n">FULL_TRACE</span>
<span class="n">tf</span><span class="o">.</span><span class="n">RunOptions</span><span class="o">.</span><span class="n">HARDWARE_TRACE</span>
<span class="n">tf</span><span class="o">.</span><span class="n">RunOptions</span><span class="o">.</span><span class="n">NO_TRACE</span>
<span class="n">tf</span><span class="o">.</span><span class="n">RunOptions</span><span class="o">.</span><span class="n">SOFTWARE_TRACE</span>
<span class="n">tf</span><span class="o">.</span><span class="n">RunOptions</span><span class="o">.</span><span class="n">TraceLevel</span>
</pre></div>


<ol>
<li>将上述二者结合并评估模型耗时、内存占用、参数数量等情况；</li>
</ol>
<div class="hlcode"><pre><span class="n">op</span><span class="o">=</span><span class="err">···</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">total_steps</span><span class="p">):</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="n">run_options</span><span class="p">,</span> <span class="n">run_metadata</span><span class="o">=</span><span class="n">run_metadata</span><span class="p">)</span> <span class="c">#@1</span>
        <span class="n">profiler</span><span class="o">.</span><span class="n">add_step</span><span class="p">(</span><span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">,</span> <span class="n">run_meta</span><span class="o">=</span><span class="n">run_metadata</span><span class="p">)</span> <span class="c">#@2</span>
<span class="n">profiler</span><span class="o">.</span><span class="n">profile_graph</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="n">profile_graph_opts_builder</span><span class="o">.</span><span class="n">build</span><span class="p">())</span> <span class="c">#@3</span>
</pre></div>


<p>profiler 分为数据搜集和数据显示两个主要步骤。</p>
<p>graph node的每一次执行，记录单步统计数据，主要是执行时间和占用内存，格式参见step_stats.proto，作为原始的最小粒度统计数据源；<br />
@1 每一次session.Run()，所有执行到的graph node的统计数据，都集中汇总保存到 RunMetadata 数据结构中;<br />
@2 用户程序把每一次搜集到的 RunMetadata 添加到profiler实例，做数据累计和加工处理。<br />
@3 将profiler以某一视图按某一设定输出</p>
<ol>
<li>profiler 持久后保存/可视化</li>
</ol>
<div class="hlcode"><pre> <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">current_dir_path</span><span class="o">+</span><span class="s">&#39;/profile.pd&#39;</span><span class="p">,</span> <span class="s">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">profiler</span><span class="o">.</span><span class="n">serialize_to_string</span><span class="p">())</span>
</pre></div>


<p>例子4： code view – 显示python代码的执行资源消耗 <br />
按照python代码的方式来显示统计数据，也就是统计每一行python代码产生的node的执行性能</p>
<h3 id="162">16.2 评估器实例</h3>
<h4 id="1621">16.2.1 实例的建立</h4>
<div class="hlcode"><pre><span class="n">Profiler</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">Profiler</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">op_log</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="c">#graph：tf.Graph。如果为“None”或者未启用“eager执行”，请使用默认图形。</span>
<span class="c">#op_log：可选的。tensorflow :: tfprof :: OpLogProto proto。用于定义额外的op类型。</span>
</pre></div>


<h4 id="1622">16.2.2 实例操作</h4>
<div class="hlcode"><pre><span class="c"># 将run_meta和step 添加到Profiler</span>
<span class="n">Profiler</span><span class="o">.</span><span class="n">add_step</span><span class="p">(</span><span class="n">step</span><span class="p">,</span><span class="n">run_meta</span><span class="p">)</span> <span class="c"># 添加步骤的统计信息。</span>
<span class="c">#step:int-&gt;0</span>
<span class="c">#run_meta-&gt;run_metadata:</span>

<span class="c">#将ProfileProto序列化为二进制字符串。</span>
<span class="n">Profiler</span><span class="o">.</span><span class="n">serialize_to_string</span><span class="p">()</span>  

<span class="c"># 将run_meta输出到log_dir中</span>
<span class="n">tf</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">write_op_log</span><span class="p">(</span>
    <span class="n">graph</span><span class="p">,</span>
    <span class="n">log_dir</span><span class="p">,</span>
    <span class="n">op_log</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">run_meta</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">add_trace</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
</pre></div>


<h4 id="1622_1">16.2.2 评估运行并得到结果</h4>
<div class="hlcode"><pre><span class="n">Advise</span><span class="o">=</span><span class="n">Profiler</span><span class="o">.</span><span class="n">advise</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">{})</span> <span class="c"># 自动检测问题并生成报告,返回一个包含所有检查者的报告的Advise原型。</span>

<span class="n">GraphNodeProto</span><span class="o">=</span><span class="n">Profiler</span><span class="o">.</span><span class="n">profile_graph</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">{})</span> <span class="c">#通过数据流图组织图形节点的统计信息。</span>
<span class="n">GraphNodeProto</span><span class="o">=</span><span class="n">Profiler</span><span class="o">.</span><span class="n">profile_name_scope</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">{})</span><span class="c">#按名称范围组织图形节点的统计信息。</span>
<span class="n">MultiGraphNodeProto</span><span class="o">=</span><span class="n">Profiler</span><span class="o">.</span><span class="n">profile_operations</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">{})</span><span class="c">#描述操作类型的统计信息（例如：MatMul，Conv2D）。</span>
<span class="n">MultiGraphNodeProto</span><span class="o">=</span><span class="n">Profiler</span><span class="o">.</span><span class="n">Profiler</span><span class="o">.</span><span class="n">profile_python</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">{})</span>  <span class="c">#描述Python代码的统计信息。</span>

<span class="nb">type</span><span class="p">(</span><span class="n">options</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span><span class="nb">dict</span> 
</pre></div>


<div class="hlcode"><pre><span class="c"># ----------profile</span>
<span class="n">profile</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span>
    <span class="n">graph</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">run_meta</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">op_log</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">cmd</span><span class="o">=</span><span class="s">&#39;graph&#39;</span><span class="p">,</span> <span class="c"># &#39;op&#39;，&#39;scope&#39;，&#39;graph&#39;,&#39;code&#39;</span>
    <span class="n">options</span><span class="o">=</span><span class="p">{}</span>
<span class="p">)</span>
<span class="nb">type</span><span class="p">(</span><span class="n">profile</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tensorflow</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">tfprof_output_pb2</span><span class="o">.</span><span class="n">GraphNodeProto</span>
<span class="c"># 上述等同于--|</span>
<span class="n">Profiler</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">Profiler</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">op_log</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">Profiler</span><span class="o">.</span><span class="n">add_step</span><span class="p">(</span><span class="n">step</span><span class="p">,</span><span class="n">run_meta</span><span class="p">)</span>
<span class="n">Profiler</span><span class="o">.</span><span class="n">profile_graph</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">{})</span>

<span class="c">#---------advise</span>
<span class="n">advise</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">advise</span><span class="p">(</span>
    <span class="n">graph</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">run_meta</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">options</span><span class="o">=</span><span class="n">_DEFAULT_ADVISE_OPTIONS</span>
<span class="p">)</span>
<span class="nb">type</span><span class="p">(</span><span class="n">advise</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tensorflow</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">tfprof_output_pb2</span><span class="o">.</span><span class="n">AdviceProto</span>
<span class="c"># 上述等同于--|</span>
<span class="n">Profiler</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">Profiler</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">op_log</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">Profiler</span><span class="o">.</span><span class="n">add_step</span><span class="p">(</span><span class="n">step</span><span class="p">,</span><span class="n">run_meta</span><span class="p">)</span>
<span class="n">Profiler</span><span class="o">.</span><span class="n">advise</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">{})</span>
</pre></div>


<h3 id="163-protobuf">16.3 protobuf格式的数据结构对象</h3>
<p>我们注意到Profiler 运行是输出ProtocolMessage格式统计数据，tf.profile中定义了4种 数结构</p>
<div class="hlcode"><pre><span class="n">class</span> <span class="n">AdviceProto</span><span class="err">：</span><span class="n">ProtocolMessage</span>

<span class="n">class</span> <span class="n">GraphNodeProto</span><span class="err">：</span><span class="n">ProtocolMessage</span>

<span class="n">class</span> <span class="n">MultiGraphNodeProto</span><span class="err">：</span><span class="n">ProtocolMessage</span>

<span class="n">class</span> <span class="n">OpLogProto</span><span class="err">：</span><span class="n">ProtocolMessage</span>
</pre></div>


<h3 id="164">16.4 评估器的选项设置</h3>
<p>评估器的选项设置options通过字典完成，</p>
<p>tf.profiler.profile中的options可选项为：</p>
<div class="hlcode"><pre><span class="o">-</span><span class="n">max_depth</span> <span class="mi">4</span>       <span class="err">命名空间的深度阈值，超过该阈值的分支不予展示</span>
<span class="o">-</span><span class="n">min_bytes</span> <span class="mi">0</span>       <span class="err">展示占用内存超过该阈值的</span><span class="n">OP</span>
<span class="o">-</span><span class="n">min_micros</span> <span class="mi">10</span>       <span class="err">展示耗时超过该阈值的</span><span class="n">OP</span>
<span class="o">-</span><span class="n">min_params</span>  <span class="mi">0</span>       <span class="err">展示超过该阈值的参数大小的</span><span class="n">OP</span>
<span class="o">-</span><span class="n">min_float_ops</span> <span class="mi">0</span>       <span class="err">展示超过该阈值的浮点计算量的</span> <span class="n">OP</span>
<span class="o">-</span><span class="n">min_occurrence</span>  <span class="mi">0</span>        <span class="err">展示超过该阈值的出现次数的</span> <span class="n">OP</span>
<span class="o">-</span><span class="n">step</span> <span class="o">-</span><span class="mi">1</span>       <span class="err">展示训练时候哪一步的统计情况，默认为最后一步</span>
<span class="o">-</span><span class="n">order_by</span> <span class="n">micros</span>       <span class="err">统计结果排序字段设定</span>
<span class="o">-</span><span class="n">account_type_regexes</span> <span class="n">_trainable_variables</span> 
<span class="n">Selectively</span> <span class="n">counting</span> <span class="n">statistics</span> <span class="n">based</span> <span class="n">on</span> <span class="n">node</span> <span class="n">types</span> <span class="err">，比如这里设定展示可训练变量；</span><span class="n">account_type_regexes</span><span class="o">=</span><span class="p">[</span><span class="err">&#39;</span><span class="p">.</span><span class="o">*</span><span class="n">gpu</span><span class="o">:</span><span class="mf">0.</span><span class="o">*</span><span class="err">&#39;</span><span class="p">]</span> <span class="err">设定展示</span> <span class="n">GPU</span> <span class="err">运行的变量；</span>
<span class="o">-</span><span class="n">start_name_regexes</span> <span class="p">.</span><span class="o">*</span> 
      <span class="err">以下四个选项可以更加灵活的设置展示哪些</span> <span class="n">OP</span> 
<span class="o">-</span><span class="n">trim_name_regexes</span> 
<span class="o">-</span><span class="n">show_name_regexes</span> <span class="n">siamese</span><span class="p">.</span><span class="o">*</span>
<span class="o">-</span><span class="n">hide_name_regexes</span> 
<span class="o">-</span><span class="n">account_displayed_op_only</span> <span class="nb">false</span>
<span class="o">-</span><span class="n">select</span> <span class="n">micros</span>  <span class="err">选择展示的属性</span><span class="p">,</span><span class="err">这里选择查看计算耗时，</span>
<span class="o">-</span><span class="n">output</span> <span class="n">stdout</span><span class="o">:</span> <span class="err">输出方式，这里是标准输出</span>
</pre></div>


<p>参考：https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/options.md</p>
<p>同时选项字典可以通过tf.profile.ProfileOptionBuilder构建字典 </p>
<div class="hlcode"><pre><span class="k">class</span> <span class="nc">ProfileOptionBuilder</span><span class="err">：</span>
<span class="c"># 用于Profiling API的Option Builder。</span>
<span class="c"># 返回字典</span>
</pre></div>


<p>评估器选项构建器 ProfileOptionBuilder<br />
1. 记录内容选项<br />
2. 输出内容格式选项<br />
3. 限制节点选项</p>
<h4 id="1641">16.4.1 记录内容选项</h4>
<h5 id="1_11">1. 记录时间和内存消耗</h5>
<div class="hlcode"><pre><span class="n">ProfileOptionBuilder</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfileOptionBuilder</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">{})</span>

<span class="c"># 时间和内存消耗</span>
<span class="n">ProfileOptionBuilder</span><span class="o">.</span><span class="n">time_and_memory</span><span class="p">(</span>
    <span class="n">min_micros</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">min_bytes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">min_accelerator_micros</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">min_cpu_micros</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">min_peak_bytes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">min_residual_bytes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">min_output_bytes</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
</pre></div>


<h5 id="2_12">2. 记录浮点运算情况</h5>
<div class="hlcode"><pre><span class="n">ProfileOptionBuilder</span><span class="o">.</span><span class="n">float_operation</span><span class="p">()</span>
</pre></div>


<h4 id="1642">16.4.2 输出选项</h4>
<p>输出文件格式<br />
1. time line : 输出JSON events file, 再用chrome浏览器tracing功能进行查看，可视性很棒。<br />
2. stdout ： 标准输出设备打印。<br />
3. pprof file: 输出pprof的文件格式，再用pprof工具查看。<br />
4. file: 输出到普通的文本文件。</p>
<div class="hlcode"><pre><span class="n">ProfileOptionBuilder</span><span class="p">.</span><span class="n">with_pprof_output</span><span class="p">(</span><span class="n">pprof_file</span><span class="o">=</span><span class="s">&quot;./xxx.pb.gz.&quot;</span><span class="p">)</span> <span class="err">#</span> <span class="err">生成一个</span><span class="n">pprof</span> <span class="n">profile</span> <span class="n">gzip</span> <span class="n">file</span><span class="p">.</span>
<span class="n">ProfileOptionBuilder</span><span class="p">.</span><span class="n">with_stdout_output</span><span class="p">()</span> <span class="err">#</span> <span class="n">c</span><span class="o">++</span> <span class="n">std</span> <span class="n">out</span> <span class="n">Print</span> <span class="n">the</span> <span class="n">result</span> <span class="n">to</span> <span class="n">stdout</span><span class="p">.</span>
<span class="n">ProfileOptionBuilder</span><span class="p">.</span><span class="n">with_timeline_output</span><span class="p">(</span><span class="n">timeline_file</span><span class="o">=</span><span class="s">&quot;./xxx.json&quot;</span><span class="p">)</span> <span class="err">#生成一个</span><span class="n">json</span> <span class="err">文件</span>
<span class="n">ProfileOptionBuilder</span><span class="p">.</span><span class="n">with_file_output</span><span class="p">(</span><span class="n">outfile</span><span class="o">=</span><span class="s">&quot;&quot;</span><span class="p">)</span> <span class="err">#输出一个文件</span>
<span class="n">ProfileOptionBuilder</span><span class="p">.</span><span class="n">with_empty_output</span><span class="p">()</span><span class="err">#不输出</span>

<span class="cp">#定义显示sess.Run() 第70步的统计数据,如果为-1，则使用所#有可用步骤的平均值。</span>
<span class="n">ProfileOptionBuilder</span><span class="p">.</span><span class="n">with_step</span><span class="p">(</span><span class="mi">70</span><span class="p">)</span>
</pre></div>


<h4 id="1643">16.4.3 限定选项</h4>
<h5 id="1-profiler">1. 选择制定profiler节点</h5>
<div class="hlcode"><pre><span class="n">attributes</span><span class="o">=</span><span class="p">[]</span>
<span class="n">ProfileOptionBuilder</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">attributes</span><span class="p">)</span>
</pre></div>


<h5 id="2-profiler">2. 选择消耗时间大于阈值的profiler节点</h5>
<div class="hlcode"><pre><span class="n">ProfileOptionBuilder</span><span class="o">.</span><span class="n">with_min_execution_time</span><span class="p">(</span>
    <span class="n">min_micros</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">min_accelerator_micros</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">min_cpu_micros</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
</pre></div>


<p>只显示消耗不少于“min_micros”的profiler节点。</p>
<h5 id="3-profiler">3. 选择消耗空间大于阈值的profiler节点</h5>
<div class="hlcode"><pre><span class="n">ProfileOptionBuilder</span><span class="o">.</span><span class="n">with_min_memory</span><span class="p">(</span>
    <span class="n">min_bytes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">min_peak_bytes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">min_residual_bytes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">min_output_bytes</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
</pre></div>


<h5 id="4-profiler">4. 选择运算大于阈值的profiler节点</h5>
<div class="hlcode"><pre><span class="n">ProfileOptionBuilder</span><span class="o">.</span><span class="n">with_min_float_operations</span><span class="p">(</span><span class="n">min_float_ops</span><span class="p">)</span>
</pre></div>


<h5 id="5-profiler">5. 选择参数数量大于阈值的profiler节点</h5>
<div class="hlcode"><pre><span class="n">ProfileOptionBuilder</span><span class="o">.</span><span class="n">with_min_parameters</span><span class="p">(</span><span class="n">min_params</span><span class="p">)</span>
</pre></div>


<p>仅显示不超过'min_params'参数的profiler节点。</p>
<h4 id="1644-">16.4.4 评估器选项构建器-构建</h4>
<div class="hlcode"><pre><span class="c">#构建profiling选项</span>
<span class="n">profile_opt_dict</span><span class="o">=</span><span class="n">ProfileOptionBuilder</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>
<span class="nb">type</span><span class="p">(</span><span class="n">profile_opt_dict</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span><span class="nb">dict</span> 
</pre></div>


<h4 id="1644">16.4.4 输出视图</h4>
<p>例子1：grpah view显示每个graph node运行时间，并输出到timeline</p>
<p>例子2：scope view显示模型中的参数数量分布 <br />
例子4： code view – 显示python代码的执行资源消耗 </p>
<h3 id="164_1">16.4 常用实例</h3>
<h4 id="1-flop">1. 浮点运算次数 flop</h4>
<div class="hlcode"><pre><span class="k">def</span> <span class="nf">get_flops</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
        <span class="n">run_meta</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">RunMetadata</span><span class="p">()</span>
        <span class="n">opts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfileOptionBuilder</span><span class="o">.</span><span class="n">float_operation</span><span class="p">()</span>

        <span class="c"># We use the Keras session graph in the call to the profiler.</span>
        <span class="n">flop</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">K</span><span class="o">.</span><span class="n">get_session</span><span class="p">()</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span>
                                        <span class="n">run_meta</span><span class="o">=</span><span class="n">run_meta</span><span class="p">,</span> <span class="n">cmd</span><span class="o">=</span><span class="s">&#39;op&#39;</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="n">opts</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">flop</span><span class="o">.</span><span class="n">total_float_ops</span>  <span class="c"># Prints the &quot;flop&quot; of the model.</span>
</pre></div>


<h4 id="2_13">2. 总参数量</h4>
<div class="hlcode"><pre><span class="c">## 统计参数量</span>
<span class="n">opts</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfileOptionBuilder</span><span class="o">.</span><span class="n">trainable_variables_parameter</span><span class="p">()</span>
<span class="n">param_stats</span> <span class="o">=</span> <span class="n">profiler</span><span class="o">.</span><span class="n">profile_name_scope</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="n">opts</span><span class="p">)</span>
<span class="c"># 总参数量</span>
<span class="k">print</span><span class="p">(</span><span class="s">&#39;总参数：&#39;</span><span class="p">,</span> <span class="n">param_stats</span><span class="o">.</span><span class="n">total_parameters</span><span class="p">)</span>
<span class="c"># 各scope参数量</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">param_stats</span><span class="o">.</span><span class="n">children</span><span class="p">:</span>
  <span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s">&#39;scope参数：&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">total_parameters</span><span class="p">)</span>
</pre></div>


<h4 id="3_6">3. 统计模型内存和耗时情况</h4>
<div class="hlcode"><pre><span class="n">builder</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfileOptionBuilder</span>
<span class="n">opts</span> <span class="o">=</span> <span class="n">builder</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">time_and_memory</span><span class="p">())</span>
<span class="c">#opts.with_step(1)</span>
<span class="n">opts</span><span class="o">.</span><span class="n">with_timeline_output</span><span class="p">(</span><span class="s">&#39;timeline.json&#39;</span><span class="p">)</span>
<span class="n">opts</span> <span class="o">=</span> <span class="n">opts</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>

<span class="c">#profiler.profile_name_scope(opts) # 只能保存单step的timeline</span>
<span class="n">profiler</span><span class="o">.</span><span class="n">profile_graph</span><span class="p">(</span><span class="n">opts</span><span class="p">)</span> <span class="c"># 保存各个step的timeline</span>
</pre></div>


<h4 id="4-profile">4. 给出使用profile工具给出建议</h4>
<div class="hlcode"><pre><span class="n">opts</span> <span class="o">=</span> <span class="p">{</span><span class="s">&#39;AcceleratorUtilizationChecker&#39;</span><span class="p">:</span> <span class="p">{},</span>
        <span class="s">&#39;ExpensiveOperationChecker&#39;</span><span class="p">:</span> <span class="p">{},</span>
        <span class="s">&#39;JobChecker&#39;</span><span class="p">:</span> <span class="p">{},</span>
        <span class="s">&#39;OperationChecker&#39;</span><span class="p">:</span> <span class="p">{}}</span>
<span class="n">profiler</span><span class="o">.</span><span class="n">advise</span><span class="p">(</span><span class="n">opts</span><span class="p">)</span>
</pre></div>


<h3 id="165-profile">16.5 基于profile的建议</h3>
<p>不要每次　sess.run 里面都加入 runmetadata 对象，这样会使得整个模型每次都去收集时间耗费及内存占用数据，只需要隔N次收集一次就行了；</p>
<h3 id="166-estimator-profile">16.6 在 estimator 中使用profile</h3>
<div class="hlcode"><pre><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">tfprof</span><span class="o">.</span><span class="n">ProfileContext</span><span class="p">(</span><span class="s">&#39;/tmp/train_dir&#39;</span><span class="p">,</span> <span class="n">dump_steps</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">])</span> <span class="k">as</span> <span class="n">pctx</span><span class="p">:</span>
  <span class="n">estimator</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> <span class="c"># any thing you want to profile</span>
</pre></div>


<p>在<code>/tmp/train_dir</code> 中就会得到<code>profile_10</code> 文件 </p>
<h3 id="167-keras">16.7 在keras 中使用</h3>
<div class="hlcode"><pre><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.client</span> <span class="kn">import</span> <span class="n">timeline</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>

<span class="n">run_options</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">RunOptions</span><span class="p">(</span><span class="n">trace_level</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">RunOptions</span><span class="o">.</span><span class="n">FULL_TRACE</span><span class="p">)</span>
<span class="n">run_metadata</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">RunMetadata</span><span class="p">()</span>

<span class="n">model</span> <span class="o">=</span> <span class="o">...</span>  <span class="c"># A Keras model</span>

<span class="n">fn</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="n">run_options</span><span class="p">,</span> <span class="n">run_metadata</span><span class="o">=</span><span class="n">run_metadata</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">variable</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">fn</span><span class="p">([</span><span class="n">x</span><span class="p">])</span>
    <span class="n">tl</span> <span class="o">=</span> <span class="n">timeline</span><span class="o">.</span><span class="n">Timeline</span><span class="p">(</span><span class="n">run_metadata</span><span class="o">.</span><span class="n">step_stats</span><span class="p">)</span>
    <span class="n">ctf</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">generate_chrome_trace_format</span><span class="p">()</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">&#39;timeline_</span><span class="si">%d</span><span class="s">.json&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="s">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">ctf</span><span class="p">)</span>
</pre></div>


<h1 id="17tfappflags">17.命令行传递参数(tf.app.flags)</h1>
<p><code>tf.app.flags</code> 是tensorflow自定义的用于命令行传递参数的函数，是对 <code>argparse</code>的封装，它还实现了一部分<code>python-gflags</code>功能。</p>
<p>我们建议您使用argparse或任何您喜欢的库来实现自己的标志解析。</p>
<p><a href="https://stackoverflow.com/questions/33932901/whats-the-purpose-of-tf-app-flags-in-tensorflow">stackoverflow: What's the purpose of tf.app.flags in TensorFlow?</a></p>
<p><strong>使用方法</strong><br />
我们还可以定义多种类型数据，如：</p>
<div class="hlcode"><pre><span class="c"># /bin/bash/python</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>


<span class="c">#1. 声明</span>
<span class="c">## 第一个是参数名称，第二个参数是默认值，第三个是参数描述</span>
<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_string</span><span class="p">(</span><span class="s">&#39;str_name&#39;</span><span class="p">,</span> <span class="s">&#39;default_value&#39;</span><span class="p">,</span><span class="s">&quot;description1&quot;</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_integer</span><span class="p">(</span><span class="s">&#39;int_name&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span><span class="s">&quot;description2&quot;</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_boolean</span><span class="p">(</span><span class="s">&#39;bool_name&#39;</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span> <span class="s">&quot;description3&quot;</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">DEFINE_float</span><span class="p">(</span><span class="s">&quot;float_name&quot;</span><span class="p">,</span><span class="mf">1.111</span><span class="p">,</span><span class="s">&quot;description4&quot;</span><span class="p">)</span>

<span class="n">FLAGS</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">FLAGS</span>

<span class="c">#必须带参数，否则：&#39;TypeError: main() takes no arguments (1 given)&#39;;   main的参数名随意定义，无要求</span>
<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>  
    <span class="k">print</span><span class="p">(</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">str_name</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">int_name</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">bool_name</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">float_name</span><span class="p">)</span>
<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>  <span class="c">#执行main函数</span>
</pre></div>


<div class="hlcode"><pre><span class="cp"># shell </span>
<span class="n">python</span> <span class="s">&quot;xxx.py&quot;</span> <span class="o">--</span><span class="n">str_name</span> <span class="n">hahaha</span>
</pre></div>


<p><strong>NOTE</strong><br />
在python中可以使用修饰器@对FLAGS进行更新</p>
<h1 id="18-tfnnbatch_normalization">18.归一化  tf.nn.batch_normalization</h1>
<p>Aliases:<br />
tf.compat.v1.nn.batch_normalization<br />
tf.compat.v2.nn.batch_normalization<br />
tf.nn.batch_normalization</p>
<div class="hlcode"><pre><span class="n">y</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">mean</span><span class="p">,</span>
    <span class="n">variance</span><span class="p">,</span>
    <span class="n">offset</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">,</span>
    <span class="n">variance_epsilon</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="bp">None</span>
<span class="p">)</span>
</pre></div>


<p>Normalizes a tensor by mean and variance, and applies (optionally) a scale  to it,as well as an offset :<br />
$$y=scale*\frac{x-mean}{variance}+offset$$<br />
mean, variance, offset and scale are all expected to be of one of two shapes</p>
<h1 id="19-tfgfile">19. tf.gfile文件操作模块</h1>
<p>在文档中，该模块有两个类，FastGFile和GFile，但是查看源码可发现，二者其实是一样的。</p>
<h3 id="_8">复制</h3>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">Copy</span><span class="p">(</span><span class="n">oldpath</span><span class="p">,</span> <span class="n">newpath</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>


<p>将文件从oldpath拷贝到newpath，path是要包含文件名的。如果overwrite为False，当newpath已存在时会产生错误：AlreadyExistsError: file already exists。</p>
<h3 id="_9">删除</h3>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">DeleteRecursively</span><span class="p">(</span><span class="n">dirname</span><span class="p">)</span>
<span class="c">#删除dirname目录下的所有内容。</span>
</pre></div>


<h3 id="filename">判断路径filename是否存在</h3>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">Exists</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="c">#判断路径filename是否存在，filename可以是文件路径也可以是文件夹路径。</span>
</pre></div>


<h3 id="dirname">判断路径dirname是否为一目录</h3>
<p>tf.gfile.IsDirectory(dirname)</p>
<p>tf.gfile.ListDirectory函数<br />
tf.gfile.ListDirectory(dirname)</p>
<p>返回目录dirname下的所有内容，包含子目录，但不包含‘.’和‘..’。返回形式为：<br />
[filename1, filename2, ... filenameN]</p>
<p>tf.gfile.MkDir<br />
tf.gfile.MkDir(dirname)</p>
<p>创建目录dirname。注意上层目录必须存在，如：<br />
tf.gfile.MkDir('./a/b/c')</p>
<p>则./a/b必须存在。<br />
tf.gfile.MakeDirs函数<br />
tf.gfile.MakeDirs(dirname)</p>
<p>创建目录dirname。与MkDir不同，上层目录可以不存在。<br />
tf.gfile.Remove函数<br />
tf.gfile.Remove(filename)</p>
<p>删除文件filename。<br />
tf.gfile.Rename函数<br />
tf.gfile.Rename(oldname, newname, overwrite=False)</p>
<p>重命名/移动文件/目录。<br />
tf.gfile.Walk函数<br />
tf.gfile.Walk( top, in_order=True)</p>
<p>返回一生成器，可用于递归目录树，top为顶层目录。若in_order为True，则按顺序递归。使用方式：<br />
for i in tf.gfile.Walk('./datasets/'):<br />
    print(i)</p>
<p>输出格式为(the pathname of a directory, followed by lists of all its subdirectories and leaf files)：<br />
(dirname, [subdirname, subdirname, ...], [filename, filename, ...]) </p>
<p>也可以使用next()函数来迭代。<br />
tf.gfile.FastGFile类<br />
提供了文件的读写操作。注意对于非UTF-8的文件，例化对象时，使用'rb'模式。</p>
<h1 id="20">20. 计算梯度</h1>
<div class="hlcode"><pre><span class="n">d</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span> <span class="n">weight1</span><span class="p">)</span>
</pre></div>


<h1 id="x1-tensorflow">X1. 基于Tensorflow 的模型训练 基本步骤</h1>
<ol>
<li>
<p>数据输入 （ETL）<br />
 1.1 Extrat提取（来源：文件、数据库）<br />
 1.2 Trans 转换（最后repeat）<br />
 1.3 Load 加载 </p>
</li>
<li>
<p>图的绘制<br />
 graph定义了computation，它不计算任何东西，不包含任何值，只是定义了你在代码中指定的操作。其中图中最重要的工作就是定义op<br />
2.1.  定义 op<br />
先定义 train op<br />
最后定义init op</p>
</li>
<li>
<p>会话的运行</p>
<blockquote>
<p>执行op</p>
</blockquote>
</li>
</ol>
<h1 id="x2-tensorflow">X2. Tensorflow 的运行过程</h1>
<ol>
<li>
<p>图构建：用户在client中基于TensorFlow的多语言编程接口，添加算子，完成计算图的构造。</p>
</li>
<li>
<p>图传递：client开启session，通过它建立和master之间的连接。执行session.run()时，将构造好的graph序列化为graphDef后，以protobuf的格式传递给master。</p>
</li>
<li>
<p>图剪枝：master根据session.run()传递的fetches和feeds列表，反向遍历全图full graph，实施剪枝，得到最小依赖子图</p>
</li>
<li>
<p>图分裂：master将最小子图分裂为多个Graph Partition，并注册到多个worker上。一个worker对应一个Graph Partition。</p>
</li>
<li>
<p>图二次分裂：worker根据当前可用硬件资源，如CPU GPU，将Graph Partition按照op算子设备约束规范（例如tf.device(’/cpu:0’)，二次分裂到不同设备上。每个计算设备对应一个Graph Partition。</p>
</li>
<li>
<p>图运行：对于每一个计算设备，worker依照op在kernel中的实现，完成op的运算。设备间数据通信可以使用send/recv节点，而worker间通信，则使用GRPC或RDMA协议。</p>
</li>
</ol>
<h1 id="_10">参考文献</h1>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>fff&#160;<a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
</div>
<div id="renote">
  <HR style=" FILTER: alpha (opacity = 100, finishopacity =0 , style= 3 )" width="80%" color=#987 cb 9 SIZE=3>
  <p>如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多!</p>
  <img src="/Wiki/static/images/pay.jpg" width="25%">
</div>

    </div>
    <div id="footer">
        <span>
            Copyright © 2021 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>

    
</body>
<script>
    function changeImgurl(site_root_url) {
        var images = document.images;
        var site_root = site_root_url;
        for (i = 0, len = images.length; i < len; i++) {
            image = images[i];
            image_src = image.src;
            if (image_src.search("attach") >= 0) {
                re_image_src = image_src.slice(image_src.search("attach"));
                abs_image_src = (site_root.endsWith("/")) ? site_root + re_image_src : site_root + "/" +
                    re_image_src;
                image.src = abs_image_src;
            }
        }
    }
    var site_root_url = "/Wiki";
    changeImgurl(site_root_url);
    let isMathjaxConfig = false; // 防止重复调用Config，造成性能损耗
    const initMathjaxConfig = () => {
        if (!window.MathJax) {
            return;
        }
        window.MathJax.Hub.Config({
            showProcessingMessages: false, //关闭js加载过程信息
            messageStyle: "none", //不显示信息
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
                displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
                skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
            },
            "HTML-CSS": {
                availableFonts: ["STIX", "TeX"], //可选字体
                showMathMenu: false //关闭右击菜单显示
            }
        });
        isMathjaxConfig = true; //
    };
    if (isMathjaxConfig === false) {
        // 如果：没有配置MathJax
        initMathjaxConfig();
    };
</script>

</html>