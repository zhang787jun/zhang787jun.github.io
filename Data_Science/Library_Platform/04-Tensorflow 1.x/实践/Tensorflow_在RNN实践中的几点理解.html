<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <title>Tensorflow 基本操作 - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
</head>

<body>

    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="/Wiki/">Home</a>&nbsp;»&nbsp;<a href="/Wiki/#Data_Science">Data_Science</a>&nbsp;»&nbsp;<a href="/Wiki/#-Library_Platform">Library_Platform</a>&nbsp;»&nbsp;<a href="/Wiki/#-04-Tensorflow 1.x">04-Tensorflow 1.x</a>&nbsp;»&nbsp;<a href="/Wiki/#-实践">实践</a>&nbsp;»&nbsp;Tensorflow 基本操作</div>
</div>
<div class="clearfix"></div>
<div id="title">Tensorflow 基本操作</div>
<div id="content">
  <h1 id="rnn">关于RNN的几点理解</h1>
<div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#rnn">关于RNN的几点理解</a><ul>
<li><a href="#0-lstm">0. LSTM</a><ul>
<li><a href="#_1">关于门</a><ul>
<li><a href="#-forget-gate">- 遗忘门 Forget gate</a></li>
<li><a href="#-update-gateinuput-gat">- 更新门/输入门 Update gate/Inuput gat</a></li>
<li><a href="#-updating-the-cell">- 更新细胞 Updating the cell （基于 更新门和遗忘门）</a></li>
<li><a href="#-output-gate">- 输出门 Output gate</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#lstm">LSTM 变体</a></li>
<li><a href="#1-lstmcell">1. LSTMCell 实例</a></li>
<li><a href="#2-bbtp">2. 时间 BBTP 反向传播</a></li>
<li><a href="#3">3. 梯度消逝与爆炸</a></li>
</ul>
</li>
</ul>
</div>
<h2 id="0-lstm">0. LSTM</h2>
<p>LSTM cell 单元 的输入</p>
<p>$X={x_1,x_2,...x_n}$,  X.shape=(counts,n,...)</p>
<table>
<thead>
<tr>
<th>编号</th>
<th>符号</th>
<th>描述</th>
<th>shape</th>
<th>tf/keras</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.</td>
<td i-1="i-1">$C_</td>
<td>$i-1$ cell 的单元状态，记忆细胞 （Memory Cell）</td>
<td></td>
<td>初始化，</td>
</tr>
<tr>
<td>2.</td>
<td i-1="i-1">$H_</td>
<td>$i-1$ cell 的输出值，输出隐含层（Hidden Layer）</td>
<td>shape=(counts,1,units)</td>
<td></td>
</tr>
<tr>
<td>3.</td>
<td i="i">$x_</td>
<td>$i$ cell 的输入量</td>
<td>$x_i$.shape=(counts,1,...)</td>
<td>数据输入</td>
</tr>
</tbody>
</table>
<p>LSTM cell 单元 的输出</p>
<table>
<thead>
<tr>
<th>编号</th>
<th>符号</th>
<th>描述</th>
<th>shape</th>
<th>tf/keras</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.</td>
<td i="i">$C_</td>
<td>$i$ cell 的单元状态，记忆细胞 （Memory Cell）</td>
<td></td>
<td>初始化，</td>
</tr>
<tr>
<td>2.</td>
<td i="i">$H_</td>
<td>$i$ cell 的输出值，输出隐含层（Hidden Layer）</td>
<td>shape=(counts,1,units)</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="_1">关于门</h3>
<p><strong>遗忘门</strong>的功能是决定应丢弃或保留哪些信息<br />
<strong>输入门(更新门)</strong> 用于更新细胞状态。<br />
<strong>输出门</strong> 用来确定下一个隐藏状态的值</p>
<table>
<thead>
<tr>
<th>gate 门</th>
<th>范围</th>
</tr>
</thead>
<tbody>
<tr>
<td _langle="\langle" _rangle="\rangle" t="t">$\Gamma_f^</td>
<td>(0,1)</td>
</tr>
<tr>
<td _langle="\langle" _rangle="\rangle" t="t">$\Gamma_i^</td>
<td>(0,1)</td>
</tr>
<tr>
<td _langle="\langle" _rangle="\rangle" t="t">$\Gamma_o^</td>
<td>(0,1)</td>
</tr>
</tbody>
</table>
<h4 id="-forget-gate">- 遗忘门 Forget gate</h4>
<p>$$\Gamma_f^{\langle t \rangle} = \sigma(W_f[h^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_f)\tag{1} $$</p>
<p>$b_f$ :遗忘门的偏执，在tensorflow 中 体现为:<code>tf.nn.rnn_cell.BasicLSTMCell(num_units,forget_bias=1.0)</code><br />
$h^{\langle t-1 \rangle}$: 上一个cell 的 hidden state<br />
$W_f$: 权重 recurrent_initializer对应的权重<br />
$x^{\langle t \rangle}$: 当下输入 </p>
<p>参数 num_units=128 的话，对于公式 (1) </p>
<table>
<thead>
<tr>
<th align="right">参数</th>
<th align="left">维度</th>
</tr>
</thead>
<tbody>
<tr>
<td _langle="\langle" _rangle="\rangle" align="right" t-1="t-1">$h^</td>
<td align="left">shape=(128,1)</td>
</tr>
<tr>
<td _langle="\langle" _rangle="\rangle" align="right" t="t">$x^</td>
<td align="left">shape=(28,1)</td>
</tr>
<tr>
<td _langle="\langle" _rangle="\rangle" align="right" t-1="t-1">$[h^</td>
<td align="left">shape=(28,1)+(128,1)=(156,1)</td>
</tr>
<tr>
<td align="right">$W_f$</td>
<td align="left">shape=(128,156)</td>
</tr>
<tr>
<td align="right">$b_f$</td>
<td align="left">shape=(128,1)</td>
</tr>
</tbody>
</table>
<p>由于激活函数的存在 $0&lt;=\Gamma_f^{\langle t \rangle}&lt;=1$<br />
如果 $\Gamma_f^{\langle t \rangle}==0$ 意味着 当前 LSTM cell 将忽略 上一个cell 传来的 $h^{\langle t-1 \rangle}$</p>
<p>如果 $\Gamma_f^{\langle t \rangle}==1$ 意味着 当前 LSTM cell 将保存 上一个cell 传来的 $h^{\langle t-1 \rangle}$</p>
<h4 id="-update-gateinuput-gat">- 更新门/输入门 Update gate/Inuput gat</h4>
<p>$$\Gamma_u^{\langle t \rangle} = \sigma(W_u[h^{\langle t-1 \rangle}, x^{{t}}] + b_u)\tag{2} $$ </p>
<p>Similar to the forget gate, here $\Gamma_u^{\langle t \rangle}$ is again a vector of values between 0 and 1. This will be multiplied element-wise with $\tilde{c}^{\langle t \rangle}$, in order to compute $c^{\langle t \rangle}$.</p>
<h4 id="-updating-the-cell">- 更新细胞 Updating the cell （基于 更新门和遗忘门）</h4>
<p>To update the new subject we need to create a new vector of numbers that we can add to our previous cell state. The equation we use is: </p>
<p>$$ \tilde{c}^{\langle t \rangle} = \tanh(W_c[h^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_c)\tag{3} $$</p>
<p>Finally, the new cell state is: </p>
<p>$$ c^{\langle t \rangle} = \Gamma_f^{\langle t \rangle}<em> c^{\langle t-1 \rangle} + \Gamma_u^{\langle t \rangle} </em>\tilde{c}^{\langle t \rangle} \tag{4} $$</p>
<p>tanh 对应  </p>
<h4 id="-output-gate">- 输出门 Output gate</h4>
<p>To decide which outputs we will use, we will use the following two formulas: </p>
<p>$$ \Gamma_o^{\langle t \rangle}=  \sigma(W_o[h^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_o)\tag{5}$$ <br />
$$ h^{\langle t \rangle} = \Gamma_o^{\langle t \rangle}* \tanh(c^{\langle t \rangle})\tag{6} $$</p>
<h2 id="lstm">LSTM 变体</h2>
<h2 id="1-lstmcell">1. LSTMCell 实例</h2>
<p>Keras LSTM 参数解析 </p>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span>

<span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span>
<span class="o">|</span><span class="n">_tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span>
    <span class="o">|</span><span class="n">_tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">RNN</span>
        <span class="o">|</span><span class="n">_tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span>

<span class="n">__init__</span><span class="p">(</span>
    <span class="n">units</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s">&#39;tanh&#39;</span><span class="p">,</span>
    <span class="n">recurrent_activation</span><span class="o">=</span><span class="s">&#39;hard_sigmoid&#39;</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">&#39;glorot_uniform&#39;</span><span class="p">,</span>
    <span class="n">recurrent_initializer</span><span class="o">=</span><span class="s">&#39;orthogonal&#39;</span><span class="p">,</span>
    <span class="n">bias_initializer</span><span class="o">=</span><span class="s">&#39;zeros&#39;</span><span class="p">,</span>
    <span class="n">unit_forget_bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">kernel_regularizer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">recurrent_regularizer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">bias_regularizer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">activity_regularizer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">kernel_constraint</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">recurrent_constraint</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">bias_constraint</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">implementation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">return_sequences</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">return_state</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">go_backwards</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">stateful</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">unroll</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>

<span class="n">__call__</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">initial_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="c"># a tensor or list of tensors representing the initial state of the RNN layer.</span>
    <span class="n">constants</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</pre></div>


<ol>
<li>return_sequences=False; return_state=False</li>
</ol>
<div class="hlcode"><pre><span class="n">x</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">cell_num</span><span class="o">=</span><span class="mi">2</span>
<span class="n">a</span><span class="o">=</span><span class="n">LSTM</span><span class="p">(</span><span class="n">cell_num</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span> <span class="p">(</span><span class="s">&quot;cell_num is {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cell_num</span><span class="p">))</span>
<span class="n">sess</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">x_instance</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]],[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]],[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]],[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]]])</span>
<span class="k">print</span> <span class="p">(</span><span class="s">&quot;x input counts/shape[0] is {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x_instance</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
<span class="n">h_t</span><span class="o">=</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span><span class="n">x_instance</span><span class="p">})</span>
<span class="k">print</span> <span class="p">(</span><span class="s">&quot;h_t shape is {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

<span class="c">#cell_num is 2</span>
<span class="c">#x input counts/shape[0] is 4</span>
<span class="c">#result shape is (4, 2)</span>
</pre></div>


<p>h_t 是 $h^{\langle t \rangle}$ ,$h^{\langle t \rangle} shape=()$ </p>
<ol>
<li>return_sequences=True; return_state=False</li>
</ol>
<div class="hlcode"><pre><span class="n">x</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">cell_num</span><span class="o">=</span><span class="mi">2</span>
<span class="n">a</span><span class="o">=</span><span class="n">LSTM</span><span class="p">(</span><span class="n">cell_num</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="bp">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span> <span class="p">(</span><span class="s">&quot;cell_num is {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cell_num</span><span class="p">))</span>
<span class="n">sess</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">x_instance</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]],[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]],[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]],[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]]])</span>
<span class="k">print</span> <span class="p">(</span><span class="s">&quot;x input counts/shape[0] is {},x input timestamp/shape[1] is {},shape is {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x_instance</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">x_instance</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">x_instance</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
<span class="n">H_all</span><span class="o">=</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span><span class="n">x_instance</span><span class="p">})</span>
<span class="k">print</span> <span class="p">(</span><span class="s">&quot;H_all shape is {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">H_all</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="c"># cell_num is 2</span>
<span class="c"># x input counts/shape[0] is 4,x input timestamp/shape[1] is 1,shape is (4, 1, 3)</span>
<span class="c"># H_all shape is (4, 1, 2)</span>
</pre></div>


<p>H_all 是 $np.array(h^{\langle t \rangle},h^{\langle t-1 \rangle},h^{\langle t-2 \rangle},...,h^{\langle 1 \rangle})$</p>
<ol>
<li>return_sequences=False; return_state=True</li>
</ol>
<div class="hlcode"><pre><span class="n">x</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">cell_num</span><span class="o">=</span><span class="mi">2</span>

<span class="n">a</span><span class="o">=</span><span class="n">LSTM</span><span class="p">(</span><span class="n">cell_num</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="bp">True</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span> <span class="p">(</span><span class="s">&quot;cell_num is {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cell_num</span><span class="p">))</span>
<span class="n">sess</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">x_instance</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]],[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]],[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]],[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]]])</span>
<span class="k">print</span> <span class="p">(</span><span class="s">&quot;x input counts/shape[0] is {},x input timestamp/shape[1] is {},shape is {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x_instance</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">x_instance</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">x_instance</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
<span class="n">H_all</span><span class="p">,</span> <span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span>  <span class="o">=</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span><span class="n">x_instance</span><span class="p">})</span>
</pre></div>


<p>此时 H_all==h_t</p>
<ol>
<li>return_sequences=True; return_state=True</li>
</ol>
<div class="hlcode"><pre><span class="n">x</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">cell_num</span><span class="o">=</span><span class="mi">2</span>
<span class="n">a</span><span class="o">=</span><span class="n">LSTM</span><span class="p">(</span><span class="n">cell_num</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="bp">True</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span> <span class="p">(</span><span class="s">&quot;cell_num is {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cell_num</span><span class="p">))</span>
<span class="n">sess</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">x_instance</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]],[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]],[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]],[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]]])</span>
<span class="k">print</span> <span class="p">(</span><span class="s">&quot;x input counts/shape[0] is {},x input timestamp/shape[1] is {},shape is {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x_instance</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">x_instance</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">x_instance</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
<span class="n">H_all</span><span class="p">,</span> <span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span>  <span class="o">=</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span><span class="n">x_instance</span><span class="p">})</span>
</pre></div>


<p>H_all 是 $np.array(h^{\langle t \rangle},h^{\langle t-1 \rangle},h^{\langle t-2 \rangle},...,h^{\langle 1 \rangle})$</p>
<p>经过初步调查，常用的LSTM层有Keras.layers.LSTM 和 Tensorflow.contrib.nn.LSTMCell 及 Tensorflow.nn.rnn_cell.LSTMCell ，其中后面两个的实现逻辑是一样的。</p>
<div class="hlcode"><pre><span class="n">LSTMCell</span> <span class="o">=</span>
<span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="n">num_units</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">rnn_cell</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="n">num_units</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span>
    <span class="n">cell</span><span class="p">,</span>
    <span class="n">return_sequences</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">return_state</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">go_backwards</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">stateful</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">unroll</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span> <span class="c"># 推荐</span>
</pre></div>


<h2 id="2-bbtp">2. 时间 BBTP 反向传播</h2>
<p>对于记忆细胞 M</p>
<div class="hlcode"><pre><span class="c"># h_all_1 shape=[counts,time_steps,cell_num_1]</span>
<span class="c"># h_all_2 = LSTM(32, return_sequences=True,</span>
<span class="c">#               return_state=False, dropout=0, recurrent_initializer=&#39;orthogonal&#39;, kernel_initializer=&#39;orthogonal&#39;)(h_all_0)</span>
<span class="c"># h_all_1 shape=[counts,time_steps,cell_num_2]</span>
<span class="c"># h_all_3 = LSTM(8, return_sequences=False,</span>
<span class="c">#               return_state=False, dropout=0, recurrent_initializer=&#39;orthogonal&#39;, kernel_initializer=&#39;orthogonal&#39;)(h_all_2)</span>
<span class="c"># h_all_3 shape=[counts,time_steps,cell_num_2]</span>
<span class="c"># lay4 = GlobalMaxPool1D()(h_all_3)</span>
<span class="c"># lay3 shape =[counts,cell_num_2]</span>
<span class="c"># lay4 = tf.layers.dense(lay3, units=20, activation=&quot;relu&quot;)</span>
<span class="c"># output = activation(dot(input, kernel)+bias)</span>
<span class="c"># lay4 shape =[counts,units_nums]</span>
<span class="c"># print(&quot; lay4 shape is {}&quot;.format(lay4.shape))</span>
</pre></div>


<h2 id="3">3. 梯度消逝与爆炸</h2>
</div>
<div id="renote">
  <HR style=" FILTER: alpha (opacity = 100, finishopacity =0 , style= 3 )" width="80%" color=#987 cb 9 SIZE=3>
  <p>如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多!</p>
  <img src="/Wiki/static/images/pay.jpg" width="25%">
</div>

    </div>
    <div id="footer">
        <span>
            Copyright © 2021 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>

    
</body>
<script>
    function changeImgurl(site_root_url) {
        var images = document.images;
        var site_root = site_root_url;
        for (i = 0, len = images.length; i < len; i++) {
            image = images[i];
            image_src = image.src;
            if (image_src.search("attach") >= 0) {
                re_image_src = image_src.slice(image_src.search("attach"));
                abs_image_src = (site_root.endsWith("/")) ? site_root + re_image_src : site_root + "/" +
                    re_image_src;
                image.src = abs_image_src;
            }
        }
    }
    var site_root_url = "/Wiki";
    changeImgurl(site_root_url);
    let isMathjaxConfig = false; // 防止重复调用Config，造成性能损耗
    const initMathjaxConfig = () => {
        if (!window.MathJax) {
            return;
        }
        window.MathJax.Hub.Config({
            showProcessingMessages: false, //关闭js加载过程信息
            messageStyle: "none", //不显示信息
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
                displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
                skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
            },
            "HTML-CSS": {
                availableFonts: ["STIX", "TeX"], //可选字体
                showMathMenu: false //关闭右击菜单显示
            }
        });
        isMathjaxConfig = true; //
    };
    if (isMathjaxConfig === false) {
        // 如果：没有配置MathJax
        initMathjaxConfig();
    };
</script>

</html>