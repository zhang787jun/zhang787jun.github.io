<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <title>Tensorflow 里的优化器 - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
</head>

<body>

    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="/Wiki/">Home</a>&nbsp;»&nbsp;<a href="/Wiki/#Data_Science">Data_Science</a>&nbsp;»&nbsp;<a href="/Wiki/#-Library_Platform">Library_Platform</a>&nbsp;»&nbsp;<a href="/Wiki/#-04-Tensorflow 1.x">04-Tensorflow 1.x</a>&nbsp;»&nbsp;<a href="/Wiki/#-模块分解笔记">模块分解笔记</a>&nbsp;»&nbsp;Tensorflow 里的优化器</div>
</div>
<div class="clearfix"></div>
<div id="title">Tensorflow 里的优化器</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#1-tensorflow">1. Tensorflow 里的优化器</a><ul>
<li><a href="#11">1.1. 概述</a></li>
<li><a href="#12-tftrainoptimizer">1.2. tf.train.Optimizer</a><ul>
<li><a href="#121-tftraingradientdescentoptimizer">1.2.1. tf.train.GradientDescentOptimizer</a></li>
<li><a href="#122-tftrainadadeltaoptimizer">1.2.2. tf.train.AdadeltaOptimizer</a></li>
<li><a href="#123-tftrainadagradoptimizer">1.2.3. tf.train.AdagradOptimizer</a></li>
<li><a href="#124-tftrainmomentumoptimizer">1.2.4. tf.train.MomentumOptimizer</a></li>
<li><a href="#125-tftrainadamoptimizer">1.2.5. tf.train.AdamOptimizer</a></li>
<li><a href="#126-tftrainftrloptimizer">1.2.6. tf.train.FtrlOptimizer</a></li>
<li><a href="#127-tftrainrmspropoptimizer">1.2.7. tf.train.RMSPropOptimizer</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#2">2. 梯度计算与截断</a><ul>
<li><a href="#21">2.1. 梯度计算</a></li>
<li><a href="#22">2.2. 梯度截断</a></li>
</ul>
</li>
<li><a href="#3-decaying-the-learning-rate">3. 学习率衰减(Decaying the learning rate)</a></li>
<li><a href="#4-moving-averages">4. 移动平均(Moving Averages)</a></li>
<li><a href="#5-coordinator-and-queuerunner">5. 协调器和队列运行器(Coordinator and QueueRunner)</a></li>
<li><a href="#6-distributed-execution">6. 分布示执行(Distributed execution)</a></li>
<li><a href="#7-summary-operations">7. 汇总操作(Summary Operations)</a></li>
<li><a href="#8-training-utilities">8. 训练的通用函数及其他(Training utilities)</a></li>
<li><a href="#9-utilities">9. 共用(Utilities)</a></li>
<li><a href="#10-gradient-checking">10. 梯度检查(Gradient checking)</a></li>
</ul>
</div>
<h1 id="1-tensorflow">1. Tensorflow 里的优化器</h1>
<h2 id="11">1.1. 概述</h2>
<p>优化器（Optimizer）。实际上代表的是一组Operation 操作，因为将Optimizer加入图之后，tensorflow会自动为图加入两类Operation，分别是<br />
1. compute_gradients() <strong>计算梯度</strong>；<br />
2. apply_gradients() <strong>更新梯度</strong>。</p>
<p>前者接收代表损失的Tensor输入, 输出梯度Tensor，后者对梯度Tensor作一些聚合并据此用tf.assign()对模型参数变量进行更新</p>
<div class="hlcode"><pre><span class="n">digraph</span> <span class="n">A</span><span class="p">{</span>
  <span class="n">Optimizer</span><span class="o">-&gt;</span><span class="err">计算梯度</span>
  <span class="n">Optimizer</span><span class="o">-&gt;</span><span class="err">更新梯度</span>
  <span class="err">计算梯度</span><span class="o">-&gt;</span><span class="err">计算方法</span>
  <span class="err">计算梯度</span><span class="o">-&gt;</span><span class="err">计算方式</span>
  <span class="err">计算方法</span><span class="o">-&gt;</span><span class="err">梯度截断</span>
  <span class="err">计算方法</span><span class="o">-&gt;</span><span class="err">梯度均值</span>
  <span class="err">计算方式</span><span class="o">-&gt;</span><span class="err">单机</span>
  <span class="err">计算方式</span><span class="o">-&gt;</span><span class="err">分布式</span>

<span class="p">}</span>
</pre></div>


<h2 id="12-tftrainoptimizer">1.2. tf.train.Optimizer</h2>
<p>GradientDescentOptimizer<br />
AdagradOptimizer<br />
AdagradDAOptimizer<br />
MomentumOptimizer<br />
AdamOptimizer<br />
FtrlOptimizer<br />
RMSPropOptimizer</p>
<p>本文所讲的内容主要为以下列表中相关函数。函数training()通过<strong>梯度下降法</strong>为最小化损失函数增加了相关的优化操作，在训练过程中，并基于一定的学习率进行梯度优化训练：</p>
<p>与其他tensorflow操作类似，这些训练操作都需要在tf.session会话中进行<br />
<strong>基本操作</strong></p>
<div class="hlcode"><pre><span class="c"># 1. 先实例化一个优化器Optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c">#2. 设置 一个用于记录全局训练步骤的单值 变量</span>
<span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;global_step&#39;</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c"># 3.使用minimize()操作。</span>
<span class="c">## 该操作不仅可以1. 优化更新训练的模型参数，2.也可以为全局步骤(global step)计数</span>
<span class="n">train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">global_step</span><span class="p">)</span>
</pre></div>


<table>
<thead>
<tr>
<th>操作组</th>
<th>操作</th>
</tr>
</thead>
<tbody>
<tr>
<td>Training</td>
<td>Optimizers，Gradient Computation，Gradient Clipping，Distributed execution</td>
</tr>
<tr>
<td>Testing</td>
<td>Unit tests，Utilities，Gradient checking</td>
</tr>
</tbody>
</table>
<p><strong>Slots</strong></p>
<p>一些optimizer的之类，比如 MomentumOptimizer 和 AdagradOptimizer 分配和管理着额外的用于训练的变量。这些变量称之为'Slots'，Slots有相应的名称，可以向optimizer访问的slots名称。有助于在log debug一个训练算法以及报告slots状态</p>
<table>
<thead>
<tr>
<th>操作</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.train.Optimizer.get_slot_names()</td>
<td>返回一个由Optimizer所创建的slots的名称列表</td>
</tr>
<tr>
<td>tf.train.Optimizer.get_slot(var, name)</td>
<td>返回一个name所对应的slot，name是由Optimizer为var所创建</td>
</tr>
</tbody>
</table>
<p>var为用于传入 minimize() 或 apply_gradients()的变量</p>
<h3 id="121-tftraingradientdescentoptimizer">1.2.1. tf.train.GradientDescentOptimizer</h3>
<p>使用梯度下降算法的Optimizer</p>
<div class="hlcode"><pre><span class="n">Optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> 
<span class="n">use_locking</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;GradientDescent&#39;</span><span class="p">)</span>  <span class="err">构建一个新的梯度下降优化器</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">)</span>
</pre></div>


<h3 id="122-tftrainadadeltaoptimizer">1.2.2. tf.train.AdadeltaOptimizer</h3>
<p>使用Adadelta算法的 Optimizer</p>
<div class="hlcode"><pre><span class="c"># 创建Adadelta优化器</span>
<span class="n">Optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdadeltaOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span><span class="n">rho</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span><span class="n">use_locking</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;Adadelta&#39;</span><span class="p">)</span>
</pre></div>


<h3 id="123-tftrainadagradoptimizer">1.2.3. tf.train.AdagradOptimizer</h3>
<p>使用Adagrad算法的Optimizer</p>
<div class="hlcode"><pre><span class="c"># 创建Adagrad优化器</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdagradOptimizer</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> 
<span class="n">initial_accumulator_value</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
<span class="n">use_locking</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;Adagrad&#39;</span><span class="p">)</span>  
</pre></div>


<h3 id="124-tftrainmomentumoptimizer">1.2.4. tf.train.MomentumOptimizer</h3>
<p>使用Momentum算法的Optimizer</p>
<div class="hlcode"><pre><span class="c"># 创建momentum优化器</span>
<span class="n">Optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">MomentumOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> 
<span class="n">momentum</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
<span class="n">name</span><span class="o">=</span><span class="s">&#39;Momentum&#39;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>    
<span class="c"># momentum：动量，一个tensor或者浮点值</span>
</pre></div>


<h3 id="125-tftrainadamoptimizer">1.2.5. tf.train.AdamOptimizer</h3>
<p>使用Adam 算法的Optimizer</p>
<div class="hlcode"><pre><span class="c"># 创建Adam优化器</span>
<span class="n">Optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
<span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span>
<span class="n">use_locking</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;Adam&#39;</span><span class="p">)</span> 
</pre></div>


<h3 id="126-tftrainftrloptimizer">1.2.6. tf.train.FtrlOptimizer</h3>
<p>使用FTRL 算法的Optimizer</p>
<div class="hlcode"><pre><span class="c"># 创建FTRL算法优化器</span>
<span class="n">Optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">FtrlOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> 
<span class="n">learning_rate_power</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> 
<span class="n">initial_accumulator_value</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
<span class="n">l1_regularization_strength</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> 
<span class="n">l2_regularization_strength</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span class="n">use_locking</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;Ftrl&#39;</span><span class="p">)</span> 
</pre></div>


<h3 id="127-tftrainrmspropoptimizer">1.2.7. tf.train.RMSPropOptimizer</h3>
<p>使用RMSProp算法的Optimizer</p>
<div class="hlcode"><pre><span class="c"># 创建RMSProp算法优化器</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RMSPropOptimizer</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> 
<span class="n">decay</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span> 
<span class="n">use_locking</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;RMSProp&#39;</span><span class="p">)</span>
</pre></div>


<p>Adam 的基本运行方式，首先初始化：</p>
<div class="hlcode"><pre>m_0 <span class="o">&lt;-</span> <span class="m">0</span> <span class="p">(</span>Initialize initial <span class="m">1</span>st moment vector<span class="p">)</span>
v_0 <span class="o">&lt;-</span> <span class="m">0</span> <span class="p">(</span>Initialize initial <span class="m">2</span>nd moment vector<span class="p">)</span>
t <span class="o">&lt;-</span> <span class="m">0</span> <span class="p">(</span>Initialize timestep<span class="p">)</span>
</pre></div>


<p>在论文中的 section2 的末尾所描述了更新规则，该规则使用梯度g来更新变量：</p>
<div class="hlcode"><pre>t <span class="o">&lt;-</span> t <span class="o">+</span> <span class="m">1</span>
lr_t <span class="o">&lt;-</span> learning_rate <span class="o">*</span> sqrt<span class="p">(</span><span class="m">1</span> <span class="o">-</span> beta2<span class="o">^</span>t<span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="m">1</span> <span class="o">-</span> beta1<span class="o">^</span>t<span class="p">)</span>

m_t <span class="o">&lt;-</span> beta1 <span class="o">*</span> m_<span class="p">{</span>t<span class="m">-1</span><span class="p">}</span> <span class="o">+</span> <span class="p">(</span><span class="m">1</span> <span class="o">-</span> beta1<span class="p">)</span> <span class="o">*</span> g
v_t <span class="o">&lt;-</span> beta2 <span class="o">*</span> v_<span class="p">{</span>t<span class="m">-1</span><span class="p">}</span> <span class="o">+</span> <span class="p">(</span><span class="m">1</span> <span class="o">-</span> beta2<span class="p">)</span> <span class="o">*</span> g <span class="o">*</span> g
variable <span class="o">&lt;-</span> variable <span class="o">-</span> lr_t <span class="o">*</span> m_t <span class="o">/</span> <span class="p">(</span>sqrt<span class="p">(</span>v_t<span class="p">)</span> <span class="o">+</span> epsilon<span class="p">)</span>
</pre></div>


<p>其中epsilon 的默认值1e-8可能对于大多数情况都不是一个合适的值。例如，当在ImageNet上训练一个 Inception network时比较好的选择为1.0或者0.1。 <br />
需要注意的是，在稠密数据中即便g为0时， m_t, v_t 以及variable都将会更新。而在稀疏数据中，m_t, v_t 以及variable不被更新且值为零。</p>
<h1 id="2">2. 梯度计算与截断</h1>
<p>(Gradient Computation and Clipping)</p>
<h2 id="21">2.1. 梯度计算</h2>
<p>Gradient Computation<br />
TensorFlow 提供了计算给定tf计算图的求导函数<code>tf.gradients</code>，并在图的基础上增加节点。优化器(optimizer )类可以自动的计算网络图的导数，但是优化器中的创建器(creators)或者专业的人员可以通过本节所述的函数调用更底层的方法。</p>
<div class="hlcode"><pre><span class="c"># 构建一个符计算ys关于xs中x的偏导的和，返回xs中每个x对应的sum(dy/dx)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">grad_ys</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;gradients&#39;</span><span class="p">,</span> 
<span class="n">colocate_gradients_with_ops</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">gate_gradients</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
<span class="n">aggregation_method</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>


<h2 id="22">2.2. 梯度截断</h2>
<div class="hlcode"><pre><span class="c">#停止计算梯度，</span>
<span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span> 
</pre></div>


<p>在EM算法、Boltzmann机等可能会使用到</p>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">clip_value_min</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span> 
</pre></div>


<p>基于定义的min与max对tesor数据进行截断操作，<br />
目的是为了应对梯度爆发或者梯度消失的情况</p>
<div class="hlcode"><pre><span class="c"># 使用L2范式标准化tensor最大值为clip_norm,返回 t * clip_norm / l2norm(t)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">clip_by_norm</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">clip_norm</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span> 

<span class="c"># 使用平均L2范式规范tensor数据t，并以clip_norm为最大值</span>
<span class="n">tf</span><span class="o">.</span><span class="n">clip_by_average_norm</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">clip_norm</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="c"># 返回 t * clip_norm / l2norm_avg(t)</span>

<span class="n">tf</span><span class="o">.</span><span class="n">clip_by_global_norm</span><span class="p">(</span><span class="n">t_list</span><span class="p">,</span> 
<span class="n">clip_norm</span><span class="p">,</span> <span class="n">use_norm</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>    <span class="err">返回</span><span class="n">t_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">clip_norm</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">global_norm</span><span class="p">,</span> <span class="n">clip_norm</span><span class="p">)</span>

<span class="c"># 其中global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))</span>
<span class="n">tf</span><span class="o">.</span><span class="n">global_norm</span><span class="p">(</span><span class="n">t_list</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>   <span class="err">返回</span><span class="n">global_norm</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="nb">sum</span><span class="p">([</span><span class="n">l2norm</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">t_list</span><span class="p">]))</span>
</pre></div>


<h1 id="3-decaying-the-learning-rate">3. 学习率衰减(Decaying the learning rate)</h1>
<div class="hlcode"><pre><span class="c"># 对学习率进行指数衰退</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">exponential_decay</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">global_step</span><span class="p">,</span> 
<span class="n">decay_steps</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">staircase</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>    
</pre></div>


<p>tf.train.exponential_decay</p>
<div class="hlcode"><pre><span class="cp">#该函数返回以下结果</span>
<span class="n">decayed_learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span> <span class="o">*</span>
         <span class="n">decay_rate</span> <span class="o">^</span> <span class="p">(</span><span class="n">global_step</span> <span class="o">/</span> <span class="n">decay_steps</span><span class="p">)</span>
<span class="cp">##例： 以0.96为基数，每100000 步进行一次学习率的衰退</span>
</pre></div>


<p><strong>基本用法</strong></p>
<div class="hlcode"><pre><span class="c"># 1. 设置global_step</span>
<span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="c"># 2. 设置起始学习率</span>
<span class="n">starter_learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="c"># 3. 设置learning_rate</span>
<span class="n">learning_rate</span> <span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">exponential_decay</span><span class="p">(</span><span class="n">starter_learning_rate</span><span class="p">,</span> <span class="n">global_step</span><span class="p">,</span><span class="mi">100000</span><span class="p">,</span> <span class="mf">0.96</span><span class="p">,</span> <span class="n">staircase</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c"># 4. learning_rate 输入到Optimizer，将global_step输入到minimize 操作中</span>
<span class="n">learning_step</span> <span class="o">=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="o">...</span><span class="n">my</span> <span class="n">loss</span><span class="o">...</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">global_step</span><span class="p">))</span>
</pre></div>


<h1 id="4-moving-averages">4. 移动平均(Moving Averages)</h1>
<p>一些训练优化算法，比如<code>GradientDescent</code> <code>和Momentum</code> 在优化过程中便可以使用到移动平均方法。使用移动平均常常可以较明显地改善结果。<br />
操作  描述</p>
<p>class tf.train.ExponentialMovingAverage     将指数衰退加入到移动平均中<br />
tf.train.ExponentialMovingAverage.apply(var_list=None)  对var_list变量保持移动平均<br />
tf.train.ExponentialMovingAverage.average_name(var)     返回var均值的变量名称<br />
tf.train.ExponentialMovingAverage.average(var)  返回var均值变量<br />
tf.train.ExponentialMovingAverage.variables_to_restore(moving_avg_variables=None)   返回用于保存的变量名称的映射</p>
<p>▷ tf.train.ExponentialMovingAverage</p>
<div class="hlcode"><pre><span class="cp"># Example usage when creating a training model:</span>
<span class="cp"># Create variables.</span>
<span class="n">var0</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(...)</span>
<span class="n">var1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(...)</span>
<span class="cp"># ... use the variables to build a training model...</span>
<span class="p">...</span>
<span class="cp"># Create an op that applies the optimizer.  This is what we usually</span>
<span class="cp"># would use as a training op.</span>
<span class="n">opt_op</span> <span class="o">=</span> <span class="n">opt</span><span class="p">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">my_loss</span><span class="p">,</span> <span class="p">[</span><span class="n">var0</span><span class="p">,</span> <span class="n">var1</span><span class="p">])</span>

<span class="cp"># Create an ExponentialMovingAverage object</span>
<span class="n">ema</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">ExponentialMovingAverage</span><span class="p">(</span><span class="n">decay</span><span class="o">=</span><span class="mf">0.9999</span><span class="p">)</span>

<span class="cp"># Create the shadow variables, and add ops to maintain moving averages</span>
<span class="cp"># of var0 and var1.</span>
<span class="n">maintain_averages_op</span> <span class="o">=</span> <span class="n">ema</span><span class="p">.</span><span class="n">apply</span><span class="p">([</span><span class="n">var0</span><span class="p">,</span> <span class="n">var1</span><span class="p">])</span>

<span class="cp"># Create an op that will update the moving averages after each training</span>
<span class="cp"># step.  This is what we will use in place of the usual training op.</span>
<span class="n">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">opt_op</span><span class="p">])</span><span class="o">:</span>
    <span class="n">training_op</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">group</span><span class="p">(</span><span class="n">maintain_averages_op</span><span class="p">)</span>

<span class="p">...</span><span class="n">train</span> <span class="n">the</span> <span class="n">model</span> <span class="n">by</span> <span class="n">running</span> <span class="n">training_op</span><span class="p">...</span>

<span class="cp">#Example of restoring the shadow variable values:</span>
<span class="cp"># Create a Saver that loads variables from their saved shadow values.</span>
<span class="n">shadow_var0_name</span> <span class="o">=</span> <span class="n">ema</span><span class="p">.</span><span class="n">average_name</span><span class="p">(</span><span class="n">var0</span><span class="p">)</span>
<span class="n">shadow_var1_name</span> <span class="o">=</span> <span class="n">ema</span><span class="p">.</span><span class="n">average_name</span><span class="p">(</span><span class="n">var1</span><span class="p">)</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">Saver</span><span class="p">({</span><span class="n">shadow_var0_name</span><span class="o">:</span> <span class="n">var0</span><span class="p">,</span> <span class="n">shadow_var1_name</span><span class="o">:</span> <span class="n">var1</span><span class="p">})</span>
<span class="n">saver</span><span class="p">.</span><span class="n">restore</span><span class="p">(...</span><span class="n">checkpoint</span> <span class="n">filename</span><span class="p">...)</span>
<span class="cp"># var0 and var1 now hold the moving average values</span>
</pre></div>


<p>tf.train.ExponentialMovingAverage.variables_to_restore</p>
<div class="hlcode"><pre><span class="n">variables_to_restore</span> <span class="o">=</span><span class="n">ema</span><span class="o">.</span><span class="n">variables_to_restore</span><span class="p">()</span>

<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">variables_to_restore</span><span class="p">)</span>
</pre></div>


<h1 id="5-coordinator-and-queuerunner">5. 协调器和队列运行器(Coordinator and QueueRunner)</h1>
<p>查看queue中，queue相关的内容，了解tensorflow中队列的运行方式。<br />
操作  描述</p>
<p>class tf.train.Coordinator  线程的协调器<br />
tf.train.Coordinator.clear_stop()   清除停止标记<br />
tf.train.Coordinator.join(threads=None, stop_grace_period_secs=120)     等待线程终止<br />
threads:一个threading.Threads的列表，启动的线程，将额外加入到registered的线程中<br />
tf.train.Coordinator.register_thread(thread)    Register一个用于join的线程<br />
tf.train.Coordinator.request_stop(ex=None)  请求线程结束<br />
tf.train.Coordinator.should_stop()  检查是否被请求停止<br />
tf.train.Coordinator.stop_on_exception()    上下文管理器，当一个例外出现时请求停止<br />
tf.train.Coordinator.wait_for_stop(timeout=None)    等待Coordinator提示停止进程<br />
class tf.train.QueueRunner  持有一个队列的入列操作列表，用于线程中运行<br />
queue:一个队列<br />
enqueue_ops: 用于线程中运行的入列操作列表<br />
tf.train.QueueRunner.create_threads(sess, <br />
coord=None, daemon=False, start=False)  创建运行入列操作的线程，返回一个线程列表<br />
tf.train.QueueRunner.from_proto(queue_runner_def)   返回由queue_runner_def创建的QueueRunner对象<br />
tf.train.add_queue_runner(qr, collection='queue_runners')   增加一个QueueRunner到graph的收集器(collection )中<br />
tf.train.start_queue_runners(sess=None, coord=None, daemon=True, start=True, collection='queue_runners')    启动所有graph收集到的队列运行器(queue runners)</p>
<p>▷ class tf.train.Coordinator</p>
<div class="hlcode"><pre><span class="cp">#Coordinator的使用，用于多线程的协调</span>
<span class="nl">try:</span>
  <span class="p">...</span>
  <span class="n">coord</span> <span class="o">=</span> <span class="n">Coordinator</span><span class="p">()</span>
  <span class="err">#</span> <span class="n">Start</span> <span class="n">a</span> <span class="n">number</span> <span class="n">of</span> <span class="n">threads</span><span class="p">,</span> <span class="n">passing</span> <span class="n">the</span> <span class="n">coordinator</span> <span class="n">to</span> <span class="n">each</span> <span class="n">of</span> <span class="n">them</span><span class="p">.</span>
  <span class="p">...</span><span class="n">start</span> <span class="kr">thread</span> <span class="mf">1.</span><span class="p">..(</span><span class="n">coord</span><span class="p">,</span> <span class="p">...)</span>
  <span class="p">...</span><span class="n">start</span> <span class="kr">thread</span> <span class="n">N</span><span class="p">...(</span><span class="n">coord</span><span class="p">,</span> <span class="p">...)</span>
  <span class="err">#</span> <span class="n">Wait</span> <span class="k">for</span> <span class="n">all</span> <span class="n">the</span> <span class="n">threads</span> <span class="n">to</span> <span class="n">terminate</span><span class="p">,</span> <span class="n">give</span> <span class="n">them</span> <span class="mi">10</span><span class="n">s</span> <span class="n">grace</span> <span class="n">period</span>
  <span class="n">coord</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">threads</span><span class="p">,</span> <span class="n">stop_grace_period_secs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">except</span> <span class="n">RuntimeException</span><span class="o">:</span>
  <span class="p">...</span><span class="n">one</span> <span class="n">of</span> <span class="n">the</span> <span class="n">threads</span> <span class="n">took</span> <span class="n">more</span> <span class="n">than</span> <span class="mi">10</span><span class="n">s</span> <span class="n">to</span> <span class="n">stop</span> <span class="n">after</span> <span class="n">request_stop</span><span class="p">()</span>
  <span class="p">...</span><span class="n">was</span> <span class="n">called</span><span class="p">.</span>
<span class="n">except</span> <span class="n">Exception</span><span class="o">:</span>
  <span class="p">...</span><span class="n">exception</span> <span class="n">that</span> <span class="n">was</span> <span class="n">passed</span> <span class="n">to</span> <span class="n">coord</span><span class="p">.</span><span class="n">request_stop</span><span class="p">()</span>
</pre></div>


<p>▷ tf.train.Coordinator.stop_on_exception()</p>
<div class="hlcode"><pre><span class="n">with</span> <span class="n">coord</span><span class="p">.</span><span class="n">stop_on_exception</span><span class="p">()</span><span class="o">:</span>
  <span class="err">#</span> <span class="n">Any</span> <span class="n">exception</span> <span class="n">raised</span> <span class="n">in</span> <span class="n">the</span> <span class="n">body</span> <span class="n">of</span> <span class="n">the</span> <span class="n">with</span>
  <span class="err">#</span> <span class="n">clause</span> <span class="n">is</span> <span class="n">reported</span> <span class="n">to</span> <span class="n">the</span> <span class="n">coordinator</span> <span class="n">before</span> <span class="n">terminating</span>
  <span class="err">#</span> <span class="n">the</span> <span class="n">execution</span> <span class="n">of</span> <span class="n">the</span> <span class="n">body</span><span class="p">.</span>
  <span class="p">...</span><span class="n">body</span><span class="p">...</span>
<span class="cp">#等价于</span>
<span class="nl">try:</span>
  <span class="p">...</span><span class="n">body</span><span class="p">...</span>
<span class="n">exception</span> <span class="n">Exception</span> <span class="n">as</span> <span class="n">ex</span><span class="o">:</span>
  <span class="n">coord</span><span class="p">.</span><span class="n">request_stop</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span>
</pre></div>


<h1 id="6-distributed-execution">6. 分布示执行(Distributed execution)</h1>
<p>可以阅读TensorFlow的分布式学习框架简介 查看更多tensorflow分布式细节。</p>
<p>操作  描述<br />
class tf.train.Server   一个进程内的tensorflow服务，用于分布式训练<br />
tf.train.Server.init(server_or_cluster_def, <br />
job_name=None, task_index=None, protocol=None,<br />
config=None, start=True)    创建一个新的服务，其中job_name, task_index, <br />
和protocol为可选参数，<br />
优先级高于server_or_cluster_def中相关信息<br />
server_or_cluster_def : 为一个tf.train.ServerDef <br />
或 tf.train.ClusterDef 协议(protocol)的buffer，<br />
或者一个tf.train.ClusterSpec对象<br />
tf.train.Server.create_local_server(config=None, start=True)    创建一个新的运行在本地主机的单进程集群<br />
tf.train.Server.target  返回tf.Session所连接的目标服务器<br />
tf.train.Server.server_def  返回该服务的tf.train.ServerDef<br />
tf.train.Server.start()     开启服务<br />
tf.train.Server.join()  阻塞直到服务已经关闭</p>
<p>class tf.train.Supervisor   一个训练辅助器，用于checkpoints模型以及计算的summaries。该监视器只是一个小的外壳(wrapper),用于Coordinator, a Saver, 和a SessionManager周围<br />
tf.train.Supervisor.<strong>init</strong>(graph=None, ready_op=0, is_chief=True, init_op=0, init_feed_dict=None, local_init_op=0, logdir=None, <br />
summary_op=0, saver=0, global_step=0, <br />
save_summaries_secs=120, save_model_secs=600, <br />
recovery_wait_secs=30, stop_grace_secs=120,<br />
checkpoint_basename='model.ckpt', session_manager=None, summary_writer=0, init_fn=None)     创建一个监视器Supervisor<br />
tf.train.Supervisor.managed_session(master=”, config=None, start_standard_services=True, close_summary_writer=True)     返回一个管路session的上下文管理器<br />
tf.train.Supervisor.prepare_or_wait_for_session(master=”, config=None, wait_for_checkpoint=False, max_wait_secs=7200, start_standard_services=True)     确保model已经准备好<br />
tf.train.Supervisor.start_standard_services(sess)   为sess启动一个标准的服务<br />
tf.train.Supervisor.start_queue_runners(sess, queue_runners=None)   为QueueRunners启动一个线程，queue_runners为一个QueueRunners列表<br />
tf.train.Supervisor.summary_computed(sess, summary, global_step=None)   指示计算的summary<br />
tf.train.Supervisor.stop(threads=None, close_summary_writer=True)   停止服务以及协调器(coordinator),并没有关闭session<br />
tf.train.Supervisor.request_stop(ex=None)   参考Coordinator.request_stop()<br />
tf.train.Supervisor.should_stop()   参考Coordinator.should_stop()<br />
tf.train.Supervisor.stop_on_exception()     参考 Coordinator.stop_on_exception()<br />
tf.train.Supervisor.Loop(timer_interval_secs, target, args=None, kwargs=None)   开启一个循环器线程用于调用一个函数<br />
每经过timer_interval_secs秒执行，target(<em>args, </em>*kwargs)<br />
tf.train.Supervisor.coord   返回监督器(Supervisor)使用的协调器(Coordinator )</p>
<p>class tf.train.SessionManager   训练的辅助器，用于从checkpoint恢复数据以及创建一个session<br />
tf.train.SessionManager.<strong>init</strong>(local_init_op=None, ready_op=None, graph=None, recovery_wait_secs=30)  创建一个SessionManager<br />
tf.train.SessionManager.prepare_session(master, init_op=None, saver=None, checkpoint_dir=None, wait_for_checkpoint=False, max_wait_secs=7200, config=None, init_feed_dict=None, init_fn=None)   创建一个session，并确保model可以被使用<br />
tf.train.SessionManager.recover_session(master, saver=None, checkpoint_dir=None, wait_for_checkpoint=False, max_wait_secs=7200, config=None)    创建一个session，如果可以的话，使用恢复方法创建<br />
tf.train.SessionManager.wait_for_session(master, config=None, max_wait_secs=inf)    创建一个session，并等待model准备完成</p>
<p>class tf.train.ClusterSpec  将一个集群表示为一系列“tasks”，并整合至“jobs”中<br />
tf.train.ClusterSpec.as_cluster_def()   返回该cluster中一个tf.train.ClusterDef协议的buffer<br />
tf.train.ClusterSpec.as_dict()  返回一个字典，由job名称对应于网络地址<br />
tf.train.ClusterSpec.job_tasks(job_name)    返回一个给定的job对应的task列表<br />
tf.train.ClusterSpec.jobs   返回该cluster的job名称列表<br />
tf.train.replica_device_setter(ps_tasks=0, ps_device='/job:ps', worker_device='/job:worker', merge_devices=True, cluster=None, ps_ops=None)     返回一个设备函数(device function)，以在建立一个副本graph的时候使用，设备函数(device function)用在with tf.device(device_function)中</p>
<p>▷ tf.train.Server</p>
<div class="hlcode"><pre><span class="n">server</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">Server</span><span class="p">(...)</span>
<span class="n">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">(</span><span class="n">server</span><span class="p">.</span><span class="n">target</span><span class="p">)</span><span class="o">:</span>
  <span class="err">#</span> <span class="p">...</span>

<span class="mi">1</span>
<span class="mi">2</span>
<span class="mi">3</span>
<span class="mi">4</span>

<span class="mi">1</span>
<span class="mi">2</span>
<span class="mi">3</span>
<span class="mi">4</span>
</pre></div>


<p>▷ tf.train.Supervisor <br />
相关参数： <br />
ready_op : 一维 字符串 tensor。该tensor是用过监视器在prepare_or_wait_for_session()计算，检查model是否准备好可以使用。如果准备好，将返回一个空阵列，如果为None，该model没有被检查。 <br />
is_chief : 如果为True，创建一个主监视器用于负责初始化与模型的恢复，若为False，则依赖主监视器。 <br />
init_op : 一个操作，用于模型不能恢复时的初始化操作。默认初始化所有操作 <br />
local_init_op : 可被所有监视器运行的初始化操作。 <br />
logdir : 设置log目录 <br />
summary_op : 一个操作(Operation )，返回Summary 和事件logs，需要设置 logdir <br />
saver : 一个Saver对象 <br />
save_summaries_secs : 保存summaries的间隔秒数 <br />
save_model_secs : 保存model的间隔秒数 <br />
checkpoint_basename : checkpoint保存的基本名称</p>
<div class="hlcode"><pre><span class="nx">使用在单进程中</span>

<span class="k">with</span> <span class="nx">tf.Graph</span><span class="p">()</span><span class="bp">.</span><span class="nx">as_default</span><span class="p">():</span>
  <span class="nx">...add</span> <span class="nx">operations</span> <span class="k">to</span> <span class="nx">the</span> <span class="nx">graph...</span>
  <span class="err">#</span> <span class="nb">Create</span> <span class="nx">a</span> <span class="nx">Supervisor</span> <span class="nx">that</span> <span class="nx">will</span> <span class="nx">checkpoint</span> <span class="nx">the</span> <span class="n">model</span> <span class="k">in</span> <span class="s1">&#39;/tmp/mydir&#39;</span><span class="nx">.</span>
  <span class="n">sv</span> <span class="o">=</span> <span class="nx">Supervisor</span><span class="p">(</span><span class="n">logdir</span><span class="o">=</span><span class="s1">&#39;/tmp/mydir&#39;</span><span class="p">)</span>
  <span class="err">#</span> <span class="nb">Get</span> <span class="nx">a</span> <span class="nx">TensorFlow</span> <span class="nx">session</span> <span class="nx">managed</span> <span class="k">by</span> <span class="nx">the</span> <span class="nx">supervisor.</span>
  <span class="k">with</span> <span class="nx">sv.managed_session</span><span class="p">(</span><span class="nx">FLAGS.master</span><span class="p">)</span> <span class="nx">as</span> <span class="nx">sess</span><span class="p">:</span>
    <span class="err">#</span> <span class="nx">Use</span> <span class="nx">the</span> <span class="nx">session</span> <span class="k">to</span> <span class="nx">train</span> <span class="nx">the</span> <span class="nx">graph.</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="nx">sv.should_stop</span><span class="p">():</span>
      <span class="nx">sess.run</span><span class="p">(</span><span class="o">&lt;</span><span class="nx">my_train_op</span><span class="o">&gt;</span><span class="p">)</span>
<span class="err">#</span> <span class="nx">在上下文管理器with</span> <span class="nx">sv.managed_session</span><span class="p">()</span><span class="nx">内</span><span class="err">，</span><span class="nx">所有在graph的变量都被初始化</span><span class="err">。</span>
<span class="err">#</span> <span class="nx">或者说</span><span class="err">，</span><span class="nx">一些服务器checkpoint相应模型并增加summaries至事件log中</span><span class="err">。</span>
<span class="err">#</span> <span class="nx">如果有例外发生</span><span class="err">，</span><span class="nx">should_stop</span><span class="p">()</span><span class="nx">将返回True</span>
<span class="nx">使用在多副本运行情况中</span> 
<span class="nx">要使用副本训练已经部署在集群上的相同程序</span><span class="err">，</span><span class="nx">必须指定其中一个task为主要</span><span class="err">，</span><span class="nx">该task处理</span> <span class="nx">initialization</span><span class="p">,</span> <span class="nx">checkpoints</span><span class="p">,</span> <span class="nx">summaries</span><span class="p">,</span> <span class="nx">和recovery相关事物</span><span class="err">。</span><span class="nx">其他task依赖该task</span><span class="err">。</span>

<span class="err">#</span> <span class="nx">Choose</span> <span class="nx">a</span> <span class="nx">task</span> <span class="nx">as</span> <span class="nx">the</span> <span class="nx">chief.</span> <span class="nx">This</span> <span class="nx">could</span> <span class="nx">be</span> <span class="nx">based</span> <span class="k">on</span> <span class="nx">server_def.task_index</span><span class="p">,</span>
<span class="err">#</span> <span class="ow">or</span> <span class="nx">job_def.name</span><span class="p">,</span> <span class="ow">or</span> <span class="nx">job_def.tasks.</span> <span class="nx">It</span><span class="s1">&#39;s entirely up to the end user.</span>
<span class="s1"># But there can be only one *chief*.</span>
<span class="s1">is_chief = (server_def.task_index == 0)</span>
<span class="s1">server = tf.train.Server(server_def)</span>

<span class="s1">with tf.Graph().as_default():</span>
<span class="s1">  ...add operations to the graph...</span>
<span class="s1">  # Create a Supervisor that uses log directory on a shared file system.</span>
<span class="s1">  # Indicate if you are the &#39;</span><span class="nx">chief</span><span class="s1">&#39;</span>
<span class="s1">  sv = Supervisor(logdir=&#39;</span><span class="p">/</span><span class="nx">shared_directory</span><span class="p">/</span><span class="nx">...</span><span class="s1">&#39;, is_chief=is_chief)</span>
<span class="s1">  # Get a Session in a TensorFlow server on the cluster.</span>
<span class="s1">  with sv.managed_session(server.target) as sess:</span>
<span class="s1">    # Use the session to train the graph.</span>
<span class="s1">    while not sv.should_stop():</span>
<span class="s1">      sess.run(&lt;my_train_op&gt;)</span>
</pre></div>


<p>如果有task崩溃或重启，managed_session() 将检查是否Model被初始化。如果已经初始化，它只需要创建一个session并将其返回至正在训练的正常代码中。如果model需要被初始化，主task将对它进行重新初始化，而其他task将等待模型初始化完成。 <br />
注意：该程序方法一样适用于单进程的work，该单进程标注自己为主要的便行</p>
<p>▷ supervisor中master的字符串形式 <br />
无论运行在本机或者集群上，都可以使用以下值设定master flag：</p>
<div class="hlcode"><pre><span class="err">定义为</span> <span class="err">”</span> <span class="err">，要求一个进程内且没有使用</span><span class="n">RPC</span><span class="err">的</span><span class="n">session</span>
<span class="err">定义为</span> <span class="err">‘</span><span class="n">local</span><span class="err">&#39;，要求一个使用基于</span><span class="n">RPC</span><span class="err">的主服务接口</span><span class="p">(</span><span class="err">“</span><span class="n">Master</span> <span class="n">interface</span><span class="err">”</span> <span class="p">)</span><span class="err">的</span><span class="n">session</span><span class="err">来运行</span><span class="n">tensorflow</span><span class="err">程序。更多细节可以查看</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">Server</span><span class="p">.</span><span class="n">create_local_server</span><span class="p">()</span><span class="err">相关内容。</span>
<span class="err">定义为</span> <span class="err">‘</span><span class="n">grpc</span><span class="o">:</span><span class="c1">//hostname:port&#39;，要求一个指定的RPC接口的session，同时运行内部进程的master接入远程的tensorflow workers。可用server.target返回该形式</span>
</pre></div>


<p>▷ supervisor高级用法</p>
<div class="hlcode"><pre><span class="err">启动额外的服务</span> 
<span class="n">managed_session</span><span class="p">()</span><span class="err">启动了</span> <span class="n">Checkpoint</span> <span class="err">和</span><span class="n">Summary</span><span class="err">服务。如果需要运行更多的服务，可以在</span><span class="n">managed_session</span><span class="p">()</span><span class="err">控制的模块中启动他们。</span>

<span class="cp">#例如： 开启一个线程用于打印loss. 设置每60秒该线程运行一次，我们使用sv.loop()</span>
 <span class="p">...</span>
  <span class="n">sv</span> <span class="o">=</span> <span class="n">Supervisor</span><span class="p">(</span><span class="n">logdir</span><span class="o">=</span><span class="err">&#39;</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">mydir</span><span class="err">&#39;</span><span class="p">)</span>
  <span class="n">with</span> <span class="n">sv</span><span class="p">.</span><span class="n">managed_session</span><span class="p">(</span><span class="n">FLAGS</span><span class="p">.</span><span class="n">master</span><span class="p">)</span> <span class="n">as</span> <span class="n">sess</span><span class="o">:</span>
    <span class="n">sv</span><span class="p">.</span><span class="n">loop</span><span class="p">(</span><span class="mi">60</span><span class="p">,</span> <span class="n">print_loss</span><span class="p">,</span> <span class="p">(</span><span class="n">sess</span><span class="p">))</span>
    <span class="k">while</span> <span class="n">not</span> <span class="n">sv</span><span class="p">.</span><span class="n">should_stop</span><span class="p">()</span><span class="o">:</span>
      <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">my_train_op</span><span class="p">)</span>

<span class="mi">1</span>
<span class="mi">2</span>
<span class="mi">3</span>
<span class="mi">4</span>
<span class="mi">5</span>
<span class="mi">6</span>
<span class="mi">7</span>
<span class="mi">8</span>

<span class="mi">1</span>
<span class="mi">2</span>
<span class="mi">3</span>
<span class="mi">4</span>
<span class="mi">5</span>
<span class="mi">6</span>
<span class="mi">7</span>
<span class="mi">8</span>

<span class="err">启动更少的的服务</span> 
<span class="n">managed_session</span><span class="p">()</span> <span class="err">启动了</span> <span class="err">“</span><span class="n">summary</span><span class="err">”</span> <span class="err">和</span> <span class="err">“</span><span class="n">checkpoint</span><span class="err">”</span> <span class="err">线程，这些线程通过构建器或者监督器默认自动创建了</span><span class="n">summary_op</span> <span class="err">和</span><span class="n">saver</span><span class="err">操作。如果想运行自己的</span> <span class="n">summary</span> <span class="err">和</span><span class="n">checkpointing</span><span class="err">方法，关闭这些服务，通过传递</span><span class="n">None</span><span class="err">值给</span><span class="n">summary_op</span> <span class="err">和</span><span class="n">saver</span><span class="err">参数。</span>

<span class="err">在</span><span class="n">chief</span><span class="err">中每</span><span class="mi">100</span><span class="err">个</span><span class="n">step</span><span class="err">，创建</span><span class="n">summaries</span>
  <span class="err">#</span> <span class="n">Create</span> <span class="n">a</span> <span class="n">Supervisor</span> <span class="n">with</span> <span class="n">no</span> <span class="n">automatic</span> <span class="n">summaries</span><span class="p">.</span>
  <span class="n">sv</span> <span class="o">=</span> <span class="n">Supervisor</span><span class="p">(</span><span class="n">logdir</span><span class="o">=</span><span class="err">&#39;</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">mydir</span><span class="err">&#39;</span><span class="p">,</span> <span class="n">is_chief</span><span class="o">=</span><span class="n">is_chief</span><span class="p">,</span> <span class="n">summary_op</span><span class="o">=</span><span class="n">None</span><span class="p">)</span>
  <span class="err">#</span> <span class="n">As</span> <span class="n">summary_op</span> <span class="n">was</span> <span class="n">None</span><span class="p">,</span> <span class="n">managed_session</span><span class="p">()</span> <span class="n">does</span> <span class="n">not</span> <span class="n">start</span> <span class="n">the</span>
  <span class="err">#</span> <span class="n">summary</span> <span class="kr">thread</span><span class="p">.</span>
  <span class="n">with</span> <span class="n">sv</span><span class="p">.</span><span class="n">managed_session</span><span class="p">(</span><span class="n">FLAGS</span><span class="p">.</span><span class="n">master</span><span class="p">)</span> <span class="n">as</span> <span class="n">sess</span><span class="o">:</span>
    <span class="k">for</span> <span class="n">step</span> <span class="n">in</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">1000000</span><span class="p">)</span><span class="o">:</span>
      <span class="k">if</span> <span class="n">sv</span><span class="p">.</span><span class="n">should_stop</span><span class="p">()</span><span class="o">:</span>
        <span class="k">break</span>
      <span class="k">if</span> <span class="n">is_chief</span> <span class="n">and</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="o">:</span>
        <span class="err">#</span> <span class="n">Create</span> <span class="n">the</span> <span class="n">summary</span> <span class="n">every</span> <span class="mi">100</span> <span class="n">chief</span> <span class="n">steps</span><span class="p">.</span>
        <span class="n">sv</span><span class="p">.</span><span class="n">summary_computed</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">my_summary_op</span><span class="p">))</span>
      <span class="nl">else:</span>
        <span class="err">#</span> <span class="n">Train</span> <span class="n">normally</span>
        <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">my_train_op</span><span class="p">)</span>


<span class="mi">11</span>
<span class="mi">12</span>
<span class="mi">13</span>
<span class="mi">14</span>
<span class="mi">15</span>
<span class="mi">16</span>
<span class="mi">17</span>

<span class="mi">11</span>
<span class="mi">12</span>
<span class="mi">13</span>
<span class="mi">14</span>
<span class="mi">15</span>
<span class="mi">16</span>
<span class="mi">17</span>
</pre></div>


<p>▷ tf.train.Supervisor.managed_session</p>
<div class="hlcode"><pre><span class="nx">def</span> <span class="nx">train</span><span class="p">():</span>
  <span class="n">sv</span> <span class="o">=</span> <span class="nx">tf.train.Supervisor</span><span class="p">(</span><span class="nx">...</span><span class="p">)</span>
  <span class="k">with</span> <span class="nx">sv.managed_session</span><span class="p">(</span><span class="o">&lt;</span><span class="nx">master</span><span class="o">&gt;</span><span class="p">)</span> <span class="nx">as</span> <span class="nx">sess</span><span class="p">:</span>
    <span class="nb">for</span> <span class="n">step</span> <span class="k">in</span> <span class="nx">xrange</span><span class="p">(</span><span class="nx">..</span><span class="p">):</span>
      <span class="k">if</span> <span class="nx">sv.should_stop</span><span class="p">():</span>
        <span class="nx">break</span>
      <span class="nx">sess.run</span><span class="p">(</span><span class="o">&lt;</span><span class="nx">my</span> <span class="nx">training</span> <span class="nx">op</span><span class="o">&gt;</span><span class="p">)</span>
      <span class="nx">...do</span> <span class="nx">other</span> <span class="nx">things</span> <span class="nx">needed</span> <span class="nx">at</span> <span class="nb">each</span> <span class="nx">training</span> <span class="nx">step...</span>

<span class="mi">1</span>
<span class="mi">2</span>
<span class="mi">3</span>
<span class="mi">4</span>
<span class="mi">5</span>
<span class="mi">6</span>
<span class="mi">7</span>
<span class="mi">8</span>

<span class="mi">1</span>
<span class="mi">2</span>
<span class="mi">3</span>
<span class="mi">4</span>
<span class="mi">5</span>
<span class="mi">6</span>
<span class="mi">7</span>
<span class="mi">8</span>
</pre></div>


<p>▷ tf.train.SessionManager</p>
<div class="hlcode"><pre><span class="k">with</span> <span class="nx">tf.Graph</span><span class="p">()</span><span class="bp">.</span><span class="nx">as_default</span><span class="p">():</span>
   <span class="nx">...add</span> <span class="nx">operations</span> <span class="k">to</span> <span class="nx">the</span> <span class="nx">graph...</span>
  <span class="err">#</span> <span class="nb">Create</span> <span class="nx">a</span> <span class="nx">SessionManager</span> <span class="nx">that</span> <span class="nx">will</span> <span class="nx">checkpoint</span> <span class="nx">the</span> <span class="n">model</span> <span class="k">in</span> <span class="s1">&#39;/tmp/mydir&#39;</span><span class="nx">.</span>
  <span class="n">sm</span> <span class="o">=</span> <span class="nx">SessionManager</span><span class="p">()</span>
  <span class="n">sess</span> <span class="o">=</span> <span class="nx">sm.prepare_session</span><span class="p">(</span><span class="nx">master</span><span class="p">,</span> <span class="nx">init_op</span><span class="p">,</span> <span class="nx">saver</span><span class="p">,</span> <span class="nx">checkpoint_dir</span><span class="p">)</span>
  <span class="err">#</span> <span class="nx">Use</span> <span class="nx">the</span> <span class="nx">session</span> <span class="k">to</span> <span class="nx">train</span> <span class="nx">the</span> <span class="nx">graph.</span>
  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="nx">sess.run</span><span class="p">(</span><span class="o">&lt;</span><span class="nx">my_train_op</span><span class="o">&gt;</span><span class="p">)</span>
<span class="err">#</span><span class="nx">其中prepare_session</span><span class="p">()</span><span class="nx">初始化和恢复一个模型参数</span><span class="err">。</span>

<span class="err">#</span><span class="nx">另一个进程将等待model准备完成</span><span class="err">，</span><span class="nx">代码如下</span>
<span class="k">with</span> <span class="nx">tf.Graph</span><span class="p">()</span><span class="bp">.</span><span class="nx">as_default</span><span class="p">():</span>
  <span class="nx">...add</span> <span class="nx">operations</span> <span class="k">to</span> <span class="nx">the</span> <span class="nx">graph...</span>
  <span class="err">#</span> <span class="nb">Create</span> <span class="nx">a</span> <span class="nx">SessionManager</span> <span class="nx">that</span> <span class="nx">will</span> <span class="nb">wait</span> <span class="nb">for</span> <span class="nx">the</span> <span class="nx">model</span> <span class="k">to</span> <span class="nx">become</span> <span class="nx">ready.</span>
  <span class="n">sm</span> <span class="o">=</span> <span class="nx">SessionManager</span><span class="p">()</span>
  <span class="n">sess</span> <span class="o">=</span> <span class="nx">sm.wait_for_session</span><span class="p">(</span><span class="nx">master</span><span class="p">)</span>
  <span class="err">#</span> <span class="nx">Use</span> <span class="nx">the</span> <span class="nx">session</span> <span class="k">to</span> <span class="nx">train</span> <span class="nx">the</span> <span class="nx">graph.</span>
  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="nx">sess.run</span><span class="p">(</span><span class="o">&lt;</span><span class="nx">my_train_op</span><span class="o">&gt;</span><span class="p">)</span>
<span class="vi">#wait_for_session</span><span class="p">()</span><span class="nx">等待一个model被其他进程初始化</span>

<span class="mi">11</span>
<span class="mi">12</span>
<span class="mi">13</span>
<span class="mi">14</span>
<span class="mi">15</span>
<span class="mi">16</span>
<span class="mi">17</span>
<span class="mi">18</span>
<span class="mi">19</span>
<span class="mi">20</span>
<span class="mi">21</span>

<span class="mi">11</span>
<span class="mi">12</span>
<span class="mi">13</span>
<span class="mi">14</span>
<span class="mi">15</span>
<span class="mi">16</span>
<span class="mi">17</span>
<span class="mi">18</span>
<span class="mi">19</span>
<span class="mi">20</span>
<span class="mi">21</span>
</pre></div>


<p>▷ tf.train.ClusterSpec <br />
一个tf.train.ClusterSpec表示一系列的进程，这些进程都参与分布式tensorflow的计算。每一个 tf.train.Server都在一个独有的集群中构建。 <br />
创建一个具有两个jobs及其5个tasks的集群们需要定义从job名称列表到网络地址列表之间的映射。</p>
<div class="hlcode"><pre><span class="n">cluster</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">ClusterSpec</span><span class="p">({</span><span class="s">&quot;worker&quot;</span><span class="o">:</span> <span class="p">[</span><span class="s">&quot;worker0.example.com:2222&quot;</span><span class="p">,</span>
                                           <span class="s">&quot;worker1.example.com:2222&quot;</span><span class="p">,</span>
                                           <span class="s">&quot;worker2.example.com:2222&quot;</span><span class="p">],</span>
                                <span class="s">&quot;ps&quot;</span><span class="o">:</span> <span class="p">[</span><span class="s">&quot;ps0.example.com:2222&quot;</span><span class="p">,</span>
                                       <span class="s">&quot;ps1.example.com:2222&quot;</span><span class="p">]})</span>

<span class="mi">1</span>
<span class="mi">2</span>
<span class="mi">3</span>
<span class="mi">4</span>
<span class="mi">5</span>

<span class="mi">1</span>
<span class="mi">2</span>
<span class="mi">3</span>
<span class="mi">4</span>
<span class="mi">5</span>
</pre></div>


<p>▷ tf.train.replica_device_setter</p>
<div class="hlcode"><pre><span class="cp"># To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker</span>
<span class="cp"># jobs on hosts worker0, worker1 and worker2.</span>
<span class="n">cluster_spec</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">&quot;ps&quot;</span><span class="o">:</span> <span class="p">[</span><span class="s">&quot;ps0:2222&quot;</span><span class="p">,</span> <span class="s">&quot;ps1:2222&quot;</span><span class="p">],</span>
    <span class="s">&quot;worker&quot;</span><span class="o">:</span> <span class="p">[</span><span class="s">&quot;worker0:2222&quot;</span><span class="p">,</span> <span class="s">&quot;worker1:2222&quot;</span><span class="p">,</span> <span class="s">&quot;worker2:2222&quot;</span><span class="p">]}</span>
<span class="n">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">replica_device_setter</span><span class="p">(</span><span class="n">cluster</span><span class="o">=</span><span class="n">cluster_spec</span><span class="p">))</span><span class="o">:</span>
  <span class="err">#</span> <span class="n">Build</span> <span class="n">your</span> <span class="n">graph</span>
  <span class="n">v1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(...)</span>  <span class="err">#</span> <span class="n">assigned</span> <span class="n">to</span> <span class="o">/</span><span class="n">job</span><span class="o">:</span><span class="n">ps</span><span class="o">/</span><span class="n">task</span><span class="o">:</span><span class="mi">0</span>
  <span class="n">v2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(...)</span>  <span class="err">#</span> <span class="n">assigned</span> <span class="n">to</span> <span class="o">/</span><span class="n">job</span><span class="o">:</span><span class="n">ps</span><span class="o">/</span><span class="n">task</span><span class="o">:</span><span class="mi">1</span>
  <span class="n">v3</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(...)</span>  <span class="err">#</span> <span class="n">assigned</span> <span class="n">to</span> <span class="o">/</span><span class="n">job</span><span class="o">:</span><span class="n">ps</span><span class="o">/</span><span class="n">task</span><span class="o">:</span><span class="mi">0</span>
<span class="cp"># Run compute</span>

<span class="mi">11</span>
<span class="mi">12</span>

<span class="mi">11</span>
<span class="mi">12</span>
</pre></div>


<h1 id="7-summary-operations">7. 汇总操作(Summary Operations)</h1>
<p>我们可以在一个session中获取summary操作的输出，并将其传输到SummaryWriter以添加至一个事件记录文件中。<br />
操作  描述<br />
tf.scalar_summary(tags, values, collections=None, name=None)    输出一个标量值的summary协议buffer<br />
tag的shape需要与values的相同，用来做summaries的tags，为字符串<br />
tf.image_summary(tag, tensor, max_images=3, collections=None, name=None)    输出一个图像tensor的summary协议buffer<br />
tf.audio_summary(tag, tensor, sample_rate, max_outputs=3, collections=None, name=None)  输出一个音频tensor的summary协议buffer<br />
tf.histogram_summary(tag, values, collections=None, name=None)  输出一个直方图的summary协议buffer<br />
tf.nn.zero_fraction(value, name=None)   返回0在value中的小数比例<br />
tf.merge_summary(inputs, collections=None, name=None)   合并summary<br />
tf.merge_all_summaries(key='summaries')     合并在默认graph中手机的summaries</p>
<p>▶▶将记录汇总写入文件中(Adding Summaries to Event Files)<br />
操作  描述<br />
class tf.train.SummaryWriter    将summary协议buffer写入事件文件中<br />
tf.train.SummaryWriter.<strong>init</strong>(logdir, graph=None, max_queue=10, flush_secs=120, graph_def=None)   创建一个SummaryWriter实例以及新建一个事件文件<br />
tf.train.SummaryWriter.add_summary(summary, global_step=None)   将一个summary添加到事件文件中<br />
tf.train.SummaryWriter.add_session_log(session_log, global_step=None)   添加SessionLog到一个事件文件中<br />
tf.train.SummaryWriter.add_event(event)     添加一个事件到事件文件中<br />
tf.train.SummaryWriter.add_graph(graph, global_step=None, graph_def=None)   添加一个Graph到时间文件中<br />
tf.train.SummaryWriter.add_run_metadata(run_metadata, tag, global_step=None)    为一个单一的session.run()调用添加一个元数据信息<br />
tf.train.SummaryWriter.flush()  刷新时间文件到硬盘中<br />
tf.train.SummaryWriter.close()  将事件问价写入硬盘中并关闭该文件<br />
tf.train.summary_iterator(path)     一个用于从时间文件中读取时间协议buffer的迭代器</p>
<p>▷ tf.train.SummaryWriter <br />
创建一个SummaryWriter 和事件文件。如果我们传递一个Graph进入该构建器中，它将被添加到事件文件当中，这一点与使用add_graph()具有相同功能。 <br />
TensorBoard 将从事件文件中提取该graph，并将其显示。所以我们能直观地看到我们建立的graph。我们通常从我们启动的session中传递graph：</p>
<div class="hlcode"><pre><span class="p">...</span><span class="n">create</span> <span class="n">a</span> <span class="n">graph</span><span class="p">...</span>
<span class="cp"># Launch the graph in a session.</span>
<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">()</span>
<span class="cp"># Create a summary writer, add the &#39;graph&#39; to the event file.</span>
<span class="n">writer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">SummaryWriter</span><span class="p">(</span><span class="o">&lt;</span><span class="n">some</span><span class="o">-</span><span class="n">directory</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">sess</span><span class="p">.</span><span class="n">graph</span><span class="p">)</span>
</pre></div>


<p>▷ tf.train.summary_iterator</p>
<div class="hlcode"><pre><span class="cp">#打印时间文件中的内容</span>
<span class="k">for</span> <span class="n">e</span> <span class="n">in</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">summary_iterator</span><span class="p">(</span><span class="n">path</span> <span class="n">to</span> <span class="n">events</span> <span class="n">file</span><span class="p">)</span><span class="o">:</span>
    <span class="n">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

<span class="cp">#打印指定的summary值</span>
<span class="cp"># This example supposes that the events file contains summaries with a</span>
<span class="cp"># summary value tag &#39;loss&#39;.  These could have been added by calling</span>
<span class="cp"># `add_summary()`, passing the output of a scalar summary op created with</span>
<span class="cp"># with: `tf.scalar_summary([&#39;loss&#39;], loss_tensor)`.</span>
<span class="k">for</span> <span class="n">e</span> <span class="n">in</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">summary_iterator</span><span class="p">(</span><span class="n">path</span> <span class="n">to</span> <span class="n">events</span> <span class="n">file</span><span class="p">)</span><span class="o">:</span>
    <span class="k">for</span> <span class="n">v</span> <span class="n">in</span> <span class="n">e</span><span class="p">.</span><span class="n">summary</span><span class="p">.</span><span class="n">value</span><span class="o">:</span>
        <span class="k">if</span> <span class="n">v</span><span class="p">.</span><span class="n">tag</span> <span class="o">==</span> <span class="err">&#39;</span><span class="n">loss</span><span class="err">&#39;</span><span class="o">:</span>
            <span class="n">print</span><span class="p">(</span><span class="n">v</span><span class="p">.</span><span class="n">simple_value</span><span class="p">)</span>

<span class="mi">11</span>
<span class="mi">12</span>
<span class="mi">13</span>

<span class="mi">11</span>
<span class="mi">12</span>
<span class="mi">13</span>
</pre></div>


<h1 id="8-training-utilities">8. 训练的通用函数及其他(Training utilities)</h1>
<p>操作  描述<br />
tf.train.global_step(sess, global_step_tensor)  一个用于获取全局step的小辅助器<br />
tf.train.write_graph(graph_def, logdir, name, as_text=True)     将一个graph proto写入一个文件中</p>
<div class="hlcode"><pre><span class="o">:</span><span class="err">—</span>
</pre></div>


<p>class tf.train.LooperThread     可重复地执行代码的线程<br />
tf.train.LooperThread.init(coord, timer_interval_secs, target=None, args=None, kwargs=None)     创建一个LooperThread<br />
tf.train.LooperThread.is_alive()    返回是否该线程是活跃的<br />
tf.train.LooperThread.join(timeout=None)    等待线程结束<br />
tf.train.LooperThread.loop(coord, timer_interval_secs, target, args=None, kwargs=None)  启动一个LooperThread，用于周期地调用某个函数<br />
调用函数target(args)<br />
tf.py_func(func, inp, Tout, stateful=True, name=None)   将python函数包装成tf中操作节点</p>
<p>▷ tf.train.global_step</p>
<div class="hlcode"><pre><span class="cp"># Creates a variable to hold the global_step.</span>
<span class="n">global_step_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="err">&#39;</span><span class="n">global_step</span><span class="err">&#39;</span><span class="p">)</span>
<span class="cp"># Creates a session.</span>
<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">()</span>
<span class="cp"># Initializes the variable.</span>
<span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">global_step_tensor</span><span class="p">.</span><span class="n">initializer</span><span class="p">)</span>
<span class="n">print</span><span class="p">(</span><span class="err">&#39;</span><span class="n">global_step</span><span class="o">:</span> <span class="o">%</span><span class="n">s</span><span class="err">&#39;</span> <span class="o">%</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">global_step</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">global_step_tensor</span><span class="p">))</span>

<span class="nl">global_step:</span> <span class="mi">10</span>
</pre></div>


<p>▷ tf.train.write_graph</p>
<div class="hlcode"><pre><span class="n">v</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="err">&#39;</span><span class="n">my_variable</span><span class="err">&#39;</span><span class="p">)</span>
<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">write_graph</span><span class="p">(</span><span class="n">sess</span><span class="p">.</span><span class="n">graph_def</span><span class="p">,</span> <span class="err">&#39;</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">my</span><span class="o">-</span><span class="n">model</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">train</span><span class="p">.</span><span class="n">pbtxt</span><span class="err">&#39;</span><span class="p">)</span>
</pre></div>


<p>▷ tf.py_func</p>
<div class="hlcode"><pre><span class="cp">#tf.py_func(func, inp, Tout, stateful=True, name=None)</span>
<span class="cp">#func：为一个python函数</span>
<span class="cp">#inp：为输入函数的参数，Tensor列表</span>
<span class="cp">#Tout： 指定func返回的输出的数据类型，是一个列表</span>
<span class="n">def</span> <span class="n">my_func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">:</span>
  <span class="err">#</span> <span class="n">x</span> <span class="n">will</span> <span class="n">be</span> <span class="n">a</span> <span class="n">numpy</span> <span class="n">array</span> <span class="n">with</span> <span class="n">the</span> <span class="n">contents</span> <span class="n">of</span> <span class="n">the</span> <span class="n">placeholder</span> <span class="n">below</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">sinh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">inp</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[...])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">py_func</span><span class="p">(</span><span class="n">my_func</span><span class="p">,</span> <span class="p">[</span><span class="n">inp</span><span class="p">],</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">])</span>
</pre></div>


<p>2.2 测试 (Testing)</p>
<p>TensorFlow 提供了一个方便的继承unittest.TestCase类的方法，该类增加有关TensorFlow 测试的方法。如下例子：</p>
<div class="hlcode"><pre><span class="n">import</span> <span class="n">tensorflow</span> <span class="n">as</span> <span class="n">tf</span>


<span class="n">class</span> <span class="n">SquareTest</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">test</span><span class="p">.</span><span class="n">TestCase</span><span class="p">)</span><span class="o">:</span>

  <span class="n">def</span> <span class="n">testSquare</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="o">:</span>
    <span class="n">with</span> <span class="n">self</span><span class="p">.</span><span class="n">test_session</span><span class="p">()</span><span class="o">:</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">square</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
      <span class="n">self</span><span class="p">.</span><span class="n">assertAllEqual</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">eval</span><span class="p">(),</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">])</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="err">&#39;</span><span class="n">__main__</span><span class="err">&#39;</span><span class="o">:</span>
  <span class="n">tf</span><span class="p">.</span><span class="n">test</span><span class="p">.</span><span class="n">main</span><span class="p">()</span>
</pre></div>


<h1 id="9-utilities">9. 共用(Utilities)</h1>
<p>操作  描述<br />
tf.test.main()  运行所有的单元测试<br />
tf.test.assert_equal_graph_def(actual, expected)    断言 两个GraphDefs 是否几乎一样<br />
tf.test.get_temp_dir()  返回测试期间使用的临时目录<br />
tf.test.is_built_with_cuda()    返回是否Tensorflow支持CUDA(GPU)的build</p>
<h1 id="10-gradient-checking">10. 梯度检查(Gradient checking)</h1>
<p>可对比compute_gradient 和compute_gradient_error函数的用法<br />
操作  描述<br />
tf.test.compute_gradient(x, x_shape, y, y_shape, x_init_value=None, delta=0.001, init_targets=None)     </p>
<p>计算并返回理论的和数值的Jacobian矩阵</p>
<p>tf.test.compute_gradient_error(x, x_shape, y, y_shape, x_init_value=None, delta=0.001, init_targets=None)   计算梯度的error。在计算所得的与数值估计的Jacobian中 为dy/dx计算最大的error</p>
</div>
<div id="renote">
  <HR style=" FILTER: alpha (opacity = 100, finishopacity =0 , style= 3 )" width="80%" color=#987 cb 9 SIZE=3>
  <p>如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多!</p>
  <img src="/Wiki/static/images/pay.jpg" width="25%">
</div>

    </div>
    <div id="footer">
        <span>
            Copyright © 2021 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>

    
</body>
<script>
    function changeImgurl(site_root_url) {
        var images = document.images;
        var site_root = site_root_url;
        for (i = 0, len = images.length; i < len; i++) {
            image = images[i];
            image_src = image.src;
            if (image_src.search("attach") >= 0) {
                re_image_src = image_src.slice(image_src.search("attach"));
                abs_image_src = (site_root.endsWith("/")) ? site_root + re_image_src : site_root + "/" +
                    re_image_src;
                image.src = abs_image_src;
            }
        }
    }
    var site_root_url = "/Wiki";
    changeImgurl(site_root_url);
    let isMathjaxConfig = false; // 防止重复调用Config，造成性能损耗
    const initMathjaxConfig = () => {
        if (!window.MathJax) {
            return;
        }
        window.MathJax.Hub.Config({
            showProcessingMessages: false, //关闭js加载过程信息
            messageStyle: "none", //不显示信息
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
                displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
                skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
            },
            "HTML-CSS": {
                availableFonts: ["STIX", "TeX"], //可选字体
                showMathMenu: false //关闭右击菜单显示
            }
        });
        isMathjaxConfig = true; //
    };
    if (isMathjaxConfig === false) {
        // 如果：没有配置MathJax
        initMathjaxConfig();
    };
</script>

</html>