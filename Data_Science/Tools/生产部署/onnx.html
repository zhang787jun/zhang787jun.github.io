<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <title>ONNX[推荐] - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
</head>

<body>

    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="/Wiki/">Home</a>&nbsp;»&nbsp;<a href="/Wiki/#Data_Science">Data_Science</a>&nbsp;»&nbsp;<a href="/Wiki/#-Tools">Tools</a>&nbsp;»&nbsp;<a href="/Wiki/#-生产部署">生产部署</a>&nbsp;»&nbsp;ONNX[推荐]</div>
</div>
<div class="clearfix"></div>
<div id="title">ONNX[推荐]</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#1-onnx">1. 什么是ONNX</a></li>
<li><a href="#2-onnx">2. 获取 ONNX 模型</a><ul>
<li><a href="#21-winmltools">2.1. 通用转换--基于winmltools</a></li>
<li><a href="#22-tf_pd-onnx">2.2. tf_pd-&gt;onnx</a></li>
<li><a href="#23-torch_pt-onnx">2.3. torch_pt-&gt;onnx</a></li>
</ul>
</li>
<li><a href="#3-onnxruntime">3. onnxruntime</a><ul>
<li><a href="#31">3.1. 安装</a></li>
<li><a href="#32">3.2. 快速上手</a></li>
</ul>
</li>
<li><a href="#4">4. 参考资料</a></li>
</ul>
</div>
<h1 id="1-onnx">1. 什么是ONNX</h1>
<p>ONNX（英语：Open Neural Network Exchange）是一种针对机器学习所设计的开放式的文件格式，用于存储训练好的模型。它使得不同的人工智能框架（如Pytorch、MXNet）可以采用相同格式存储模型数据并交互。 ONNX的规范及代码主要由微软，亚马逊，Facebook和IBM等公司共同开发，以开放源代码的方式托管在<a href="https://github.com/onnx/onnx">Github</a>上<sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup> 目前官方支持加载ONNX模型并进行推理的深度学习框架有： Caffe2, PyTorch, MXNet，ML.NET，TensorRT 和 Microsoft CNTK，并且 TensorFlow 也非官方的支持ONNX。</p>
<p><img alt="" src="../../../../attach/images/2020-09-17-20-57-13.png" /></p>
<h1 id="2-onnx">2. 获取 ONNX 模型</h1>
<p>可通过以下几种方式获取 ONNX 模型：<br />
1. 训练新的 ONNX 模型<br />
2. 将现有模型从其他格式转换为 ONNX（请参阅 教程）<br />
3. 从 ONNX 模型 Zoo 获取预先定型的 ONNX 模型</p>
<p>参考https://github.com/onnx/tutorials</p>
<h2 id="21-winmltools">2.1. 通用转换--基于winmltools</h2>
<p><sup id="fnref:4"><a class="footnote-ref" href="#fn:4" rel="footnote">4</a></sup></p>
<div class="hlcode"><pre><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">U</span> <span class="n">winmltools</span>
</pre></div>


<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">coremltools.models.utils</span> <span class="kn">import</span> <span class="n">load_spec</span>
<span class="c"># Load model file</span>
<span class="n">model_coreml</span> <span class="o">=</span> <span class="n">load_spec</span><span class="p">(</span><span class="s">&#39;example.mlmodel&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">winmltools</span> <span class="kn">import</span> <span class="n">convert_coreml</span>
<span class="c"># Convert it!</span>
<span class="c"># The automatic code generator (mlgen) uses the name parameter to generate class names.</span>
<span class="n">model_onnx</span> <span class="o">=</span> <span class="n">convert_coreml</span><span class="p">(</span><span class="n">model_coreml</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;ExampleModel&#39;</span><span class="p">)</span>
</pre></div>


<h2 id="22-tf_pd-onnx">2.2. tf_pd-&gt;onnx</h2>
<h2 id="23-torch_pt-onnx">2.3. torch_pt-&gt;onnx</h2>
<div class="hlcode"><pre><span class="c"># Input to the model</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c"># Export the model</span>
<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">torch_model</span><span class="p">,</span>               <span class="c"># model being run</span>
                  <span class="n">x</span><span class="p">,</span>                         <span class="c"># model input (or a tuple for multiple inputs)</span>
                  <span class="s">&quot;super_resolution.onnx&quot;</span><span class="p">,</span>   <span class="c"># where to save the model (can be a file or file-like object)</span>
                  <span class="n">export_params</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>        <span class="c"># store the trained parameter weights inside the model file</span>
                  <span class="n">opset_version</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>          <span class="c"># the ONNX version to export the model to</span>
                  <span class="n">do_constant_folding</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c"># whether to execute constant folding for optimization</span>
                  <span class="n">input_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;input&#39;</span><span class="p">],</span>   <span class="c"># the model&#39;s input names</span>
                  <span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;output&#39;</span><span class="p">],</span> <span class="c"># the model&#39;s output names</span>
                  <span class="n">dynamic_axes</span><span class="o">=</span><span class="p">{</span><span class="s">&#39;input&#39;</span> <span class="p">:</span> <span class="p">{</span><span class="mi">0</span> <span class="p">:</span> <span class="s">&#39;batch_size&#39;</span><span class="p">},</span>    <span class="c"># variable lenght axes</span>
                                <span class="s">&#39;output&#39;</span> <span class="p">:</span> <span class="p">{</span><span class="mi">0</span> <span class="p">:</span> <span class="s">&#39;batch_size&#39;</span><span class="p">}})</span>
</pre></div>


<h1 id="3-onnxruntime">3. onnxruntime</h1>
<p>onnxruntime 是一种用于将 ONNX 模型部署到生产环境的高性能推理引擎。 它针对云和 Edge 进行了优化，适用于 Linux、Windows 和 Mac。 它使用 C++ 编写，还包含 C、Python、C#、Java 和 Javascript (Node.js) API，可在各种环境中使用。 ONNX 运行时同时支持 DNN 和传统 ML 模型，并与不同硬件上的加速器（例如，NVidia GPU 上的 TensorRT、Intel 处理器上的 OpenVINO、Windows 上的 DirectML 等）集成。 通过使用 ONNX 运行时，可以从大量的生产级优化、测试和不断改进中受益。<sup id="fnref:3"><a class="footnote-ref" href="#fn:3" rel="footnote">3</a></sup></p>
<p><img alt="" src="../../../../attach/images/2020-09-17-21-08-33.png" /></p>
<h2 id="31">3.1. 安装</h2>
<div class="hlcode"><pre><span class="n">pip</span> <span class="n">install</span> <span class="n">onnxruntime</span>       <span class="err">#</span> <span class="n">CPU</span> <span class="n">build</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">onnxruntime</span><span class="o">-</span><span class="n">gpu</span>   <span class="err">#</span> <span class="n">GPU</span> <span class="n">build</span>
</pre></div>


<h2 id="32">3.2. 快速上手</h2>
<div class="hlcode"><pre><span class="kn">import</span> <span class="nn">onnxruntime</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="s">&quot;path to model&quot;</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">([],</span> <span class="p">{</span><span class="s">&quot;input1&quot;</span><span class="p">:</span> <span class="n">indata1</span><span class="p">,</span> <span class="s">&quot;input2&quot;</span><span class="p">:</span> <span class="n">indata2</span><span class="p">})</span>
</pre></div>


<div class="hlcode"><pre><span class="n">session</span><span class="o">.</span><span class="n">get_modelmeta</span><span class="p">()</span>
<span class="n">first_input_name</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span>
<span class="n">first_output_name</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get_outputs</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span>
</pre></div>


<h1 id="4">4. 参考资料</h1>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>项目官方仓库<a href="https://github.com/onnx/onnx">https://github.com/onnx/onnx</a>&#160;<a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p><a href="https://www.jiqizhixin.com/articles/2018-11-30-6">机器之心：开源一年多的模型交换格式ONNX，已经一统框架江湖了？</a>&#160;<a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p><a href="https://docs.microsoft.com/zh-cn/azure/machine-learning/concept-onnx">微软Azure：ONNX 和 Azure 机器学习：创建和加速 ML 模型</a>&#160;<a class="footnote-backref" href="#fnref:3" rev="footnote" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p><a href="https://docs.microsoft.com/zh-cn/windows/ai/windows-ml/convert-model-winmltools">使用 WinMLTools 将 ML 模型转换为 ONNX</a>&#160;<a class="footnote-backref" href="#fnref:4" rev="footnote" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
</ol>
</div>
</div>
<div id="renote">
  <HR style=" FILTER: alpha (opacity = 100, finishopacity =0 , style= 3 )" width="80%" color=#987 cb 9 SIZE=3>
  <p>如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多!</p>
  <img src="/Wiki/static/images/pay.jpg" width="25%">
</div>

    </div>
    <div id="footer">
        <span>
            Copyright © 2021 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>

    
</body>
<script>
    function changeImgurl(site_root_url) {
        var images = document.images;
        var site_root = site_root_url;
        for (i = 0, len = images.length; i < len; i++) {
            image = images[i];
            image_src = image.src;
            if (image_src.search("attach") >= 0) {
                re_image_src = image_src.slice(image_src.search("attach"));
                abs_image_src = (site_root.endsWith("/")) ? site_root + re_image_src : site_root + "/" +
                    re_image_src;
                image.src = abs_image_src;
            }
        }
    }
    var site_root_url = "/Wiki";
    changeImgurl(site_root_url);
    let isMathjaxConfig = false; // 防止重复调用Config，造成性能损耗
    const initMathjaxConfig = () => {
        if (!window.MathJax) {
            return;
        }
        window.MathJax.Hub.Config({
            showProcessingMessages: false, //关闭js加载过程信息
            messageStyle: "none", //不显示信息
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
                displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
                skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
            },
            "HTML-CSS": {
                availableFonts: ["STIX", "TeX"], //可选字体
                showMathMenu: false //关闭右击菜单显示
            }
        });
        isMathjaxConfig = true; //
    };
    if (isMathjaxConfig === false) {
        // 如果：没有配置MathJax
        initMathjaxConfig();
    };
</script>

</html>