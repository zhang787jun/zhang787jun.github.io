<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <title>分布式Tensorflow - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
</head>

<body>

    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="/Wiki/">Home</a>&nbsp;»&nbsp;<a href="/Wiki/#Virtualization_and_Cloud_Computing">Virtualization_and_Cloud_Computing</a>&nbsp;»&nbsp;<a href="/Wiki/#-3. Kubernetes容器集群管理">3. Kubernetes容器集群管理</a>&nbsp;»&nbsp;<a href="/Wiki/#-Kubernetes在高性能计算中的应用">Kubernetes在高性能计算中的应用</a>&nbsp;»&nbsp;分布式Tensorflow</div>
</div>
<div class="clearfix"></div>
<div id="title">分布式Tensorflow</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#1">练习1</a><ul>
<li><a href="#_1">步骤</a></li>
</ul>
</li>
<li><a href="#tf_configjson">从环境变量TF_CONFIG中读取json格式的数据</a></li>
<li><a href="#python">反序列化成python对象</a></li>
<li><a href="#cluster-spec">获取Cluster Spec</a></li>
<li><a href="#id-job_name-worker-and-task_id-0">获取角色类型和id， 比如这里的job_name 是 "worker" and task_id 是 0</a></li>
<li><a href="#tensorflow-training-server">创建TensorFlow Training Server对象</a></li>
<li><a href="#job_namepsserverjoin">如果job_name为ps，则调用server.join()</a></li>
<li><a href="#master-mastersessionsummary">检查当前进程是否是master， 如果是master，就需要负责创建session和保存summary。</a></li>
<li><a href="#psworkertfjobmastertensorflowtfjobmasterworker_device">通常分布式训练的例子只有ps和worker两个角色，而在TFJob里增加了master这个角色，实际在分布式TensorFlow的编程模型并没有这个设计。而这需要使用TFJob的分布式代码里进行处理，不过这个处理并不复杂，只需要将master也看做worker_device的类型</a></li>
</ul>
</div>
<h1 id="1">练习1</h1>
<h2 id="_1">步骤</h2>
<p>在容器（或容器集群）中运行训练脚本，首先需要构建容器镜像</p>
<ol>
<li>构建<strong>程序运行</strong>的容器镜像</li>
</ol>
<div class="hlcode"><pre><span class="n">vim</span> <span class="n">Dockerfile</span>
<span class="o">&gt;&gt;&gt;</span>
</pre></div>


<p>Dockerfile 文件内容如下：</p>
<div class="hlcode"><pre><span class="n">FROM</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">:</span><span class="mf">1.10.0</span>
<span class="cp"># 拉取tensorflow 镜像</span>
<span class="n">COPY</span> <span class="n">main</span><span class="p">.</span><span class="n">py</span> <span class="o">/</span><span class="n">app</span><span class="o">/</span><span class="n">main</span><span class="p">.</span><span class="n">py</span>
<span class="cp"># 将本地的main.py 文件复制到镜像指定目录中</span>

<span class="n">ENTRYPOINT</span> <span class="p">[</span><span class="s">&quot;python&quot;</span><span class="p">,</span> <span class="s">&quot;/app/main.py&quot;</span><span class="p">]</span>
<span class="cp"># ENTRYPOINT指令指定的运行命令的参数，运行 python /app/main.py 程序</span>
</pre></div>


<ol>
<li>
<p>修改TFJob 的定制<code>yaml</code> 模板</p>
<p>定义 1 master, 2 workers and 1 PS， 1 TensorBoard</p>
</li>
</ol>
<div class="hlcode"><pre><span class="n">apiVersion</span><span class="o">:</span> <span class="n">kubeflow</span><span class="o">.</span><span class="na">org</span><span class="o">/</span><span class="n">v1beta1</span>
<span class="n">kind</span><span class="o">:</span> <span class="n">TFJob</span>
<span class="n">metadata</span><span class="o">:</span>
  <span class="n">name</span><span class="o">:</span> <span class="n">module6</span><span class="o">-</span><span class="n">ex1</span><span class="o">-</span><span class="n">gpu</span>
<span class="n">spec</span><span class="o">:</span>
  <span class="n">tfReplicaSpecs</span><span class="o">:</span>
    <span class="n">MASTER</span><span class="o">:</span>
      <span class="n">replicas</span><span class="o">:</span> <span class="mi">1</span>
      <span class="n">template</span><span class="o">:</span>
        <span class="n">spec</span><span class="o">:</span>
          <span class="n">containers</span><span class="o">:</span>
            <span class="o">-</span> <span class="n">image</span><span class="o">:</span> <span class="o">&lt;</span><span class="n">DOCKER_USERNAME</span><span class="o">&gt;/</span><span class="n">tf</span><span class="o">-</span><span class="n">mnist</span><span class="o">:</span><span class="n">gpu</span> <span class="err">#改成自定义的镜像</span>
              <span class="n">name</span><span class="o">:</span> <span class="n">tensorflow</span>
              <span class="n">resources</span><span class="o">:</span>
                <span class="n">limits</span><span class="o">:</span>
                  <span class="n">nvidia</span><span class="o">.</span><span class="na">com</span><span class="o">/</span><span class="n">gpu</span><span class="o">:</span> <span class="mi">1</span>
          <span class="n">restartPolicy</span><span class="o">:</span> <span class="n">OnFailure</span>
</pre></div>


<p>tf_operator的工作就是<br />
1. 创建对应的5个Pod, <br />
2. 将环境变量TF_CONFIG传入到<strong>每个Pod</strong>中，</p>
<ul>
<li>TF_CONFIG包含三部分的内容：</li>
<li>当前<strong>集群</strong> ClusterSpec；</li>
<li>该<strong>节点</strong>的角色类型；</li>
<li>节点id。</li>
</ul>
<p>比如该Pod为worker0，它所收到的环境变量TF_CONFIG为:</p>
<div class="hlcode"><pre><span class="p">{</span>  
   <span class="err">//1.</span> <span class="err">集群信息</span> 
   <span class="nt">&quot;cluster&quot;</span><span class="p">:{</span>  
      <span class="nt">&quot;master&quot;</span><span class="p">:[</span>  
         <span class="s2">&quot;distributed-mnist-master-0:2222&quot;</span>
      <span class="p">],</span>
      <span class="nt">&quot;ps&quot;</span><span class="p">:[</span>  
         <span class="s2">&quot;distributed-mnist-ps-0:2222&quot;</span>
      <span class="p">],</span>
      <span class="nt">&quot;worker&quot;</span><span class="p">:[</span>  
         <span class="s2">&quot;distributed-mnist-worker-0:2222&quot;</span><span class="p">,</span>
         <span class="s2">&quot;distributed-mnist-worker-1:2222&quot;</span>
      <span class="p">]</span>
   <span class="p">},</span>
   <span class="err">//</span> <span class="err">2.</span> <span class="err">Node</span> <span class="err">role</span> <span class="err">节点角色</span>
   <span class="nt">&quot;task&quot;</span><span class="p">:{</span>  
      <span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="s2">&quot;worker&quot;</span><span class="p">,</span>
      <span class="nt">&quot;index&quot;</span><span class="p">:</span><span class="mi">0</span>
   <span class="p">},</span>
   <span class="err">//3.</span> <span class="err">节点环境</span>
   <span class="nt">&quot;environment&quot;</span><span class="p">:</span><span class="s2">&quot;cloud&quot;</span>
<span class="p">}</span>
</pre></div>


<p>在这里,tf_operator负责将集群拓扑的发现和配置工作完成，免除了使用者的麻烦。对于使用者来说，他只需要在这里代码中使用通过获取环境变量TF_CONFIG中的上下文。</p>
<div class="hlcode"><pre><span class="n">kubectl</span> <span class="n">create</span> <span class="o">-</span><span class="n">f</span> <span class="o">&lt;</span><span class="n">template</span><span class="o">-</span><span class="n">path</span><span class="o">&gt;</span>
</pre></div>


<h1 id="tf_configjson">从环境变量TF_CONFIG中读取json格式的数据</h1>
<p>tf_config_json = os.environ.get("TF_CONFIG", "{}")</p>
<h1 id="python">反序列化成python对象</h1>
<p>tf_config = json.loads(tf_config_json)</p>
<h1 id="cluster-spec">获取Cluster Spec</h1>
<p>cluster_spec = tf_config.get("cluster", {})<br />
cluster_spec_object = tf.train.ClusterSpec(cluster_spec)</p>
<h1 id="id-job_name-worker-and-task_id-0">获取角色类型和id， 比如这里的job_name 是 "worker" and task_id 是 0</h1>
<p>task = tf_config.get("task", {})<br />
job_name = task["type"]<br />
task_id = task["index"]</p>
<h1 id="tensorflow-training-server">创建TensorFlow Training Server对象</h1>
<p>server_def = tf.train.ServerDef(<br />
    cluster=cluster_spec_object.as_cluster_def(),<br />
    protocol="grpc",<br />
    job_name=job_name,<br />
    task_index=task_id)<br />
server = tf.train.Server(server_def)</p>
<h1 id="job_namepsserverjoin">如果job_name为ps，则调用server.join()</h1>
<p>if job_name == 'ps':<br />
    server.join()</p>
<h1 id="master-mastersessionsummary">检查当前进程是否是master， 如果是master，就需要负责创建session和保存summary。</h1>
<p>is_chief = (job_name == 'master')</p>
<h1 id="psworkertfjobmastertensorflowtfjobmasterworker_device">通常分布式训练的例子只有ps和worker两个角色，而在TFJob里增加了master这个角色，实际在分布式TensorFlow的编程模型并没有这个设计。而这需要使用TFJob的分布式代码里进行处理，不过这个处理并不复杂，只需要将master也看做worker_device的类型</h1>
<p>with tf.device(tf.train.replica_device_setter(<br />
    worker_device="/job:{0}/task:{1}".format(job_name,task_id),<br />
    cluster=cluster_spec)):</p>
</div>
<div id="renote">
  <HR style=" FILTER: alpha (opacity = 100, finishopacity =0 , style= 3 )" width="80%" color=#987 cb 9 SIZE=3>
  <p>如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多!</p>
  <img src="/Wiki/static/images/pay.jpg" width="25%">
</div>

    </div>
    <div id="footer">
        <span>
            Copyright © 2021 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>

    
</body>
<script>
    function changeImgurl(site_root_url) {
        var images = document.images;
        var site_root = site_root_url;
        for (i = 0, len = images.length; i < len; i++) {
            image = images[i];
            image_src = image.src;
            if (image_src.search("attach") >= 0) {
                re_image_src = image_src.slice(image_src.search("attach"));
                abs_image_src = (site_root.endsWith("/")) ? site_root + re_image_src : site_root + "/" +
                    re_image_src;
                image.src = abs_image_src;
            }
        }
    }
    var site_root_url = "/Wiki";
    changeImgurl(site_root_url);
    let isMathjaxConfig = false; // 防止重复调用Config，造成性能损耗
    const initMathjaxConfig = () => {
        if (!window.MathJax) {
            return;
        }
        window.MathJax.Hub.Config({
            showProcessingMessages: false, //关闭js加载过程信息
            messageStyle: "none", //不显示信息
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
                displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
                skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
            },
            "HTML-CSS": {
                availableFonts: ["STIX", "TeX"], //可选字体
                showMathMenu: false //关闭右击菜单显示
            }
        });
        isMathjaxConfig = true; //
    };
    if (isMathjaxConfig === false) {
        // 如果：没有配置MathJax
        initMathjaxConfig();
    };
</script>

</html>