<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <title>最优化问题--损失函数的讨论 - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
</head>

<body>

    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="/Wiki/">Home</a>&nbsp;»&nbsp;<a href="/Wiki/#Data_Science\Algorithm\机器学习基础\01-模型构建及评价基础\基于最优化的模型">Data_Science\Algorithm\机器学习基础\01-模型构建及评价基础\基于最优化的模型</a>&nbsp;»&nbsp;最优化问题--损失函数的讨论</div>
</div>
<div class="clearfix"></div>
<div id="title">最优化问题--损失函数的讨论</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#1">1. 损失函数讨论</a></li>
<li><a href="#2">2. 分类损失</a><ul>
<li><a href="#21-0-1">2.1. 0-1 损失函数</a></li>
<li><a href="#22-log">2.2. log对数函数（难点）</a><ul>
<li><a href="#221">2.2.1. 关于概率的讨论</a></li>
<li><a href="#222">2.2.2. 函数的讨论</a></li>
<li><a href="#223">2.2.3. 对于二分类问题</a></li>
<li><a href="#224">2.2.4. 对于多分类问题</a></li>
</ul>
</li>
<li><a href="#23-hinge">2.3. Hinge损失函数</a></li>
</ul>
</li>
<li><a href="#3">3. 回归损失</a><ul>
<li><a href="#31">3.1. 绝对值损失函数</a></li>
<li><a href="#32">3.2. 平方损失函数</a></li>
<li><a href="#33">3.3. 指数损失函数</a></li>
<li><a href="#34-huber">3.4. Huber损失函数</a></li>
<li><a href="#35-log-cosh">3.5. Log-Cosh损失</a></li>
</ul>
</li>
<li><a href="#4">4. 熵</a><ul>
<li><a href="#41-">4.1. 熵--当只有一个变量分布</a></li>
<li><a href="#42-2">4.2. 交叉熵--当有2个变量分布</a></li>
<li><a href="#43">4.3. 实际应用</a></li>
</ul>
</li>
<li><a href="#learning-to-rank">learning to rank 算法</a><ul>
<li><a href="#pointwise">PointWise</a></li>
<li><a href="#pairwise">PairWise</a></li>
</ul>
</li>
<li><a href="#5">5. 参考资料</a></li>
</ul>
</div>
<h1 id="1">1. 损失函数讨论</h1>
<p><strong>损失函数</strong>是衡量由特征值x经过模型f得到的预测值y_=f(x)与真实值y的差距，是衡量预测错误程度的指标</p>
<p>函数的问题最终还是要归结于 任务类型，是处理 predict_label 和 real_label 的问题</p>
<p>单个样本：损失函数（Loss Function）<br />
多个样本：成本函数（Cost Function）</p>
<p>目标函数（Object Function）：在有约束条件下的最小化成本函数，通常市经验损失(loss/cost)+结构损失(Ω)</p>
<p>经验损失(loss/cost)：就是传说中的损失函数或者代价函数。<br />
结构损失(Ω)：就是正则项之类的来控制模型复杂程度的函数。<br />
1. 离散数据<br />
   1. 分类<br />
      1. 二分类<br />
      2. 多分类<br />
2. 连续数据<br />
   1. 回归 </p>
<h1 id="2">2. 分类损失</h1>
<h3 id="21-0-1">2.1. 0-1 损失函数</h3>
<p>$$<br />
Loss(y_ ,y)=\begin{cases}<br />
1   (y_!=y)\<br />
0   (y_==y)\<br />
\end{cases}<br />
$$</p>
<p><img alt="" src="/attach/images/2019-09-24-11-58-12.png" /></p>
<p><strong>手动：</strong></p>
<div class="hlcode"><pre><span class="c"># real_label shape =(N,···)</span>
<span class="c"># predict_label shape =(N,···)</span>
<span class="c"># predict_label.shape==real_label.shape</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">real_label</span><span class="p">,</span> <span class="n">predict_label</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="c"># loss shape =(N,1)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="c"># cost shape= ()  is a  val 数字</span>
</pre></div>


<p><strong>封装：</strong></p>
<div class="hlcode"><pre><span class="c"># 注意正确率的定义中 相等以及不等 相反,函数为不正确率 </span>
<span class="n">tensorflow</span> <span class="err">无单独封装</span> <span class="p">,</span><span class="err">可从</span><span class="n">tf</span><span class="o">.</span><span class="n">meteitc</span> <span class="err">正确率转换（但还不如手写来的快）</span>
</pre></div>


<p><strong>应用：</strong><br />
感知机</p>
<p>但是由于0-1损失函数相等这个条件太过严格，因此我们可以放宽条件，即满足时认为相等</p>
<p>$$Loss(y_ ,y)=\begin{cases}<br />
1 (|y_ -y|&gt;=error)\<br />
0 (|y_ -y|&lt;error)\<br />
\end{cases}<br />
$$</p>
<h3 id="22-log">2.2. log对数函数（难点）</h3>
<h4 id="221">2.2.1. 关于概率的讨论</h4>
<ol>
<li>概率（Probability）<br />
<strong>概率（Probability） P</strong> 描述的是某事件A出现的次数与所有事件出现的次数之比:</li>
</ol>
<p>$$P(A)=\frac{发生事件A的次数}{发送所有事件的次数}$$</p>
<p>$$P(A) \subset[0,1]$$</p>
<ol>
<li>
<p>几率 （Odds）<br />
如果针对二分类问题，事件A为正样本事件，将P(A)视为正样本的概率，则1-P(A)为负样本的可能性，形如下面的则称为 事件为正样本的<strong>几率 （Odds）</strong>：<br />
$$Odds(A)=\frac{P(A)}{1-P(A)}=\frac{事件A发生的概率}{事件A不发生的概率}$$<br />
$$Odds(A) \subset[0,+\infty)$$</p>
<p><strong>几率 （Odds）</strong> 反应一个事件发生是该事件发生和不发生的比率</p>
</li>
<li>
<p>后验概率Posterior probability P(y|x)<br />
   后验概率P(y|x)属于条件概率的一种，指的是在给定证据X后，参数y的概率。</p>
</li>
</ol>
<p>$$P(y|x) \subset[0,1]$$</p>
<div class="hlcode"><pre><span class="err">在神经网络模型中，通常将经过激活函数的</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="err">范围的输出层值作为作为</span><span class="o">**</span><span class="err">后验概率</span><span class="n">P</span><span class="p">(</span><span class="n">y</span><span class="o">|</span><span class="n">x</span><span class="p">)</span><span class="o">**</span>

<span class="err">$$</span><span class="n">P</span><span class="p">(</span><span class="n">y</span><span class="o">|</span><span class="n">x</span><span class="p">)</span><span class="o">=</span><span class="n">sigmod</span><span class="p">(</span><span class="n">Wx</span><span class="o">+</span><span class="n">b</span><span class="p">)</span><span class="err">$$</span>
</pre></div>


<ol>
<li>
<p>似然性（likelihood）<br />
   “似然性”与“或然性”或“概率”意思相近，都是指某种事件发生的可能性，但是在统计学中，“似然性”和“或然性”或“概率”又有明确的区分。</p>
<p><strong>概率(probability)</strong> 用于在已知一些参数$x$的情况下，预测接下来的观测所得到的结果y;  $P(y|x)$</p>
<p><strong>似然性(likelihood)</strong> 则是用于在已知某些观测所得到的结果y时，对有关事物的性质的参数x进行估计。$L(x|y)$</p>
</li>
<li>
<p>参数估计 <br />
最大似然估计（MLE）</p>
</li>
</ol>
<p>已知一组数据集D是独立地从概率分布P上采样生成的，且 [公式] 具有确定的形式（如高斯分布，二项分布等）但参数 [公式] 未知。</p>
<p>问题：如何根据数据集[公式] 估计参数[公式] ?</p>
<p>为了解决上述问题，统计学界存在两种不同的解决方案：</p>
<p>频率学派：参数[公式] 是一个客观存在的固定值，其可以通过找到使数据集 [公式] 出现可能性最大的值，对参数 [公式] 进行估计，此便是极大似然估计的核心思想。<br />
贝叶斯学派：参数 [公式] 是一个随机变量，服从一个概率分布（换句话讲， [公式] 不是一个客观存在的固定值，而是可以取很多不同值的变量，且具有相应的可能性），其首先根据主观的经验假定[公式]的概率分布为 [公式] （先验分布，往往并不准确），然后根据观察到的新信息（数据集 [公式] ）对其进行修正，此时[公式]的概率分布为 [公式] （后验分布）。<br />
最大似然估计<br />
核心思想：找到使数据集 [公式] 出现可能性最大的值，对参数 [公式] 进行估计，即[公式]</p>
<p>最大后验估计<br />
原则上，贝叶斯学派对 [公式] 的估计应该就是[公式]的后验分布[公式]，但是大多数时候后验分布的计算较为棘手，因此此时出现一种折衷解法：找到使后验概率最大的值，对参数 [公式] 进行估计，即</p>
<p>[公式]</p>
<p>[公式] （大名鼎鼎的贝叶斯公式）</p>
<p>[公式] （ [公式] 与 [公式] 无关）</p>
<p>[公式] （取 [公式] 不影响最优值）</p>
<p>根据上式可以发现，最大后验估计与最大似然估计优化过程中的差异便是多了一项 [公式] ，相当于加了一项与 [公式] 的先验概率 [公式] 有关的惩罚项。<br />
在参数估计中有一类方法叫做“最大似然估计”,因为涉及到的估计函数往往是是指数型族,取对数后不影响它的单调性但会让计算过程变得简单,所以就采用了似然函数的对数,称“对数似然函数”。<br />
  根据涉及的模型不同,对数函数会不尽相同,但是原理是一样的,都是从因变量的密度函数的到来,并涉及到对随机干扰项分布的假设</p>
<p>给定输出y时，关于参数的x似然函数$L(x|y)$（在数值上）等于给定参数x后变量y的概率$P(y|x)$：<br />
$$ L(x|y) = P(y|x)$$</p>
<ol>
<li>Logit变换与对数机率函数<br />
<strong>Logit变换</strong> 是指log it(它)，Logit Odds 就是对Odds 进行log(Odds)计算</li>
</ol>
<p>对上式进行<strong>Logit变换</strong>,形如下面的则称为正样本的<strong>对数机率</strong> z<br />
$$z=ln(Odds(A))=ln\frac{P(A)}{1-P(A)}$$<br />
$$z \subset(-\infty,+\infty)$$</p>
<p><img alt="" src="/attach/images/2019-07-10-16-24-57.png" /></p>
<blockquote>
<p>数学上<br />
$$ln\frac{P(A)}{1-P(A)}=z$$<br />
可以推出，概率P(A) 形式如下：<br />
$$P(A)=f(z)=\frac{1}{1+e^{-z}}=\frac{e^z}{1+e^{z}} $$</p>
</blockquote>
<p>$f(z)=\frac{1}{1+e^{-z}}$称为<strong>对数机率函数</strong> ，其数形式与Sigmoid 函数相同</p>
<h4 id="222">2.2.2. 函数的讨论</h4>
<p><strong>log对数函数 （logarithmic loss function)</strong>，又称为 <strong>对数似然函数(log-likehood loss function)</strong> ，是对<strong>单个样本</strong>的描述，表示为<br />
$$Loss(y,P(y|x))$$</p>
<p>是关于<strong>实际值y</strong>，与 特征值x下的<strong>后验概率P(y|x)</strong> 的函数，<strong>log对数</strong>等于<strong>后验概率</strong>的对数。这个函数的值通过下面的<strong>log函数的标准形式</strong>计算：</p>
<p>$$Loss(y,P(y|x))=-\log{P(y|x)}$$</p>
<p>$$Coss(y,P(y|x))=-\frac{1}{N} \sum{log{P(y|x)}}$$</p>
<p><img alt="" src="/attach/images/2019-09-24-13-02-14.png" /></p>
<h4 id="223">2.2.3. 对于二分类问题</h4>
<p>二分类问题的<strong>后验概率P(label=0|x)</strong>（当输入为x时，label=0的概率）,可知</p>
<p>$$P(label=0|x)=1-P(label=1|x)$$</p>
<p>$$P(label=0|x) \subset[0,1]$$</p>
<p>机率Odds(label=0|x)”</p>
<p>$$Odds(label=0|x) =\frac{P(label=0|x)}{1-P(label=0|x)}\subset[0,+\infty)$$<br />
若令$y=P(label=0|x)$,则 正样本（label=0）的<strong>对数机率</strong> z</p>
<p>$$ln\frac{y}{1-y}=ln\frac{P(label=0|x)}{P(label=1|x)}=z$$</p>
<p>$$z\subset(-\infty,+\infty)$$</p>
<p>依据输入特征x，确定该特征对应的label=0的概率为$P(label=0|x)$</p>
<p>$$P(label=0|x)=\frac{1}{1+e^{-z}}=\frac{e^z}{1+e^{z}}$$</p>
<p>依据输入特征x，确定该特征对应的label=1的概率为$P(label=1|x)$，<br />
$$P(label=1|x)=1-P(label=0|x)=\frac{1}{1+e^{z}}$$</p>
<p>若 z=wx+b 即<br />
$$P(label∣x)=\begin{cases}<br />
\frac{e^z}{1+e^{z}}=\frac{e^{wx+b}}{1+e^{wx+b}}, label=0\<br />
\frac{1}{1+e^{z}}=\frac{1}{1+e^{wx+b}}, label=1\<br />
\end{cases}<br />
$$</p>
<p>$$Loss(y,P(y|x))=-\log {P(y|x)}=\begin{cases}<br />
-\log{\frac{e^z}{1+e^{z}}}, y=0\<br />
-\log{\frac{1}{1+e^{z}}}, y=1\<br />
\end{cases}<br />
$$</p>
<p>$$Coss(y,P(y|x))=\frac{1}{n}\sum{Loss(y,P(y|x))}$$</p>
<p>$$Coss(y,P(y|x))=\frac{1}{n_0+n_1}(\sum_{i=1}^{n_0}{Loss(y=0,P(y=0|x))}+\sum_{i=1}^{n_<code>}{Loss(y=1,P(y=1|x))})
$$
$$Coss(y,P(y|x))=\frac{1}{n_0+n_1}(\sum_{i=1}^{n_0}{-1*log(P(1|x))}+\sum_{i=1}^{n_</code>}{(1-0)log(1-p(1|x))})$$</p>
<p><strong>手动：</strong></p>
<div class="hlcode"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c"># y_true ==labels</span>
<span class="c"># y_pred ==predictions</span>

<span class="k">def</span> <span class="nf">logcoss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-15</span><span class="p">):</span>
    <span class="c"># Prepare numpy array data</span>
    <span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span> 
    <span class="k">assert</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">))</span>
    <span class="c"># Clip y_pred between eps and 1-eps</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">eps</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span> <span class="n">y_true</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">))</span>
    <span class="n">cost</span><span class="o">=</span><span class="n">loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cost</span>
</pre></div>


<p><strong>封装：</strong></p>
<p>$$logloss=weights\times(labels\times \log{(predictions+epsilon)} + (1-labels)* \log{(1-predictions+epsilon)}) $$</p>
<p>$$logloss=W\times(L\times\log{(P+e)} + (1-L)\times\log{(1-P+e)}) $$</p>
<div class="hlcode"><pre><span class="n">log_loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">log_loss</span><span class="p">(</span>
    <span class="n">labels</span><span class="p">,</span>
    <span class="n">predictions</span><span class="p">,</span> 
    <span class="n">weights</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-07</span><span class="p">,</span> 
    <span class="n">scope</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">loss_collection</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">LOSSES</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="n">Reduction</span><span class="o">.</span><span class="n">SUM_BY_NONZERO_WEIGHTS</span>
<span class="p">)</span>
<span class="c"># 注意</span>
<span class="n">If</span> <span class="n">reduction</span> <span class="ow">is</span> <span class="n">NONE</span><span class="p">,</span> <span class="n">this</span> <span class="n">has</span> <span class="n">the</span> <span class="n">same</span> <span class="n">shape</span> <span class="k">as</span> <span class="n">labels</span><span class="p">;</span> <span class="n">otherwise</span><span class="p">,</span> <span class="n">it</span> <span class="ow">is</span> <span class="n">scalar</span><span class="o">.</span>


<span class="n">log_loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">log_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span><span class="n">y_predict</span><span class="p">,</span><span class="n">reduction</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="nb">type</span><span class="p">(</span><span class="n">log_loss</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Tensor</span> 
<span class="n">log_loss_results</span><span class="o">=</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">log_loss</span><span class="p">)</span>
<span class="c"># log_loss_results.shape =y_true.shape </span>
<span class="n">log_cost</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">log_loss</span><span class="p">)</span>


<span class="n">log_loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">log_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span><span class="n">y_predict</span><span class="p">)</span>
<span class="nb">type</span><span class="p">(</span><span class="n">log_loss</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">Tensor</span> 
<span class="n">log_loss_results</span><span class="o">=</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">log_loss</span><span class="p">)</span>
<span class="c"># log_loss_results 标量</span>

<span class="nb">type</span><span class="p">(</span><span class="n">log_cost</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="err">标量</span>
</pre></div>


<h4 id="224">2.2.4. 对于多分类问题</h4>
<p><strong>应用</strong><br />
Logistic回归</p>
<h3 id="23-hinge">2.3. Hinge损失函数</h3>
<p>$$loss(y_,y)=max(0,1-y_ · y)$$</p>
<p><strong>手动：</strong></p>
<div class="hlcode"><pre><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">real_label</span><span class="p">,</span> <span class="n">predict_label</span><span class="p">))</span>
<span class="c"># loss shape =real_label.shape==predict_label.shape</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">keep_dims</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">reduction_indices</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="c"># cost shape= ()  is a  val 数字</span>
</pre></div>


<p><strong>封装：</strong></p>
<div class="hlcode"><pre><span class="n">hinge_loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">hinge_loss</span><span class="p">(</span>
    <span class="n">labels</span><span class="p">,</span>
    <span class="n">logits</span><span class="p">,</span>
    <span class="n">weights</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">scope</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">loss_collection</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">LOSSES</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="n">Reduction</span><span class="o">.</span><span class="n">SUM_BY_NONZERO_WEIGHTS</span>
<span class="p">)</span>
</pre></div>


<p><strong>应用</strong><br />
Hinge loss用于最大间隔（maximum-margin）分类，其中最有代表性的就是支持向量机SVM。</p>
<h1 id="3">3. 回归损失</h1>
<h3 id="31">3.1. 绝对值损失函数</h3>
<p>$$Loss(y_ ,y)=|y_ -y|$$</p>
<p><img alt="" src="/attach/images/2019-09-24-15-46-57.png" /></p>
<p><strong>手动：</strong></p>
<div class="hlcode"><pre><span class="c"># real_label shape =(N,···)</span>
<span class="c"># predict_label shape =(N,···)</span>
<span class="c"># predict_label.shape==real_label.shape</span>
<span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">real_label</span><span class="p">,</span> <span class="n">predict_label</span><span class="p">))</span>
<span class="c"># loss shape =(N,···)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">keep_dims</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">reduction_indices</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="c"># cost shape= ()  is a  val 数字</span>
</pre></div>


<p><strong>封装：</strong></p>
<div class="hlcode"><pre><span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">absolute_difference</span><span class="p">(</span>
    <span class="n">labels</span><span class="p">,</span>
    <span class="n">predictions</span><span class="p">,</span>
    <span class="n">weights</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">scope</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">loss_collection</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">LOSSES</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="n">Reduction</span><span class="o">.</span><span class="n">SUM_BY_NONZERO_WEIGHTS</span>
<span class="p">)</span>
</pre></div>


<h3 id="32">3.2. 平方损失函数</h3>
<p>$$loss(y_,y)=\frac{1}{N}\sum_{i=1}^{N}{(y_-y)^2}$$</p>
<p><img alt="" src="/attach/images/2019-09-24-11-07-20.png" /></p>
<p><strong>手动：</strong></p>
<div class="hlcode"><pre><span class="c"># real_label shape =(N,···)</span>
<span class="c"># predict_label shape =(N,···)</span>
<span class="c"># predict_label.shape==real_label.shape</span>
<span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">real_label</span><span class="p">,</span> <span class="n">predict_label</span><span class="p">))</span>
<span class="c"># loss shape =(N,···)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">keep_dims</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">reduction_indices</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="c"># cost shape= ()  is a  val 数字</span>
</pre></div>


<p><strong>封装：</strong></p>
<div class="hlcode"><pre><span class="n">mean_squared_error</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span>
    <span class="n">labels</span><span class="p">,</span>
    <span class="n">predictions</span><span class="p">,</span>
    <span class="n">weights</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">scope</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">loss_collection</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">LOSSES</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="n">Reduction</span><span class="o">.</span><span class="n">SUM_BY_NONZERO_WEIGHTS</span>
<span class="p">)</span>
<span class="c"># mean_squared_error shape =() is a value</span>

<span class="c"># 平方函数实际为 均方差（MSE）</span>
<span class="n">mean_squared_error</span><span class="p">,</span><span class="n">update_op</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span>
    <span class="n">labels</span><span class="p">,</span>
    <span class="n">predictions</span><span class="p">,</span> <span class="c">#　predictions　为predict＿label</span>
    <span class="n">weights</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">metrics_collections</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">updates_collections</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="bp">None</span>
<span class="p">)</span>
<span class="c"># mean_squared_error shape =() is a value</span>
</pre></div>


<p><strong>示例</strong></p>
<div class="hlcode"><pre><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]])</span>

<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>     
<span class="n">mse_op</span><span class="p">,</span> <span class="n">mse_update_op</span><span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&quot;mse&quot;</span><span class="p">)</span>
<span class="n">init_op</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initialize_all_variables</span><span class="p">()</span>
<span class="n">init_op2</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initialize_local_variables</span><span class="p">()</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init_op2</span><span class="p">)</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init_op</span><span class="p">)</span>

<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span> <span class="c">#4.6666665</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">mse_update_op</span><span class="p">)</span> <span class="c">#4.6666665</span>
</pre></div>


<p><strong>应用：</strong><br />
最小二乘法通常用欧式距离进行距离的度量,使用平方损失函数</p>
<h3 id="33">3.3. 指数损失函数</h3>
<p>$$loss(y_,y)=e^{-y_·y}=\frac{e^y} {e^{y_}}$$</p>
<p>$$coss(y_,y)=\frac{1}{N}\sum_{i=1}^{N}{e^{-y_·y}}$$</p>
<p>Tensorflow中没有指数损失函数的封装包，可以自定义</p>
<div class="hlcode"><pre><span class="c"># real_label.shape ==predict_label.shape==(counts,····)</span>
<span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="o">-</span><span class="n">real_label</span><span class="p">,</span><span class="n">predict_label</span><span class="p">))</span>
<span class="c"># loss shape =(counts,····)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">keep_dims</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">reduction_indices</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="c"># cost shape= ()  is a  val 数字</span>
</pre></div>


<p><strong>应用</strong><br />
AdaBoost使用指数损失函数。</p>
<p>关于Adaboost的推导，可以参考Wikipedia：AdaBoost或者《统计学习方法》P145.</p>
<h3 id="34-huber">3.4. Huber损失函数</h3>
<p>Huber损失函数，平滑平均绝对误差 相比平方误差损失，Huber损失对于数据中异常值的敏感性要差一些。在值为0时，它也是可微分的。它基本上是绝对值，在误差很小时会变为平方值。误差使其平方值的大小如何取决于一个超参数δ，该参数可以调整。当δ~ 0时，Huber损失会趋向于MAE；当δ~ ∞（很大的数字），Huber损失会趋向于MSE。</p>
<p><img alt="" src="/attach/images/2019-09-24-15-47-40.png" /><br />
使用MAE训练神经网络最大的一个问题就是不变的大梯度，这可能导致在使用梯度下降快要结束时，错过了最小点。而对于MSE，梯度会随着损失的减小而减小，使结果更加精确。</p>
<p>在这种情况下，Huber损失就非常有用。它会由于梯度的减小而落在最小值附近。比起MSE，它对异常点更加鲁棒。因此，Huber损失结合了MSE和MAE的优点。但是，Huber损失的问题是我们可能需要不断调整超参数delta。</p>
<div class="hlcode"><pre><span class="c"># huber 损失</span>
<span class="k">def</span> <span class="nf">huber</span><span class="p">(</span><span class="n">true</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">delta</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">true</span><span class="o">-</span><span class="n">pred</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">delta</span> <span class="p">,</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">((</span><span class="n">true</span><span class="o">-</span><span class="n">pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="n">delta</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">true</span> <span class="o">-</span> <span class="n">pred</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">delta</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>


<p><img alt="" src="/attach/images/2019-09-24-10-59-57.png" /></p>
<h3 id="35-log-cosh">3.5. Log-Cosh损失</h3>
<p>Log-cosh是另一种应用于回归问题中的，且比L2更平滑的的损失函数。它的计算方式是预测误差的双曲余弦的对数。<br />
$$Loss=\sum_{i}^{n}{\log{cosh(\hat{y}-y)}}$$</p>
<p><img alt="" src="/attach/images/2019-09-24-15-48-08.png" /></p>
<p>优点：<br />
1. <strong>不易受到异常点的影响</strong>。对于较小的x，log(cosh(x))近似等于(x^2)/2，对于较大的x，近似等于abs(x)-log(2)。这意味着‘logcosh’基本类似于均方误差，但不易受到异常点的影响。<br />
2. <strong>二阶处处可微</strong>。它具有Huber损失所有的优点，但不同于Huber损失的是，Log-cosh二阶处处可微。</p>
<p>为什么需要二阶导数？许多机器学习模型如XGBoost，就是采用牛顿法来寻找最优点。而牛顿法就需要求解二阶导数（Hessian）。因此对于诸如XGBoost这类机器学习框架，损失函数的二阶可微是很有必要的。</p>
<p>XgBoost中使用的目标函数。注意对一阶和二阶导数的依赖性</p>
<p>但Log-cosh损失也并非完美，其仍存在某些问题。比如误差很大的话，一阶梯度和Hessian会变成定值，这就导致XGBoost出现缺少分裂点的情况。</p>
<h1 id="4">4. 熵</h1>
<h2 id="41-">4.1. 熵--当只有一个变量分布</h2>
<p>熵是对于给定分布 $q(x)$ 的不确定性的度量， 当取自有限的样本时，熵的公式可以表示为。</p>
<p>$$H(q(x))=-\sum{q(x) \log{q(x)} }$$</p>
<p>如果我们已知 所有的点都是绿色的，单一的？ 那个分布的不确定性是0，熵为0。<br />
如果我们已知，数据点服从q（x） 分布，我们可以依据上式计算该分布的熵</p>
<h2 id="42-2">4.2. 交叉熵--当有2个变量分布</h2>
<p>在信息论中，基于相同事件测度的两个概率分布 P(x)和 q(x)的<strong>交叉熵</strong>是指，<br />
$$H(P(x),q(x))=-\sum{P(x) \log{q(x)} }$$</p>
<p>交叉熵是用来描述p分布和q分布的距离</p>
<h2 id="43">4.3. 实际应用</h2>
<p>现实情况中，多数情况式我们不知道数据的的真实分布。假设，数据真实分布为q(y)，我们推测其分为P（y_）。如果我们像这样计算熵，我们实际上是在计算两个分布之间的交叉熵：</p>
<p>$$H(q(y),P(y_))=-\sum{q(y) * \log{ P(y_ ) } }$$</p>
<p>模型训练的目的就是使 预测分布P(x) 逼近 q(x)，他们之间距离越小，函数越小。</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup></p>
<h1 id="learning-to-rank">learning to rank 算法</h1>
<p>learning to rank 算法归纳为：<br />
1. PointWise，<br />
2. PairWise，<br />
3. ListWise</p>
<p>Pointwise和Pairwise把排序问题转换成 回归、分类 或 有序分类 问题。Lisewise把Query下整个搜索结果作为一个训练的实例。3种方法的区别主要体现在损失函数（Loss Function）上。</p>
<h2 id="pointwise">PointWise</h2>
<h2 id="pairwise">PairWise</h2>
<p>$$L(F(x),y)=∑<em j="i+1">{i=1}^{n−1}∑</em>^n l(sign(y_i−y_j),f(x_i)−f(x_j))$$</p>
<p>另外，有的Pairwise方法没有考虑到排序结果前几名对整个排序的重要性，也没有考虑不同查询对应的文档集合的大小对查询结果的影响(但是有的Pairwise方法对这些进行了改进，比如IR SVM就是对Ranking SVM针对以上缺点进行改进得到的算法)。</p>
<h1 id="5">5. 参考资料</h1>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>机器学习大牛最常用的5个回归损失函数，你知道几个？:https://www.jiqizhixin.com/articles/2018-06-21-3&#160;<a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
</div>
<div id="renote">
  <HR style=" FILTER: alpha (opacity = 100, finishopacity =0 , style= 3 )" width="80%" color=#987 cb 9 SIZE=3>
  <p>如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多!</p>
  <img src="/Wiki/static/images/pay.jpg" width="25%">
</div>

    </div>
    <div id="footer">
        <span>
            Copyright © 2020 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>

    
</body>
<script>
    function changeImgurl(site_root_url) {
        var images = document.images;
        var site_root = site_root_url;
        for (i = 0, len = images.length; i < len; i++) {
            image = images[i];
            image_src = image.src;
            if (image_src.search("attach") >= 0) {
                re_image_src = image_src.slice(image_src.search("attach"));
                abs_image_src = (site_root.endsWith("/")) ? site_root + re_image_src : site_root + "/" +
                    re_image_src;
                image.src = abs_image_src;
            }
        }
    }
    var site_root_url = "/Wiki";
    changeImgurl(site_root_url);
    let isMathjaxConfig = false; // 防止重复调用Config，造成性能损耗
    const initMathjaxConfig = () => {
        if (!window.MathJax) {
            return;
        }
        window.MathJax.Hub.Config({
            showProcessingMessages: false, //关闭js加载过程信息
            messageStyle: "none", //不显示信息
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
                displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
                skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
            },
            "HTML-CSS": {
                availableFonts: ["STIX", "TeX"], //可选字体
                showMathMenu: false //关闭右击菜单显示
            }
        });
        isMathjaxConfig = true; //
    };
    if (isMathjaxConfig === false) {
        // 如果：没有配置MathJax
        initMathjaxConfig();
    };
</script>

</html>