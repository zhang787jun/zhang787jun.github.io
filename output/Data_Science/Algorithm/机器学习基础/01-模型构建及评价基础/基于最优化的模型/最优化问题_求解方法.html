<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <title>最优化问题求解--优化方法 - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
</head>

<body>

    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="/Wiki/">Home</a>&nbsp;»&nbsp;<a href="/Wiki/#Data_Science\Algorithm\机器学习基础\01-模型构建及评价基础\基于最优化的模型">Data_Science\Algorithm\机器学习基础\01-模型构建及评价基础\基于最优化的模型</a>&nbsp;»&nbsp;最优化问题求解--优化方法</div>
</div>
<div class="clearfix"></div>
<div id="title">最优化问题求解--优化方法</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#1">1. 优化方法</a><ul>
<li><a href="#11">1.1. 一阶优化</a><ul>
<li><a href="#111">1.1.1. 降梯度</a><ul>
<li><a href="#1111-batch-gradient-descentbgd">1.1.1.1. 批量梯度下降法(Batch Gradient Descent,BGD)</a></li>
<li><a href="#1112-stochastic-gradient-descentsgd">1.1.1.2. 随机梯度下降法(Stochastic Gradient Descent,SGD)</a></li>
<li><a href="#1113">1.1.1.3.  小批量梯度下降法</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#12">1.2. 二阶优化</a><ul>
<li><a href="#121">1.2.1. 牛顿法</a><ul>
<li><a href="#1211">1.2.1.1. 为什么在深度学习中很少用牛顿法</a></li>
<li><a href="#1212">1.2.1.2. 拟牛顿法</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<h2 id="1">1. 优化方法</h2>
<h3 id="11">1.1. 一阶优化</h3>
<h4 id="111">1.1.1. 降梯度</h4>
<h5 id="1111-batch-gradient-descentbgd">1.1.1.1. 批量梯度下降法(Batch Gradient Descent,BGD)</h5>
<h5 id="1112-stochastic-gradient-descentsgd">1.1.1.2. 随机梯度下降法(Stochastic Gradient Descent,SGD)</h5>
<p>SGD最大的缺点是下降速度慢，而且可能会在沟壑的两边持续震荡，停留在一个局部最优点。</p>
<h5 id="1113">1.1.1.3.  小批量梯度下降法</h5>
<p>mini-batch Gradient Descent</p>
<h3 id="12">1.2. 二阶优化</h3>
<p>二阶优化算法使用了二阶导数(也叫做Hessian方法)来最小化或最大化损失函数。由于二阶导数的计算成本很高，所以这种方法并没有广泛使用。</p>
<h4 id="121">1.2.1. 牛顿法</h4>
<p>牛顿法的优点是收敛速度快，而缺点是牛顿法的计算中需要计算海森矩阵和该矩阵的逆。海森矩阵往往需要较大的计算量，而且一般矩阵的求逆运算时间复杂度是O(p3)。</p>
<ol>
<li>牛顿法起始点不能离局部极小点太远，否则很可能不会收敛。(考虑到二阶拟合应该很容易想象)，所以实际操作中会先使用别的方法，比如梯度下降法，使更新的点离最优点比较近，再开始用牛顿法</li>
<li>牛顿法每次需要更新一个二阶矩阵，当维数增加的时候是非常耗内存的，所以实际使用是会用拟牛顿法。</li>
<li>梯度下降法在非常靠近最优点时会有震荡，就是说明明离的很近了，却很难到达，因为线性的逼近非常容易一个方向过去就过了最优点(因为只能是负梯度方向)。但牛顿法因为是二次收敛就很容易到达了。牛顿法最明显快的特点是对于二阶函数(考虑多元函数的话要在凸函数的情况下)，牛顿法能够一步到达，非常有效。</li>
</ol>
<h5 id="1211">1.2.1.1. 为什么在深度学习中很少用牛顿法</h5>
<p>原因一：牛顿法需要用到梯度和Hessian矩阵，这两个都难以求解。因为很难写出深度神经网络拟合函数的表达式，遑论直接得到其梯度表达式，更不要说得到基于梯度的Hessian矩阵了。</p>
<p>原因二：即使可以得到梯度和Hessian矩阵，当输入向量的维度N较大时，Hessian矩阵的大小是N×N，所需要的内存非常大。比如你有1亿的数据，Hessian矩阵是1亿行和1亿列，然后进行逆运算，</p>
<p>原因三：在高维非凸优化问题中，鞍点相对于局部最小值的数量非常多，而且鞍点处的损失值相对于局部最小值处也比较大。而二阶优化算法是寻找梯度为0的点，所以很容易陷入鞍点。</p>
<h5 id="1212">1.2.1.2. 拟牛顿法</h5>
<p>为了保持收敛速度快的同时，减小每一步迭代的计算量，我们可以将海森矩阵的逆替换为一个近似的矩阵。而该矩阵的计算复杂度远小于原始的海森矩阵求逆。</p>
<p>比较著名的拟牛顿法有DFP和BFGS和L-BFGS。</p>
</div>
<div id="renote">
  <HR style=" FILTER: alpha (opacity = 100, finishopacity =0 , style= 3 )" width="80%" color=#987 cb 9 SIZE=3>
  <p>如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多!</p>
  <img src="/Wiki/static/images/pay.jpg" width="25%">
</div>

    </div>
    <div id="footer">
        <span>
            Copyright © 2020 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>

    
</body>
<script>
    function changeImgurl(site_root_url) {
        var images = document.images;
        var site_root = site_root_url;
        for (i = 0, len = images.length; i < len; i++) {
            image = images[i];
            image_src = image.src;
            if (image_src.search("attach") >= 0) {
                re_image_src = image_src.slice(image_src.search("attach"));
                abs_image_src = (site_root.endsWith("/")) ? site_root + re_image_src : site_root + "/" +
                    re_image_src;
                image.src = abs_image_src;
            }
        }
    }
    var site_root_url = "/Wiki";
    changeImgurl(site_root_url);
    let isMathjaxConfig = false; // 防止重复调用Config，造成性能损耗
    const initMathjaxConfig = () => {
        if (!window.MathJax) {
            return;
        }
        window.MathJax.Hub.Config({
            showProcessingMessages: false, //关闭js加载过程信息
            messageStyle: "none", //不显示信息
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
                displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
                skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
            },
            "HTML-CSS": {
                availableFonts: ["STIX", "TeX"], //可选字体
                showMathMenu: false //关闭右击菜单显示
            }
        });
        isMathjaxConfig = true; //
    };
    if (isMathjaxConfig === false) {
        // 如果：没有配置MathJax
        initMathjaxConfig();
    };
</script>

</html>