<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <title>决策树 - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
</head>

<body>

    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="/Wiki/">Home</a>&nbsp;»&nbsp;<a href="/Wiki/#Data_Science\Algorithm\机器学习基础">Data_Science\Algorithm\机器学习基础</a>&nbsp;»&nbsp;决策树</div>
</div>
<div class="clearfix"></div>
<div id="title">决策树</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#1">1. 决策树背景</a><ul>
<li><a href="#11">1.1. 树的意义</a></li>
<li><a href="#12">1.2. 树的生成</a></li>
<li><a href="#13">1.3. 剪枝</a></li>
<li><a href="#14">1.4. 熵和信息增益</a><ul>
<li><a href="#141">1.4.1. 信息量</a></li>
<li><a href="#142">1.4.2. 熵</a></li>
<li><a href="#143">1.4.3. 条件熵</a></li>
<li><a href="#144">1.4.4. 经验熵和经验条件熵</a></li>
<li><a href="#145">1.4.5. 信息增益</a></li>
<li><a href="#146">1.4.6. 信息增益比</a></li>
<li><a href="#147">1.4.7. 基尼指数</a></li>
</ul>
</li>
<li><a href="#15">1.5. 分类树</a></li>
<li><a href="#16">1.6. 回归树</a></li>
</ul>
</li>
<li><a href="#2">2. 决策树算法</a><ul>
<li><a href="#21-id3-">2.1. ID3--最大信息增益</a></li>
<li><a href="#22-c45-">2.2. C4.5--最大信息增益</a></li>
<li><a href="#23-cart-gini">2.3. CART--最大基尼Gini指数</a></li>
</ul>
</li>
<li><a href="#_1">参考资料</a></li>
</ul>
</div>
<h1 id="1">1. 决策树背景</h1>
<h2 id="11">1.1. 树的意义</h2>
<p>决策树是一棵<code>if-then</code>树。 内部节点代表一个属性或特征，叶节点代表一个类。<br />
决策树也是给定各个特征的情况下，某类别的概率。即条件概率 <code>P(Y∣X)</code>。</p>
<div class="hlcode"><pre><span class="n">digraph</span><span class="p">{</span>
<span class="n">Root_Node</span><span class="o">-&gt;</span><span class="n">Left_Node</span><span class="p">[</span><span class="n">label</span><span class="o">=</span><span class="s">&quot;if 特征j&lt;阈值</span><span class="se">\t</span><span class="s">heta&quot;</span> <span class="p">]</span>
<span class="n">Root_Node</span><span class="o">-&gt;</span><span class="n">Right_Node</span><span class="p">[</span><span class="n">label</span><span class="o">=</span><span class="s">&quot;if 特征j&gt;阈值</span><span class="se">\t</span><span class="s">heta&quot;</span> <span class="p">]</span>
<span class="p">}</span>
</pre></div>


<p><strong>关键</strong><br />
1. 节点关键特征j<br />
2. 阈值$\theta$<br />
3. 预测值p</p>
<h2 id="12">1.2. 树的生成</h2>
<p>构建根节点，选择最优特征。按照特征划分子集，继续选择新的最优特征，直到没有或者全部被正确分类。</p>
<h2 id="13">1.3. 剪枝</h2>
<p>决策树的生成对应于模型的局部选择，会尽量拟合训练数据，导致模型复杂和过拟合。</p>
<p>决策树的剪枝对应于模型的全局选择， 自下而上删掉一些节点。</p>
<h2 id="14">1.4. 熵和信息增益</h2>
<p>在每个节点，要选择一个最优特征生成。</p>
<p>ID3 使用<strong>信息增益</strong>最大选择最优特征<br />
C4.5 使用<strong>信息增益率-ratio</strong>最大来选择最优特征<br />
CART回归树<strong>平方误差MSE</strong> 最小<br />
CART分类树<strong>基尼指数</strong>最小</p>
<div class="hlcode"><pre><span class="n">digraph</span> <span class="n">a</span><span class="p">{</span>
    <span class="err">信息量</span><span class="o">-&gt;</span><span class="err">熵</span><span class="p">[</span><span class="n">label</span><span class="o">=</span><span class="err">期望</span><span class="p">]</span>
    <span class="err">熵</span><span class="o">-&gt;</span><span class="err">条件熵</span>

<span class="p">}</span>
</pre></div>


<h3 id="141">1.4.1. 信息量</h3>
<p>信息量是随机变量X不确定性的度量。<br />
$$I(X)=−log(x)$$</p>
<h3 id="142">1.4.2. 熵</h3>
<p>熵(Entropy)是信息量的期望，也是随机变量不确定性的度量。<br />
熵偏向<code>离散属性</code>， 基尼指数偏向<code>连续属性</code>。<br />
$$H(X)=−\sum_ {x∈X} p(x)logp(x)$$</p>
<h3 id="143">1.4.3. 条件熵</h3>
<p>条件熵是在给定随机变量X的情况下，随机变量Y的不确定性。<br />
$$H(Y∣X)=\frac{1}{k}\sum_{i=1} p(x_i)H(Y|X=x_i) X$$<br />
共有K类，其中 $p(x_i)$表示X属于第i类的概率。$H(Y|X=x_i)$ 表示$X=x_i$时Y的子集的熵。</p>
<h3 id="144">1.4.4. 经验熵和经验条件熵</h3>
<p>由数据估计（极大似然估计）得到的熵和条件熵。如数据集总共有D个样本，有$K$个类别。类别为$k$的样本数量为$C_k$经验熵是:<br />
$$ H(D)=\frac{1}{k} \sum_{k=1}^K \frac{C_k}{D} \log_2\frac{C_k}{D} $$ <br />
特征A根据取值把数据集D划分为n个子集，则给定特征A时数据集D的经验条件熵是：<br />
$$ H(D|A)=\sum_{i=1}^n \frac{D_i}{D}H(D_i)=-\sum_{i=1}^n \frac{|D_i|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|}$$</p>
<h3 id="145">1.4.5. 信息增益</h3>
<p>信息增益是给定特征A，使得数据集D不确定性减少的程度。<br />
$$信息增益 = 划分前熵 - 划分后熵 $$</p>
<p>$$信息增益= 熵g - 条件熵g(D,A)=H(D)−H(D∣A)$$</p>
<p>特征A的信息增益越大，不确定性减少越多，A的分类能力就越强。</p>
<ul>
<li>信息增益<strong>不适合</strong><code>连续型</code>、<code>取值多</code>的特征</li>
<li>使得所有分支下的样本集合都是纯的，极端情况每一个叶子节点都是一个样本</li>
<li>数据更纯，信息增益更大，选择它作为根节点，结果就是庞大且深度很浅的树</li>
<li></li>
</ul>
<h3 id="146">1.4.6. 信息增益比</h3>
<p>特征A的有n个取值：<br />
$$HA(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}$$</p>
<p>$$信息增益比 = \frac{信息增益}{划分前熵}=\frac{信息增益}{D}$$</p>
<p>关于特征A的熵 ：gR(D,A)=g(D,A)HA(D)=H(D)−H(D∣A)HA(D)解决信息增益<br />
的问题：特征A分的类别越多，D关于A的熵就越大，作为分母，所以信息增益<br />
gR(D,A) 就越小。在信息增益的基础上增加了一个分母惩罚项。信息增益比的<br />
问题：实际上偏好可取类别数目较少的特征。</p>
<h3 id="147">1.4.7. 基尼指数</h3>
<p>CART分类树使用基尼指数来选择最优特征。 基尼指数也是度量不确定性。 熵偏向<code>离散属性</code>， 基尼指数偏向<code>连续属性</code>。<br />
1. <strong>概率分布基尼指数</strong><br />
分类问题中，有$K$类。 样本属于第$k$类的概率为$p_k$<br />
$$ Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$$</p>
<ol>
<li><strong>样本集合基尼指数</strong><br />
集合D，有$K$类，$D_k$ 是第k类的样本子集。则D的基尼指数:</li>
</ol>
<p>$$ Gini(D)=1-\sum_{k=1}^K(\frac{|D_k|}{|D|})^2$$</p>
<ol>
<li><strong>特征A条件基尼指数</strong><br />
特征A取值为某一可能取值为a。 根据A是否取值为a把D划分为$D_1$和$D_2$两个集合。<br />
在特征A的条件下，D的基尼指数如下：<br />
$$ Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$$</li>
</ol>
<p>$Gini(D,A)$是集合D根据特征A分割后，集合D的不确定性。</p>
<h2 id="15">1.5. 分类树</h2>
<div class="hlcode"><pre><span class="n">XGBRegressor</span><span class="p">(</span><span class="n">base_score</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">booster</span><span class="o">=</span><span class="s">&#39;gbtree&#39;</span><span class="p">,</span> <span class="n">colsample_bylevel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">colsample_bynode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">colsample_bytree</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
             <span class="n">importance_type</span><span class="o">=</span><span class="s">&#39;gain&#39;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">max_delta_step</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
             <span class="n">max_depth</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">min_child_weight</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">missing</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
             <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">nthread</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">objective</span><span class="o">=</span><span class="s">&#39;reg:linear&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
             <span class="n">reg_alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reg_lambda</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">scale_pos_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">silent</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">subsample</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">verbosity</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>


<h2 id="16">1.6. 回归树</h2>
<div class="hlcode"><pre><span class="k">class</span> <span class="nc">BaseDecisionTree</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">with_metaclass</span><span class="p">(</span><span class="n">ABCMeta</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">)):</span>
    <span class="sd">&quot;&quot;&quot;Base class for decision trees.</span>

<span class="sd">    Warning: This class should not be used directly.</span>
<span class="sd">    Use derived classes instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">criterion</span><span class="p">,</span>
                 <span class="n">splitter</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="p">,</span>
                 <span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
                 <span class="n">max_features</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="p">,</span>
                 <span class="n">min_impurity_decrease</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="p">,</span>
                 <span class="n">class_weight</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">presort</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">splitter</span> <span class="o">=</span> <span class="n">splitter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span> <span class="o">=</span> <span class="n">min_impurity_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span> <span class="o">=</span> <span class="n">min_impurity_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="o">=</span> <span class="n">class_weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">presort</span> <span class="o">=</span> <span class="n">presort</span>

<span class="k">class</span> <span class="nc">BinaryTree</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">node</span> <span class="o">=</span> <span class="n">node</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">left</span> <span class="o">=</span> <span class="n">left</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">right</span> <span class="o">=</span> <span class="n">right</span>


<span class="k">class</span> <span class="nc">CART</span><span class="p">(</span><span class="n">BinaryTree</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">DataSetMat</span><span class="p">):</span>
        <span class="c"># DataSetMat =pd.DataFrame([[]]) DataSetMat.shape=(counts,features)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">node</span> <span class="o">=</span> <span class="n">DataSetMat</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">select_feature_index</span> <span class="o">=</span> <span class="bp">None</span>        <span class="c"># 选择划分的特征名称</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">select_feature_split_value</span> <span class="o">=</span> <span class="bp">None</span>  <span class="c"># 该特征</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">samples_counts</span> <span class="o">=</span> <span class="mi">0</span>                 <span class="c"># 节点样本数量</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">impurity</span> <span class="o">=</span> <span class="mi">0</span>                       <span class="c"># 纯度</span>

    <span class="k">def</span> <span class="nf">calcErr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">error</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">error</span>

    <span class="k">def</span> <span class="nf">chooseBestFeatVal2Split</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c"># 如果所有目标变量相等，停止</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">samples_counts</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>

        <span class="n">best_feature_name</span> <span class="o">=</span> <span class="s">&quot;&quot;</span>
        <span class="n">best_value</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">lowestErr</span> <span class="o">=</span> <span class="mi">100000</span>
        <span class="n">totalErr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">impurity</span>
        <span class="n">features_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">feature_name</span> <span class="ow">in</span> <span class="n">features_names</span><span class="p">:</span>
            <span class="n">all_values_for_spec_feature</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="p">[</span><span class="n">feature_name</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
            <span class="n">uniq_values_for_spec_feature</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">all_values_for_spec_feature</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">uniq_values_for_spec_feature</span><span class="p">:</span>
                <span class="n">leftChild</span><span class="p">,</span> <span class="n">rightChild</span> <span class="o">=</span> <span class="n">splitByFeatVal</span><span class="p">(</span>
                    <span class="n">feature_name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
                <span class="n">curErr</span> <span class="o">=</span> <span class="n">calcErr</span><span class="p">(</span><span class="n">leftChild</span><span class="p">)</span> <span class="o">+</span> <span class="n">calcErr</span><span class="p">(</span><span class="n">rightChild</span><span class="p">)</span>
                <span class="k">if</span><span class="p">(</span><span class="n">curErr</span> <span class="o">&lt;</span> <span class="n">lowestErr</span><span class="p">):</span>
                    <span class="n">best_feature_name</span> <span class="o">=</span> <span class="n">feature_name</span>
                    <span class="n">bestValue</span> <span class="o">=</span> <span class="n">value</span>
                    <span class="n">lowestErr</span> <span class="o">=</span> <span class="n">curErr</span>
        <span class="c"># 如果误差减少很小，停止</span>
        <span class="k">if</span><span class="p">(</span><span class="n">totalErr</span> <span class="o">-</span> <span class="n">lowestErr</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>
        <span class="k">return</span> <span class="n">best_feature_name</span><span class="p">,</span> <span class="n">best_value</span>

    <span class="k">def</span> <span class="nf">splitByFeatVal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dataSetMat</span><span class="p">):</span>
        <span class="c"># 左子树的值大于根节点的值</span>
        <span class="n">leftChild</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="p">[</span><span class="n">nonzero</span><span class="p">(</span><span class="n">dataSetMat</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">value</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="p">:]</span>
        <span class="c"># 右子树的值小于等于根节点的值</span>
        <span class="n">rightChild</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="p">[</span><span class="n">nonzero</span><span class="p">(</span><span class="n">dataSetMat</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">value</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="p">:]</span>
        <span class="k">return</span> <span class="n">leftChild</span><span class="p">,</span> <span class="n">rightChild</span>
</pre></div>


<h1 id="2">2. 决策树算法</h1>
<div class="hlcode"><pre><span class="k">class</span> <span class="nc">BaseDecisionTree</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">with_metaclass</span><span class="p">(</span><span class="n">ABCMeta</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">)):</span>
    <span class="sd">&quot;&quot;&quot;Base class for decision trees.</span>

<span class="sd">    Warning: This class should not be used directly.</span>
<span class="sd">    Use derived classes instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">criterion</span><span class="p">,</span>
                 <span class="n">splitter</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="p">,</span>
                 <span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
                 <span class="n">max_features</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="p">,</span>
                 <span class="n">min_impurity_decrease</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="p">,</span>
                 <span class="n">class_weight</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">presort</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span> <span class="c"># 纯度</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">splitter</span> <span class="o">=</span> <span class="n">splitter</span> <span class="c"># 制决策树中的随机选项的</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span> <span class="o">=</span> <span class="n">min_impurity_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span> <span class="o">=</span> <span class="n">min_impurity_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="o">=</span> <span class="n">class_weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">presort</span> <span class="o">=</span> <span class="n">presort</span>
</pre></div>


<h2 id="21-id3-">2.1. ID3--最大信息增益</h2>
<p>ID3算法决策树的生成，ID3算法以<strong>信息增益最大</strong>为标准选择特征。<br />
递归构建，不断选择最优特征对训练集进行划分。递归终止条件：当前节点的所有样本，属于同一类别Ck，无需划分。该节点为叶子节点，类标记为Ck当前属性集为空，或所有样本在属性集上取值相同当前节点的样本集合为空，没有样本在集合D中，选择信息增益最大的特征Ag ：增益小于阈值，则不继续向下分裂，到达叶子节点。该节点的标记为该节点所有样本中的majority classCk。 这也是预剪枝增益大于阈值，按照特征Ag的每一个取值Ag=ai把D划分为各个子集Di，去掉特征Ag继续对每个内部节点进行递归划分。</p>
<h2 id="22-c45-">2.2. C4.5--最大信息增益</h2>
<p>C4.5算法是ID3的改进，C4.5以信息增益率最大为标准选择特征。</p>
<p>ID3/C4.5决策树剪枝决策树的生成，会过多地考虑如何提高对训练数据的分类，从而构建出非常复杂的决策树。就容易过拟合。剪枝就是裁掉一些子树和叶节点，并将其根节点或父节点作为叶节点。剪枝分为预剪枝和后剪枝。预剪枝在生成树的时候，设定信息增益的阈值，如果某节点的某特征的信息增益小于该阈值，则不继续分裂，直接设为叶节点。选择该节点的D中类别数量最多的类别 （majority class）作为类别标记。后剪枝树构建好以后，基于整体，极小化损失函数，自下而上地进行剪枝。</p>
<p>树T的参数表示叶节点的个数|T|叶节点t叶节点t上有Nt个样本有K类叶节点t上的经验熵Ht(T)α≥0 为惩罚系数叶节点t上的经验熵Ht(T)=−K∑k=1 NtkNtlogNtkNt模型对训练数据的拟合程度C(T) ，所有叶节点的经验熵和：C(T)=|T|∑t=1 NtHt(T)最终损失函数 = 拟合程度 + 惩罚因子：Cα(T)=C(T)+α|T|参数α权衡了训练数据的拟合程度和模型复杂度。α大，决策树简单，拟合不好α小，决策树复杂，过拟合剪枝步骤计算每个节点的经验熵递归从树的叶节点向上回缩。叶节点回缩到父节点：整体树：回缩前T1 ，回缩后T2Cα(T2)≤Cα(T1)， 则回缩到父节点， 父节点变成新的叶节点。</p>
<h2 id="23-cart-gini">2.3. CART--最大基尼Gini指数</h2>
<p>CART-Classification and regression tree分类与回归树。</p>
<h1 id="_1">参考资料</h1>
</div>
<div id="renote">
  <HR style=" FILTER: alpha (opacity = 100, finishopacity =0 , style= 3 )" width="80%" color=#987 cb 9 SIZE=3>
  <p>如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多!</p>
  <img src="/Wiki/static/images/pay.jpg" width="25%">
</div>

    </div>
    <div id="footer">
        <span>
            Copyright © 2020 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>

    
</body>
<script>
    function changeImgurl(site_root_url) {
        var images = document.images;
        var site_root = site_root_url;
        for (i = 0, len = images.length; i < len; i++) {
            image = images[i];
            image_src = image.src;
            if (image_src.search("attach") >= 0) {
                re_image_src = image_src.slice(image_src.search("attach"));
                abs_image_src = (site_root.endsWith("/")) ? site_root + re_image_src : site_root + "/" +
                    re_image_src;
                image.src = abs_image_src;
            }
        }
    }
    var site_root_url = "/Wiki";
    changeImgurl(site_root_url);
    let isMathjaxConfig = false; // 防止重复调用Config，造成性能损耗
    const initMathjaxConfig = () => {
        if (!window.MathJax) {
            return;
        }
        window.MathJax.Hub.Config({
            showProcessingMessages: false, //关闭js加载过程信息
            messageStyle: "none", //不显示信息
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
                displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
                skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
            },
            "HTML-CSS": {
                availableFonts: ["STIX", "TeX"], //可选字体
                showMathMenu: false //关闭右击菜单显示
            }
        });
        isMathjaxConfig = true; //
    };
    if (isMathjaxConfig === false) {
        // 如果：没有配置MathJax
        initMathjaxConfig();
    };
</script>

</html>