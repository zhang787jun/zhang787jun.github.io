<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <title>特征工程 - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
</head>

<body>

    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="/Wiki/">Home</a>&nbsp;»&nbsp;<a href="/Wiki/#Data_Science\Algorithm\机器学习基础">Data_Science\Algorithm\机器学习基础</a>&nbsp;»&nbsp;特征工程</div>
</div>
<div class="clearfix"></div>
<div id="title">特征工程</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#1">1. 特征工程</a></li>
<li><a href="#2">2. 数据预处理</a><ul>
<li><a href="#21">2.1. 无量纲化</a><ul>
<li><a href="#211">2.1.1. 标准化 （对列向量处理）</a></li>
<li><a href="#212">2.1.2. 区间缩放（对列向量处理）</a></li>
<li><a href="#213">2.1.3. 归一化（对行向量处理）</a></li>
<li><a href="#_1">分箱</a></li>
</ul>
</li>
<li><a href="#22">2.2. 对定量特征二值化（对列向量处理）</a></li>
<li><a href="#23">2.3. 对定性特征哑编码（对列向量处理）</a></li>
<li><a href="#24-ids">2.4. 对IDs类特征数字化编码（对列向量处理）</a></li>
<li><a href="#25">2.5. 缺失值计算（对列向量处理）</a></li>
<li><a href="#26">2.6. 数据变换</a><ul>
<li><a href="#261">2.6.1. 随机变量分布描述及调整</a><ul>
<li><a href="#2611">2.6.1.1. 偏度</a></li>
<li><a href="#2612">2.6.1.2. 峰度</a></li>
</ul>
</li>
<li><a href="#262">2.6.2. 多项式变换（对行向量处理）</a></li>
<li><a href="#263">2.6.3. 自定义变换</a></li>
<li><a href="#264-box-cox">2.6.4. Box-cox变换</a><ul>
<li><a href="#2641-box-cox">2.6.4.1. 标准 Box-cox 变换</a></li>
<li><a href="#2642-lmbda">2.6.4.2. 参数lmbda 确定</a></li>
<li><a href="#2643-box-cox">2.6.4.3. Box-cox逆变换</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#27">2.7. 异常值与重复数据检测</a><ul>
<li><a href="#271">2.7.1. 异常值检验的目的</a></li>
<li><a href="#272">2.7.2. 异常值检验的主要方法</a><ul>
<li><a href="#2721-robustscaler">2.7.2.1. RobustScaler</a></li>
<li><a href="#2722-oneclasssvm">2.7.2.2. OneClassSVM</a></li>
<li><a href="#2723-isolation-forest">2.7.2.3. Isolation Forest</a></li>
<li><a href="#2724-lof">2.7.2.4. 局部异常因子算法 LOF</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#28">2.8. 总结</a></li>
</ul>
</li>
<li><a href="#3">3. 特征选择</a><ul>
<li><a href="#31-filter">3.1. Filter</a><ul>
<li><a href="#311">3.1.1. 依据单特征自身统计性质</a><ul>
<li><a href="#3111">3.1.1.1. 方差选择法</a></li>
</ul>
</li>
<li><a href="#_2">依据多特征之间统计性质</a></li>
<li><a href="#312-">3.1.2. 依据特征-目标相关性</a><ul>
<li><a href="#3121-">3.1.2.1. 卡方检验--分类</a></li>
<li><a href="#3122-f-">3.1.2.2. F检验--分类、回归</a></li>
<li><a href="#3123-">3.1.2.3. 互信息--分类、回归</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#32-wrapper">3.2. Wrapper</a><ul>
<li><a href="#321">3.2.1. 递归特征消除法</a></li>
</ul>
</li>
<li><a href="#33-embedded">3.3. Embedded</a><ul>
<li><a href="#331">3.3.1. 基于惩罚项的特征选择法</a></li>
<li><a href="#332">3.3.2. 基于树模型的特征选择法</a></li>
</ul>
</li>
<li><a href="#34">3.4. 特征重要性</a><ul>
<li><a href="#341-shapxgboost">3.4.1. 利用SHAP解释Xgboost模型</a></li>
</ul>
</li>
<li><a href="#35">3.5. 总结</a></li>
</ul>
</li>
<li><a href="#4">4. 降维</a><ul>
<li><a href="#41-pca">4.1. 主成分分析法（PCA）</a></li>
<li><a href="#42-lda">4.2. 线性判别分析法（LDA）</a></li>
<li><a href="#43">4.3. 降维总结</a></li>
</ul>
</li>
</ul>
</div>
<h1 id="1">1. 特征工程</h1>
<p>首先来说说这几个术语：</p>
<p><code>特征工程</code>：利用数据领域的相关知识来创建能够使机器学习算法达到最佳性能的特征的过程。</p>
<p><code>特征构建</code> ：是原始数据中人工的构建新的特征。<br />
<code>特征提取</code> ：自动地构建新的特征，将原始特征转换为一组具有明显物理意义或者统计意义或核的特征。<br />
<code>特征选择</code> ：从特征集合中挑选一组最具统计意义的特征子集，从而达到降维的效果。</p>
<p>了解这几个术语的意思后，我们来看看他们之间的关系。</p>
<p>在Quora中有人这么说：</p>
<blockquote>
<p>Feature engineering is a super-set of activities which include feature extraction, feature construction and feature selection. Each of the three are important steps and none should be ignored. We could make a generalization of the importance though, from my experience the relative importance of the steps would be feature construction &gt; feature extraction &gt; feature selection.</p>
</blockquote>
<p>用中文来说就是：特征工程是一个超集，它包括<code>特征提取</code>、<code>特征构建</code>和<code>特征选择</code>这三个子模块。在实践当中，每一个子模块都非常重要，忽略不得。根据答主的经验，他将这三个子模块的重要性进行了一个排名，即：</p>
<p>$$特征构建&gt;特征提取&gt;特征选择$$</p>
<p>事实上，真的是这样，如果特征构建做的不好，那么它会直接影响特征提取，进而影响了特征选择，最终影响模型的性能。</p>
<p>有这么一句话在业界广泛流传，<strong>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已</strong>。那特征工程到底是什么呢？顾名思义，其本质是一项工程活动，目的是最大限度地从原始数据中提取特征以供算法和模型使用。</p>
<p>特征工程主要分为三部分：<br />
1. <strong>数据预处理</strong> </p>
<p>对应的sklearn包：<a href="http://scikit-learn.org/stable/modules/preprocessing.html#non-linear-transformation">sklearn-Processing data</a><br />
2. <strong>特征选择</strong> <br />
   对应的sklearn包： <a href="http://scikit-learn.org/stable/modules/feature_selection.html">sklearn-Feature selection</a><br />
3. <strong>降维</strong> <br />
   对应的sklearn包： <a href="http://scikit-learn.org/stable/modules/decomposition.html#decompositions">sklearn-Dimensionality reduction</a></p>
<h1 id="2">2. 数据预处理</h1>
<p>通过特征提取，我们能得到未经处理的特征，这时的特征可能有以下问题：</p>
<ul>
<li>不属于同一量纲：即特征的规格不一样，不能够放在一起比较。<strong>无量纲化</strong>可以解决这一问题。</li>
<li>信息冗余：对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。<strong>二值化</strong>可以解决这一问题。</li>
<li><strong>定性特征</strong>不能直接使用：通常使用哑编码的方式将定性特征转换为定量特征，假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。哑编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用<strong>哑编码</strong>后的特征可达到非线性的效果。</li>
<li>存在缺失值：<strong>填充缺失值</strong>。</li>
<li>信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的<strong>数据变换</strong>，都能达到非线性的效果。</li>
</ul>
<p>我们使用sklearn中的preproccessing库来进行数据预处理。</p>
<h3 id="21">2.1. 无量纲化</h3>
<p>无量纲化使不同规格的数据转换到同一规格</p>
<h4 id="211">2.1.1. 标准化 （对列向量处理）</h4>
<p>标准化也叫<code>Z-score standardization</code>，是将服从正态分布的特征值转换成标准正态分布的过程。<br />
标准化需要计算特征的均值和标准差，公式表达为：<br />
$$x=\frac{x-X}{S}$$</p>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="c">#标准化，返回值为标准化后的数据</span>
<span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>


<h4 id="212">2.1.2. 区间缩放（对列向量处理）</h4>
<p>区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放，公式表达为：<br />
$$x^{'}=\frac{x-Min}{Max-Min}$$<br />
使用preproccessing库的<code>MinMaxScaler</code>类对数据进行区间缩放的代码如下：</p>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="c">#区间缩放，返回值为缩放到[0, 1]区间的数据</span>
<span class="n">iris</span><span class="o">=</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">MinMaxScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>


<p>在什么时候使用标准化比较好，什么时候区间缩放比较好呢？<br />
1. 在后续的分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA、LDA这些需要用到协方差分析进行降维的时候，同时数据分布可以近似为正太分布，标准化方法(Z-score standardization)表现更好。</p>
<ol>
<li>在不涉及距离度量、协方差计算、数据不符合正太分布的时候，可以使用区间缩放法或其他归一化方法。比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0,255]的范围。</li>
</ol>
<h4 id="213">2.1.3. 归一化（对行向量处理）</h4>
<p>归一化目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为l2的归一化公式如下：</p>
<p>$$x^{'}=\frac{x}{\sqrt{\sum_j^m·x_{j}^2}}$$</p>
<p>使用preproccessing库的Normalizer类对数据进行归一化的代码如下：</p>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">Normalizer</span>
<span class="c"># 归一化，返回值为归一化后的数据</span>
<span class="n">Normalizer</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>


<h4 id="_1">分箱</h4>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">KBinsDiscretizer</span>

<span class="n">KBinsDiscretizer</span><span class="p">(</span><span class="n">n_bins</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">encode</span><span class="o">=</span><span class="s">&#39;onehot&#39;</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s">&#39;quantile&#39;</span><span class="p">)</span>
</pre></div>


<h3 id="22">2.2. 对定量特征二值化（对列向量处理）</h3>
<p><strong>定性与定量区别</strong></p>
<blockquote>
<p>定性：博主很胖，博主很瘦<br />
定量：博主有80kg，博主有60kg</p>
</blockquote>
<p>一般定性都会有相关的描述词，定量的描述都是可以用数字来量化处理定量特征二值化的核心在于设定一个<strong>阈值</strong>，大于阈值的赋值为1，小于等于阈值的赋值为0，公式表达如下：<br />
$$x=\begin{cases}1,x&gt;threshold\0,x&lt;=threshold  \end{cases}$$<br />
使用preproccessing库的Binarizer类对数据进行二值化的代码如下：</p>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">Binarizer</span>

<span class="c">#二值化，阈值设置为3，返回值为二值化后的数据</span>
<span class="n">Binarizer</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>


<h3 id="23">2.3. 对定性特征哑编码（对列向量处理）</h3>
<p>使用preproccessing库的OneHotEncoder类对数据进行哑编码的代码如下：</p>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="c">#哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据</span>
<span class="n">OneHotEncoder</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>
</pre></div>


<h3 id="24-ids">2.4. 对IDs类特征数字化编码（对列向量处理）</h3>
<div class="hlcode"><pre><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="c">#哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据</span>
<span class="n">cat_id_series</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&quot;User_id&quot;</span><span class="p">])</span>

<span class="n">cat_id_series</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="p">[</span><span class="s">&quot;Tom&quot;</span><span class="p">,</span> <span class="s">&quot;Tony&quot;</span><span class="p">,</span><span class="s">&quot;Tom&quot;</span><span class="p">,</span><span class="o">...</span><span class="p">]</span>

<span class="nb">type</span><span class="p">(</span><span class="n">cat_id_series</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="n">pandas</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">arrays</span><span class="o">.</span><span class="n">categorical</span><span class="o">.</span><span class="n">Categorical</span>

<span class="n">cat_id_series</span><span class="o">.</span><span class="n">codes</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="o">..</span><span class="p">])</span>

<span class="n">cat_id_series</span><span class="o">.</span><span class="n">categories</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="p">[</span><span class="s">&quot;Tom&quot;</span><span class="p">,</span> <span class="s">&quot;Tony&quot;</span><span class="p">,</span><span class="o">...</span><span class="p">]</span>

<span class="err">而</span><span class="n">pd</span><span class="o">.</span><span class="n">Categorical</span><span class="p">()</span><span class="err">独立创建</span><span class="n">categorical</span><span class="err">数据时有两个新的特性，一是其通过参数</span><span class="n">categories</span><span class="err">定义类别时，若原数据中出现了</span><span class="n">categories</span><span class="err">参数中没有的数据，则会自动转换为</span><span class="n">pd</span><span class="o">.</span><span class="n">nan</span><span class="err">：</span>

<span class="n">categorical_</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Categorical</span><span class="p">([</span><span class="s">&#39;A&#39;</span><span class="p">,</span><span class="s">&#39;B&#39;</span><span class="p">,</span><span class="s">&#39;D&#39;</span><span class="p">,</span><span class="s">&#39;C&#39;</span><span class="p">],</span>
<span class="n">categories</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;B&#39;</span><span class="p">,</span><span class="s">&#39;C&#39;</span><span class="p">,</span><span class="s">&#39;D&#39;</span><span class="p">])</span>


<span class="n">categorical_</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="p">[</span><span class="n">NaN</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">C</span><span class="p">]</span>
<span class="n">Categories</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="nb">object</span><span class="p">):</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">D</span><span class="p">]</span>


<span class="n">categorical_</span><span class="o">.</span><span class="n">codes</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int8</span><span class="p">)</span>
</pre></div>


<h3 id="25">2.5. 缺失值计算（对列向量处理）</h3>
<p>使用preproccessing库的Imputer类对数据进行缺失值计算的代码如下：</p>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">vstack</span><span class="p">,</span> <span class="n">array</span><span class="p">,</span> <span class="n">nan</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">Imputer</span>

<span class="c">#缺失值计算，返回值为计算缺失值后的数据</span>
<span class="c">#参数missing_value为缺失值的表示形式，默认为NaN</span>
<span class="c">#参数strategy为缺失值填充方式，默认为mean（均值）</span>
<span class="n">Imputer</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">vstack</span><span class="p">((</span><span class="n">array</span><span class="p">([</span><span class="n">nan</span><span class="p">,</span> <span class="n">nan</span><span class="p">,</span> <span class="n">nan</span><span class="p">,</span> <span class="n">nan</span><span class="p">]),</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">)))</span>
</pre></div>


<h3 id="26">2.6. 数据变换</h3>
<h4 id="261">2.6.1. 随机变量分布描述及调整</h4>
<h5 id="2611">2.6.1.1. 偏度</h5>
<p>偏度（Skewness）是描述数据分布形态的统计量，其描述的是某总体取值分布的对称性，简单来说就是数据的不对称程度。</p>
<div class="hlcode"><pre><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span> 

<span class="n">df</span><span class="o">.</span><span class="n">skew</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span>

<span class="n">V0</span>    <span class="o">-</span><span class="mf">1.275220</span>
<span class="n">V1</span>    <span class="o">-</span><span class="mf">1.637059</span>
<span class="n">V2</span>    <span class="o">-</span><span class="mf">0.300040</span>
<span class="n">V3</span>    <span class="o">-</span><span class="mf">0.352699</span>
<span class="n">V4</span>    <span class="o">-</span><span class="mf">1.023120</span>
</pre></div>


<p>偏度是三阶中心距计算出来的。<br />
（1）Skewness = 0 ，分布形态与正态分布偏度相同。<br />
（2）Skewness &gt; 0 ，正偏差数值较大，为正偏或右偏。长尾巴拖在右边，数据右端有较多的极端值。<br />
（3）Skewness &lt; 0 ，负偏差数值较大，为负偏或左偏。长尾巴拖在左边，数据左端有较多的极端值。<br />
（4）数值的绝对值越大，表明数据分布越不对称，偏斜程度大。<br />
计算公式：<br />
$$Skewness=E[(\frac{x-E(x)}{\sqrt{D(x)}})^3]$$</p>
<p>$Skewness$越大，分布形态偏移程度越大。</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>数学运算</th>
<th>适合于:</th>
<th>不适合于</th>
</tr>
</thead>
<tbody>
<tr>
<td>取对数</td>
<td>ln(x)log10(x)</td>
<td>右偏态数据</td>
<td>零值</td>
</tr>
<tr>
<td>平方根</td>
<td>√x</td>
<td>右偏态数据</td>
<td>负数</td>
</tr>
<tr>
<td>平方</td>
<td>$x^2$</td>
<td>左偏态数据</td>
<td>负数</td>
</tr>
<tr>
<td>立方根</td>
<td 1_3="1/3">$x^</td>
<td>右偏态数据</td>
<td>负数</td>
</tr>
<tr>
<td>取倒数</td>
<td>1/x</td>
<td>使小值变大，大值变小</td>
<td>零值\负数</td>
</tr>
</tbody>
</table>
<h5 id="2612">2.6.1.2. 峰度</h5>
<p>峰度是描述某变量所有取值分布形态陡缓程度的统计量，简单来说就是数据分布顶的尖锐程度。<br />
峰度是四阶标准矩计算出来的。<br />
（1）Kurtosis=0 与正态分布的陡缓程度相同。<br />
（2）Kurtosis&gt;0 比正态分布的高峰更加陡峭——尖顶峰<br />
（3）Kurtosis&lt;0 比正态分布的高峰来得平台——平顶峰<br />
计算公式：</p>
<p>$$Kurtosis=E[ (\frac{x-E(x)}{\sqrt(D(x)})^4]-3$$</p>
<div class="hlcode"><pre><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="n">df</span><span class="o">.</span><span class="n">kurt</span><span class="p">()</span>
</pre></div>


<h4 id="262">2.6.2. 多项式变换（对行向量处理）</h4>
<p>常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。4个特征，度为2的多项式转换公式如下：<br />
<br><img alt="" src="http://images2015.cnblogs.com/blog/927391/201605/927391-20160502134944451-270339895.png" /><br />
<br>使用preproccessing库的PolynomialFeatures类对数据进行多项式转换的代码如下：</p>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>

<span class="c">#多项式转换</span>
<span class="c">#参数degree为度，默认值为2</span>
<span class="n">PolynomialFeatures</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>


<h4 id="263">2.6.3. 自定义变换</h4>
<p>基于单变元函数的数据变换可以使用一个统一的方式完成，使用preproccessing库的FunctionTransformer对数据进行对数函数转换的代码如下：</p>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">log1p</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">FunctionTransformer</span>

<span class="c">#自定义转换函数为对数函数的数据变换</span>
<span class="c">#第一个参数是单变元函数</span>
<span class="n">FunctionTransformer</span><span class="p">(</span><span class="n">log1p</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>


<h4 id="264-box-cox">2.6.4. Box-cox变换</h4>
<h5 id="2641-box-cox">2.6.4.1. 标准 Box-cox 变换</h5>
<p>$$w_t =<br />
\begin{cases}<br />
\frac{x_t^\lambda-1}{\lambda}&amp; \lambda!=0\<br />
\ln{x_t}&amp; \lambda==0<br />
\end{cases}$$</p>
<p><strong>box1p</strong><br />
$$w_t =<br />
\begin{cases}<br />
\frac{(x_t+1)^\lambda-1}{\lambda}&amp; \lambda!=0\<br />
\ln{(x_t+1)}&amp; \lambda==0<br />
\end{cases}$$</p>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">x_norm</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">boxcox</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">lmbda</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>


<h5 id="2642-lmbda">2.6.4.2. 参数<code>lmbda</code> 确定</h5>
<p>参数<code>lmbda</code> 确定 总体上依据参数估计的方法进行，<br />
1. 最大释然估计(log-likelihood function,llf)<br />
Box-cox 变换 中参数<code>lmbda</code> $\lambda$的确定方法<br />
$$llf=(\lambda-1)\sum_i(\log{(x_i)})-\frac{N}{2}log(\frac{\sum_i(y_i-y)^2}{N})$$</p>
<p>其中Y是X的Box-cox 变换后的序列</p>
<div class="hlcode"><pre><span class="n">x_norm</span><span class="p">,</span> <span class="n">maxlog</span><span class="p">,</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">boxcox</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">lmbda</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="c"># alpha: 执行区间因子0.90，90%</span>
<span class="c"># lmbda:  Box-cox</span>
<span class="c"># maxlog: log似然函数的最大值 </span>
<span class="n">maxlog</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="mf">0.69</span>

<span class="n">x_norm</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">boxcox</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">lmbda</span><span class="o">=</span><span class="n">maxlog</span><span class="p">)</span>
</pre></div>


<p><img alt="" src="../../../../attach/images/2019-10-21-09-55-57.png" /></p>
<p>依据上图 确定 maxlog=4.08</p>
<ol>
<li>依据最大皮尔森相关系数估计</li>
</ol>
<p>如果X符合正态分布，则y = boxcox(x,lmbda)，求得到最大化np.corr(y,x)时候的lmbda</p>
<div class="hlcode"><pre><span class="n">maxlog</span><span class="o">=</span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">boxcox_normmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">method</span><span class="o">=</span><span class="s">&quot;pearsonr&quot;</span><span class="p">)</span>
</pre></div>


<h5 id="2643-box-cox">2.6.4.3. Box-cox逆变换</h5>
<p>$$x_t =<br />
\begin{cases}<br />
e^{w_t}&amp; \lambda==0\<br />
(\lambda w_t +1)^{\frac{1}{\lambda}} &amp; \lambda!=0<br />
\end{cases}$$</p>
<div class="hlcode"><pre><span class="n">scipy</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">inv_boxcox</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">lmbda</span><span class="p">)</span>
</pre></div>


<p>Box-cox 逆变换  处理数据偏执 <br />
$$x_t =<br />
\begin{cases}<br />
e^{w_t}<em>[1+\frac{\sigma^2_h}{2}]&amp; \lambda==0\<br />
(\lambda w_t +1)^{\frac{1}{\lambda}} </em>[1+\frac{\sigma^2_h(1-\lambda)}{2(\lambda w_t+1)^2}]&amp; \lambda!=0<br />
\end{cases}$$</p>
<div class="hlcode"><pre><span class="c">##没有现成的，只能自己写 </span>

<span class="c">#Function</span>
<span class="k">def</span> <span class="nf">invboxcox</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">ld</span><span class="p">):</span>
   <span class="k">if</span> <span class="n">ld</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">return</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
   <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">ld</span><span class="o">*</span><span class="n">y</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">ld</span><span class="p">))</span>

<span class="c"># Test the code</span>
<span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">]</span>
<span class="n">ld</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">boxcox</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">ld</span><span class="p">)</span>
<span class="k">print</span> <span class="n">invboxcox</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">ld</span><span class="p">)</span>
</pre></div>


<p><img alt="" src="../../../../attach/images/2019-10-19-13-19-26.png" /></p>
<h3 id="27">2.7. 异常值与重复数据检测</h3>
<h4 id="271">2.7.1. 异常值检验的目的</h4>
<p>找出数据集中和大多数数据不同的数据</p>
<h4 id="272">2.7.2. 异常值检验的主要方法</h4>
<p>异常值检验主要方法有3种：</p>
<table>
<thead>
<tr>
<th align="right">类型</th>
<th align="right">详解</th>
<th align="right">示例</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">基于统计学的方法</td>
<td align="right">这种方法一般会构建一个概率分布模型，并计算对象符合该模型的概率，把具有低概率的对象视为异常点</td>
<td align="right">特征工程中的RobustScaler方法</td>
</tr>
<tr>
<td align="right">基于聚类方法</td>
<td align="right">基于数据特征的分布来做的</td>
<td align="right">BIRCH聚类算法原理、DBSCAN密度聚类</td>
</tr>
<tr>
<td align="right">novelty detection</td>
<td align="right">当训练数据中没有离群点，我们的目标是用训练好的模型去检测另外新发现的样本</td>
<td align="right">OneClassSVM</td>
</tr>
<tr>
<td align="right">outlier detection</td>
<td align="right">当训练数据中包含离群点，模型训练时要匹配训练数据的中心样本，忽视训练样本中的其它异常点</td>
<td align="right">Isolation Forest(低维度)、Local Outlier Factor（中高纬度）</td>
</tr>
</tbody>
</table>
<h5 id="2721-robustscaler">2.7.2.1. RobustScaler</h5>
<p>https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.robust_scale.html</p>
<p>对数据集进行升序排列，取数据集中间 quantile_range 范围的数据</p>
<div class="hlcode"><pre><span class="n">sklearn</span><span class="p">.</span><span class="n">preprocessing</span><span class="p">.</span><span class="n">robust_scale</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">with_centering</span><span class="o">=</span><span class="n">True</span><span class="p">,</span> <span class="n">with_scaling</span><span class="o">=</span><span class="n">True</span><span class="p">,</span> <span class="n">quantile_range</span><span class="o">=</span><span class="p">(</span><span class="mf">25.0</span><span class="p">,</span> <span class="mf">75.0</span><span class="p">),</span> <span class="n">copy</span><span class="o">=</span><span class="n">True</span>
</pre></div>


<h5 id="2722-oneclasssvm">2.7.2.2. OneClassSVM</h5>
<p>http://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html</p>
<ul>
<li>属于支持向量机大家族</li>
<li>无监督学习的方法</li>
</ul>
<p>支持向量机的边界（支持向量） 一边为无穷远，一边为球的外边缘，求最大间隔。间隔外的数据为异常值 ，间隔内的数据为正常值</p>
<p>在sklearn中，我们可以用svm包里面的OneClassSVM来做异常点检测。OneClassSVM也支持核函数，所以普通SVM里面的调参思路在这里也适用。</p>
<h5 id="2723-isolation-forest">2.7.2.3. Isolation Forest</h5>
<ul>
<li>集成学习的思路</li>
<li>随机森林大家族的一员</li>
<li>周志华老师的学生提出</li>
<li>不适用于特别高维的数据</li>
<li>它具有线性时间复杂度</li>
<li>快速分到了叶子节点的数据 判定为 异常值</li>
</ul>
<p><strong>简介：</strong><br />
孤立森林（Isolation Forest）是另外一种高效的异常检测算法，它和随机森林类似，但每次选择划分属性和划分点（值）时都是随机的，而不是根据信息增益或者基尼指数来选择。在建树过程中，如果一些样本很快就到达了叶子节点（即叶子到根的距离d很短），那么就被认为很有可能是异常点。因为那些路径d比较短的样本，都是因为距离主要的样本点分布中心比较远的。也就是说，可以通过计算样本在所有树中的平均路径长度来寻找异常点。</p>
<p>示例：</p>
<div class="hlcode"><pre><span class="n">sklearn</span><span class="o">.</span><span class="n">ensemble</span><span class="o">.</span><span class="n">IsolationForest</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_samples</span><span class="o">=</span><span class="err">’</span><span class="n">auto</span><span class="err">’</span><span class="p">,</span> <span class="n">contamination</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">bootstrap</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>


<p><strong>参数：</strong><br />
- <code>n_estimators</code>: int, optional (default=100)森林中树的颗数<br />
- <code>max_samples</code>: int or float, optional (default=”auto”)对每棵树，样本个数或比例<br />
- <code>contamination</code>: float in (0., 0.5), optional (default=0.1)  这是最关键的参数，用户设置样本中异常点的比例<br />
- <code>max_features</code>: int or float, optional (default=1.0) 对每棵树，特征个数或比例</p>
<p><strong>函数：</strong><br />
fit(X)<br />
    Fit estimator.（无监督）<br />
predict(X)<br />
    返回值：+1 表示正常样本， -1表示异常样本。<br />
decision_function(X)<br />
    返回样本的异常评分。 值越小表示越有可能是异常样本。</p>
<h5 id="2724-lof">2.7.2.4. 局部异常因子算法 LOF</h5>
<ul>
<li>中等高维数据集上执行异常值检测</li>
<li>基于密度的算法</li>
</ul>
<p>算法简介：<br />
https://blog.csdn.net/bbbeoy/article/details/80301211<br />
了解了 LOF 的定义，整个算法也就显而易见了：<br />
1. 对于每个数据点，计算它与其它所有点的<strong>距离</strong>，并按从近到远排序；<br />
2. 对于每个数据点，找到它的 k-nearest-neighbor，计算 LOF 得分。</p>
<ul>
<li>
<p>K-邻近距离（k-distance） <code>k-distance (o)</code><br />
在距离数据点 o 最近的几个点中，第 k 个最近的点跟点 O 之间的距离称为点 p 的 K-邻近距离，记为 <code>k-distance (o)</code></p>
</li>
<li>
<p>可达距离（rechability distance） <code>reach_dist_k</code><br />
可达距离的定义跟K-邻近距离是相关的，给定参数k时， 数据点 p 到 数据点 o 的可达距离 reach-dist（p, o）为数据点 o 的K-邻近距离 和 数据点p与点o之间的直接距离的最大值。</p>
</li>
</ul>
<p>$$reach_dist_k(p,o)=max{k_distance(o),d(p,o)}$$</p>
<ul>
<li>局部可达密度（local rechability density）：<br />
数据点 p 的局部可达密度为它与邻近的数据点的平均可达距离的倒数，即：</li>
</ul>
<p>$$lrd_k(p)=\frac{1}{\frac{\sum_{o\in{N_{k}(p)}}reach_dist_k(p,o)}{|N_k(p)|}}$$</p>
<p>局部异常因子（local outlier factor）：根据局部可达密度的定义，如果一个数据点跟其他点比较疏远的话，那么显然它的局部可达密度就小。但LOF算法衡量一个数据点的异常程度，并不是看它的绝对局部密度，而是看它跟周围邻近的数据点的相对密度。这样做的好处是可以允许数据分布不均匀、密度不同的情况。局部异常因子即是用局部相对密度来定义的。数据点 p 的局部相对密度（局部异常因子）为点p的邻居们的平均局部可达密度跟数据点p的局部可达密度的比值，即：</p>
<p>$$LOF_{k}(p)=\frac{\sum_{o\in{N_k(p)}}\frac{lrd(o)}{lrd(p)}}{|N_{k}(p)|}=\frac{\sum_{o\in{N_{k}(p)}}lrd(o)}{|N_{k}(p)|}/lrd(p)$$</p>
<p>根据局部异常因子的定义，如果数据点 p 的 LOF 得分在1附近，表明数据点p的局部密度跟它的邻居们差不多；如果数据点 p 的 LOF 得分小于1，表明数据点p处在一个相对密集的区域，不像是一个异常点；如果数据点 p 的 LOF 得分远大于1，表明数据点p跟其他点比较疏远，很有可能是一个异常点。下面这个图来自 Wikipedia 的 LOF 词条，展示了一个二维的例子。上面的数字标明了相应点的LOF得分，可以让人对LOF有一个直观的印象。</p>
<p><strong>Local Outlier Factor主要参数和函数介绍</strong></p>
<div class="hlcode"><pre><span class="n">sklearn</span><span class="o">.</span><span class="n">neighbors</span><span class="o">.</span><span class="n">LocalOutlierFactor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="err">’</span><span class="n">auto</span><span class="err">’</span><span class="p">,</span> <span class="n">leaf_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="err">’</span><span class="n">minkowski</span><span class="err">’</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">metric_params</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">contamination</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>


<p>1）主要参数</p>
<ol>
<li>n_neighbors : 设置k，default=20</li>
<li>contamination :设置样本中异常点的比例，default=0.1</li>
</ol>
<p>2）主要属性：</p>
<ol>
<li>negative_outlier_factor_ : numpy array, shape (n_samples,)和LOF相反的值，值越小，越有可能是异常点。（注：上面提到LOF的值越接近1，越可能是正常样本，LOF的值越大于1，则越可能是异常样本）。这里就正好反一下。</li>
</ol>
<p>3）主要函数：<br />
1. fit_predict(X)<br />
 X : array-like, shape (n_samples, n_features)</p>
<h3 id="28">2.8. 总结</h3>
<table>
<thead>
<tr>
<th>类</th>
<th align="center">功能</th>
<th align="right">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>StandardScaler</td>
<td align="center">无量纲化</td>
<td align="right">标准化，基于特征矩阵的列，将特征值转换至服从标准正态分布</td>
</tr>
<tr>
<td>MinMaxScaler</td>
<td align="center">无量纲化</td>
<td align="right">区间缩放，基于最大最小值，将特征值转换到[0, 1]区间上</td>
</tr>
<tr>
<td>Normalizer</td>
<td align="center">归一化</td>
<td align="right">基于特征矩阵的行，将样本向量转换为“单位向量”</td>
</tr>
<tr>
<td>Binarizer</td>
<td align="center">二值化</td>
<td align="right">基于给定阈值，将定量特征按阈值划分</td>
</tr>
<tr>
<td>OneHotEncoder</td>
<td align="center">哑编码</td>
<td align="right">将定性数据编码为定量数据</td>
</tr>
<tr>
<td>Imputer</td>
<td align="center">缺失值计算</td>
<td align="right">计算缺失值，缺失值可填充为均值等</td>
</tr>
<tr>
<td>PolynomialFeatures</td>
<td align="center">多项式数据转换</td>
<td align="right">多项式数据转换</td>
</tr>
<tr>
<td>FunctionTransformer</td>
<td align="center">自定义单元数据转换</td>
<td align="right">使用单变元的函数来转换数据</td>
</tr>
</tbody>
</table>
<h1 id="3">3. 特征选择</h1>
<p>当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：</p>
<ul>
<li>特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。</li>
<li>特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。</li>
</ul>
<p>根据特征选择的形式又可以将特征选择方法分为3种：</p>
<ul>
<li><strong>Filter：过滤法</strong>，不用考虑后续学习器，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。</li>
<li><strong>Wrapper：包装法</strong>，需考虑后续学习器，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。</li>
<li><strong>Embedded：嵌入法</strong>，是Filter与Wrapper方法的结合。先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。</li>
</ul>
<p>我们使用<code>sklearn</code>中的<code>sklearn.feature_selection</code>库来进行特征选择。</p>
<h2 id="31-filter">3.1. Filter</h2>
<h3 id="311">3.1.1. 依据单特征自身统计性质</h3>
<h4 id="3111">3.1.1.1. 方差选择法</h4>
<p>使用方差选择法，先要计算各个特征的方差（自身方差），然后根据阈值，选择方差大于阈值的特征。使用feature_selection库的VarianceThreshold类来选择特征的代码如下：</p>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">VarianceThreshold</span>

<span class="c">#方差选择法，返回值为特征选择后的数据</span>
<span class="c">#参数threshold为方差的阈值</span>
<span class="n">select_array</span><span class="o">=</span><span class="n">VarianceThreshold</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data_array</span><span class="p">)</span>

<span class="c"># 如果需要datafram 的 name</span>
<span class="k">def</span> <span class="nf">variance_threshold_selector</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="n">selector</span> <span class="o">=</span> <span class="n">VarianceThreshold</span><span class="p">(</span><span class="n">threshold</span><span class="p">)</span>
    <span class="n">selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">selector</span><span class="o">.</span><span class="n">get_support</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="bp">True</span><span class="p">)]]</span>
</pre></div>


<p>例如，假设我们有一个特征是布尔值的数据集，我们想要移除那些在整个数据集中特征值为0或者为1的比例超过80%的特征。布尔特征是伯努利（Bernoulli）随机变量，变量的方差为</p>
<p>$${Var}_{[X]} = p(1 - p)$$</p>
<p>因此，我们可以使用阈值 <code>0.8*(1-0.8)</code>进行选择</p>
<h3 id="_2">依据多特征之间统计性质</h3>
<div class="hlcode"><pre><span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="c"># 皮尔森相关系数</span>
<span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>

<span class="c">#绘图</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">())</span>
</pre></div>


<h3 id="312-">3.1.2. 依据特征-目标相关性</h3>
<p>可以使用的工具<br />
- <code>sklearn.feature_selection.SelectKBest(score_func, k)</code> 移除那些除了评分最高的 K 个特征之外的所有特征<br />
- <code>sklearn.feature_selection.SelectPercentile(score_func, percentile)</code> 移除除了用户指定的最高得分百分比percentile之外的所有特征</p>
<p>使用的特征相关性评价指标：<br />
- 对于回归: <code>f_regression</code> （F值） , <code>mutual_info_regression</code>（互信息）<br />
- 对于分类: <code>chi2</code>（卡方检验） , <code>f_classif</code> （F值）, <code>mutual_info_classif</code>（互信息）</p>
<h4 id="3121-">3.1.2.1. 卡方检验--分类</h4>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">chi2</span>

<span class="c"># SelectKBest(score_func, k)</span>
<span class="c">#选择K个最好的特征，返回选择特征后的数据</span>
<span class="n">SelectKBest</span><span class="p">(</span><span class="n">chi2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>


<h4 id="3122-f-">3.1.2.2. F检验--分类、回归</h4>
<p>基于 F-test 的方法计算两个随机变量（feature,target）之间的<strong>线性相关程度</strong><br />
ANOVA-F 值越大，那么特征对因变量y的预测能力就越强，就越重要！</p>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">f_classif</span><span class="p">,</span><span class="n">f_regression</span>

<span class="c"># SelectKBest(score_func, k)</span>
<span class="c">#选择K个最好的特征，返回选择特征后的数据</span>
<span class="n">SelectKBest</span><span class="p">(</span><span class="n">f_classif</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>


<h4 id="3123-">3.1.2.3. 互信息--分类、回归</h4>
<p>mutual information methods（互信息）能够计算任何种类的统计相关性，但是作为非参数的方法，互信息需要更多的样本来进行准确的估计</p>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">mutual_info_classif</span><span class="p">,</span><span class="n">mutual_info_regression</span>

<span class="c"># SelectKBest(score_func, k)</span>
<span class="c">#选择K个最好的特征，返回选择特征后的数据</span>
<span class="n">SelectKBest</span><span class="p">(</span><span class="n">mutual_info_regression</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>


<h2 id="32-wrapper">3.2. Wrapper</h2>
<h4 id="321">3.2.1. 递归特征消除法</h4>
<p>递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。使用feature_selection库的RFE类来选择特征的代码如下：</p>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c">#递归特征消除法，返回特征选择后的数据</span>
<span class="c">#参数estimator为基模型</span>
<span class="c">#参数n_features_to_select为选择的特征个数</span>
<span class="n">RFE</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">(),</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>


<h2 id="33-embedded">3.3. Embedded</h2>
<h4 id="331">3.3.1. 基于惩罚项的特征选择法</h4>
<p>使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。使用feature_selection库的SelectFromModel类结合带L1惩罚项的逻辑回归模型，来选择特征的代码如下：</p>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectFromModel</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c">#带L1惩罚项的逻辑回归作为基模型的特征选择</span>
<span class="n">SelectFromModel</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s">&quot;l1&quot;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>


<h4 id="332">3.3.2. 基于树模型的特征选择法</h4>
<p>树模型中GBDT可用来作为基模型进行特征选择，使用feature_selection库的SelectFromModel类结合GBDT模型，来选择特征的代码如下：</p>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectFromModel</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>

<span class="c">#GBDT作为基模型的特征选择</span>
<span class="n">SelectFromModel</span><span class="p">(</span><span class="n">GradientBoostingClassifier</span><span class="p">())</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>


<h2 id="34">3.4. 特征重要性</h2>
<h3 id="341-shapxgboost">3.4.1. 利用SHAP解释Xgboost模型</h3>
<p>Xgboost相对于线性模型在进行预测时往往有更好的精度，但是同时也失去了线性模型的可解释性。所以Xgboost通常被认为是黑箱模型。</p>
<p>2017年，Lundberg和Lee的论文提出了SHAP值这一广泛适用的方法用来解释各种模型（分类以及回归），其中最大的受益者莫过于之前难以被理解的黑箱模型，如boosting和神经网络模型。</p>
<p>本教程中，我们在真实数据集上进行实操，利用SHAP来解释Xgboost模型。</p>
<p>本教程基于Python 3.6版本、Xgboost 0.82版本以及shap 0.28.5版本。</p>
<p>原创者：东布东 | 修改校对：SofaSofa TeamM |<br />
1. Feature importance</p>
<p>在SHAP被广泛使用之前，我们通常用feature importance或者partial dependence plot来解释xgboost。 feature importance是用来衡量数据集中每个特征的重要性。</p>
<p>简单来说，每个特征对于提升整个模型的预测能力的贡献程度就是特征的重要性。（拓展阅读：随机森林、xgboost中feature importance，Partial Dependence Plot是什么意思？，怎么利用permutation importance来解释xgboost模型）</p>
<p>Feature importance可以直观地反映出特征的重要性，看出哪些特征对最终的模型影响较大。<strong>但是无法判断特征与最终预测结果的关系是如何的。</strong></p>
<p>下面这个例子中，我们用2018年足球球员身价数据（请在SofaSofa数据竞赛页面进行数据下载，下载解压后只需要train.csv这个文件）来具体阐述。</p>
<div class="hlcode"><pre><span class="c"># 加载模块</span>
<span class="kn">import</span> <span class="nn">xgboost</span> <span class="kn">as</span> <span class="nn">xgb</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span><span class="p">;</span> <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s">&#39;seaborn&#39;</span><span class="p">)</span>

<span class="c"># 读取数据，目标变量y是球员的身价（万欧元）</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;train.csv&#39;</span><span class="p">)</span>

<span class="c"># 获得当时球员年龄</span>
<span class="n">today</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="s">&#39;2018-01-01&#39;</span><span class="p">)</span>
<span class="n">data</span><span class="p">[</span><span class="s">&#39;birth_date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">&#39;birth_date&#39;</span><span class="p">])</span>
<span class="n">data</span><span class="p">[</span><span class="s">&#39;age&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">((</span><span class="n">today</span> <span class="o">-</span> <span class="n">data</span><span class="p">[</span><span class="s">&#39;birth_date&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">days</span><span class="p">)</span> <span class="o">/</span> <span class="mf">365.</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c"># 选择特征，这里只是举例，未必是最佳组合</span>
<span class="c"># 特征依次为身高（厘米）、潜力、速度、射门、传球、带球、防守、体格、国际知名度、年龄</span>
<span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;height_cm&#39;</span><span class="p">,</span> <span class="s">&#39;potential&#39;</span><span class="p">,</span> <span class="s">&#39;pac&#39;</span><span class="p">,</span> <span class="s">&#39;sho&#39;</span><span class="p">,</span> <span class="s">&#39;pas&#39;</span><span class="p">,</span> <span class="s">&#39;dri&#39;</span><span class="p">,</span> <span class="s">&#39;def&#39;</span><span class="p">,</span> <span class="s">&#39;phy&#39;</span><span class="p">,</span> <span class="s">&#39;international_reputation&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">]</span>

<span class="c"># 训练xgboost回归模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">cols</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="c"># 获取feature importance</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cols</span><span class="p">)),</span> <span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cols</span><span class="p">)),</span> <span class="n">cols</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=-</span><span class="mi">45</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&#39;Feature importance&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p>上图中，我们可以看出国际知名度、潜力和年龄是影响球员身价最重要的三个因素。但是这些因素和身价是正相关、负相关还是其他更复杂的相关性，我们无法从上图得知。我们也无法解读每个特征对每个个体的预测值的影响。<br />
2. SHAP value</p>
<p>SHAP的名称来源于SHapley Additive exPlanation。</p>
<p>Shapley value起源于合作博弈论。比如说甲乙丙丁四个工人一起打工，甲和乙完成了价值100元的工件，甲、乙、丙完成了价值120元的工件，乙、丙、丁完成了价值150元的工件，甲、丁完成了价值90元的工件，那么该如何公平、合理地分配这四个人的工钱呢？Shapley提出了一个合理的计算方法（有兴趣地可以查看原论文），我们称每个参与者分配到的数额为Shapley value。</p>
<p>SHAP是由Shapley value启发的可加性解释模型。对于每个预测样本，模型都产生一个预测值，SHAP value就是该样本中每个特征所分配到的数值。 假设第ii个样本为xixi，第ii个样本的第jj个特征为xi,jxi,j，模型对第ii个样本的预测值为yiyi，整个模型的基线（通常是所有样本的目标变量的均值）为ybaseybase，那么SHAP value服从以下等式。</p>
<div class="hlcode"><pre><span class="n">yi</span><span class="o">=</span><span class="n">ybase</span><span class="o">+</span><span class="n">f</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="n">f</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">+</span><span class="err">⋯</span><span class="o">+</span><span class="n">f</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span><span class="n">k</span><span class="p">)</span><span class="n">yi</span><span class="o">=</span><span class="n">ybase</span><span class="o">+</span><span class="n">f</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="n">f</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">+</span><span class="err">⋯</span><span class="o">+</span><span class="n">f</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span><span class="n">k</span><span class="p">)</span>
</pre></div>


<p>其中f(xi,1)f(xi,1)为xi,jxi,j的SHAP值。直观上看，f(xi,1)f(xi,1)就是对yiyi的贡献值，当f(xi,1)&gt;0f(xi,1)&gt;0，说明该特征提升了预测值，也正向作用；反之，说明该特征使得预测值降低，有反作用。</p>
<p>很明显可以看出，与上一节中feature importance相比，SHAP value最大的优势是SHAP能对于反映出每一个样本中的特征的影响力，而且还表现出影响的正负性。<br />
3. SHAP的Python实现</p>
<p>Python中SHAP值的计算由shap这个package实现，可以通过pip install shap安装。</p>
<p>下面我们针对第1节中训练出的模型model，计算其SHAP值。</p>
<p>引用package并且获得解释器explainer。</p>
<div class="hlcode"><pre><span class="kn">import</span> <span class="nn">shap</span>
<span class="c"># model是在第1节中训练的模型</span>
<span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>


<p>获取训练集data各个样本各个特征的SHAP值。</p>
<p>因为data中有10441个样本以及10个特征，我们得到的shap_values的维度是10441×1010441×10。</p>
<p>shap_values = explainer.shap_values(data[cols])<br />
print(shap_values.shape)<br />
(10441, 10)</p>
<p>我们也可以获得在第2节中提到的模型的基线ybaseybase。</p>
<p>通过对比发现，我们可以确认基线值就是训练集的目标变量的拟合值的均值。在这里例子中，目标变量是球员的身价（万欧元），也就是球员的平均身价为229万欧元。</p>
<div class="hlcode"><pre><span class="n">y_base</span> <span class="o">=</span> <span class="n">explainer</span><span class="o">.</span><span class="n">expected_value</span>
<span class="k">print</span><span class="p">(</span><span class="n">y_base</span><span class="p">)</span>

<span class="n">data</span><span class="p">[</span><span class="s">&#39;pred&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">cols</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">&#39;pred&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="mf">229.168</span>
<span class="mf">229.168</span>
</pre></div>


<p>3.1 单个样本的SHAP值</p>
<p>我们可以随机检查其中一位球员身价的预测值以及其特征对预测值的影响。</p>
<p>下面的数据框中第一列是特征名称，第二列是特征的数值，第三列是各个特征在该样本中对应的SHAP值。</p>
<div class="hlcode"><pre><span class="c"># 比如我们挑选数据集中的第30位</span>
<span class="n">j</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">player_explainer</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">player_explainer</span><span class="p">[</span><span class="s">&#39;feature&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cols</span>
<span class="n">player_explainer</span><span class="p">[</span><span class="s">&#39;feature_value&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">player_explainer</span><span class="p">[</span><span class="s">&#39;shap_value&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
<span class="n">player_explainer</span>

<span class="n">featurefeature_valueshap_value0height_cm185</span><span class="o">.</span><span class="mf">00.6245741</span><span class="n">potential86</span><span class="o">.</span><span class="mf">01092.6497802</span><span class="n">pac69</span><span class="o">.</span><span class="mi">0</span><span class="o">-</span><span class="mf">2.9642313</span><span class="n">sho55</span><span class="o">.</span><span class="mi">0</span><span class="o">-</span><span class="mf">27.9013924</span><span class="n">pas68</span><span class="o">.</span><span class="mi">0</span><span class="o">-</span><span class="mf">18.3465255</span><span class="n">dri71</span><span class="o">.</span><span class="mf">01.2301496</span><span class="n">def76</span><span class="o">.</span><span class="mf">0110.8400197</span><span class="n">phy84</span><span class="o">.</span><span class="mf">031.1281138</span><span class="n">international_reputation2</span><span class="o">.</span><span class="mf">097.1371239</span><span class="n">age20</span><span class="o">.</span><span class="mi">7</span><span class="o">-</span><span class="mf">180.918320</span>
</pre></div>


<p>我们知道一个样本中各特征SHAP值的和加上基线值应该等于该样本的预测值。</p>
<p>我们可以做如下的验证。</p>
<div class="hlcode"><pre><span class="k">print</span><span class="p">(</span><span class="s">&#39;y_base + sum_of_shap_values: </span><span class="si">%.2f</span><span class="s">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">y_base</span> <span class="o">+</span> <span class="n">player_explainer</span><span class="p">[</span><span class="s">&#39;shap_value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()))</span>
<span class="k">print</span><span class="p">(</span><span class="s">&#39;y_pred: </span><span class="si">%.2f</span><span class="s">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">&#39;pred&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
<span class="n">y_base</span> <span class="o">+</span> <span class="n">sum_of_shap_values</span><span class="p">:</span> <span class="mf">1332.65</span>
<span class="n">y_pred</span><span class="p">:</span> <span class="mf">1332.65</span>
</pre></div>


<p>shap还提供极其强大的数据可视化功能。下图是对上面数据框的可视化。</p>
<p>蓝色表示该特征的贡献是负数，红色则表示该特征的贡献是正数。最长的红色条是潜力值，球员的潜力值很高，而他的身价也因此增加了1092万；最长的蓝色条是年龄，这个球员年龄较小才20岁出头，尚未到职业巅峰，未来也有诸多不确定性，身价也因此降低了180万元。</p>
<div class="hlcode"><pre><span class="n">shap</span><span class="o">.</span><span class="n">initjs</span><span class="p">()</span>
<span class="n">shap</span><span class="o">.</span><span class="n">force_plot</span><span class="p">(</span><span class="n">explainer</span><span class="o">.</span><span class="n">expected_value</span><span class="p">,</span> <span class="n">shap_values</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
</pre></div>


<p>3.2 对特征的总体分析</p>
<p>除了能对单个样本的SHAP值进行可视化之外，还能对特征进行整体的可视化。</p>
<p>下图中每一行代表一个特征，横坐标为SHAP值。一个点代表一个样本，颜色越红说明特征本身数值越大，颜色越蓝说明特征本身数值越小。</p>
<p>我们可以直观地看出潜力potential是一个很重要的特征，而且基本上是与身价成正相关的。年龄age也会明显影响身价，蓝色点主要集中在SHAP小于0的区域，可见年纪小会降低身价估值，另一方面如果年纪很大，也会降低估值，甚至降低得更明显，因为age这一行最左端的点基本上都是红色的。</p>
<p>shap.summary_plot(shap_values, data[cols])</p>
<p>我们也可以把一个特征对目标变量影响程度的绝对值的均值作为这个特征的重要性。</p>
<p>因为SHAP和feature_importance的计算方法不同，所以我们这里也得到了与第1节不同的重要性排序。</p>
<p>shap.summary_plot(shap_values, data[cols], plot_type="bar")</p>
<p>3.3 部分依赖图Partial Dependence Plot</p>
<p>SHAP也提供了部分依赖图的功能，与传统的部分依赖图不同的是，这里纵坐标不是目标变量y的数值而是SHAP值。</p>
<p>比如下图中，年纪大概呈现出金字塔分布，也就是24到31岁这个年纪对球员的身价是拉抬作用，小于24以及大于31岁的球员身价则会被年纪所累。</p>
<p>shap.dependence_plot('age', shap_values, data[cols], interaction_index=None, show=False)</p>
<p>3.4 对多个变量的交互进行分析</p>
<p>我们也可以多个变量的交互作用进行分析。一种方式是采用summary_plot描绘出散点图，如下：</p>
<div class="hlcode"><pre><span class="n">shap_interaction_values</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="n">shap_interaction_values</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">cols</span><span class="p">])</span>
<span class="n">shap</span><span class="o">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">shap_interaction_values</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="n">cols</span><span class="p">],</span> <span class="n">max_display</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>


<p>我们也可以用dependence_plot描绘两个变量交互下变量对目标值的影响。</p>
<div class="hlcode"><pre><span class="n">shap</span><span class="o">.</span><span class="n">dependence_plot</span><span class="p">(</span><span class="s">&#39;potential&#39;</span><span class="p">,</span> <span class="n">shap_values</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="n">cols</span><span class="p">],</span> <span class="n">interaction_index</span><span class="o">=</span><span class="s">&#39;international_reputation&#39;</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>


<h2 id="35">3.5. 总结</h2>
<table>
<thead>
<tr>
<th>类</th>
<th align="center">所属方式</th>
<th align="right">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>VarianceThreshold</td>
<td align="center">Filter</td>
<td align="right">方差选择法</td>
</tr>
<tr>
<td>SelectKBest</td>
<td align="center">Filter</td>
<td align="right">可选关联系数、卡方校验、最大信息系数作为得分计算的方法</td>
</tr>
<tr>
<td>RFE</td>
<td align="center">Wrapper</td>
<td align="right">递归地训练基模型，将权值系数较小的特征从特征集合中消除</td>
</tr>
<tr>
<td>SelectFromModel</td>
<td align="center">Embedded</td>
<td align="right">训练基模型，选择权值系数较高的特征</td>
</tr>
</tbody>
</table>
<h1 id="4">4. 降维</h1>
<p>当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法除了以上提到的基于L1惩罚项的模型以外，另外还有主成分分析法（PCA）和线性判别分析（LDA），线性判别分析本身也是一个分类模型。PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：<strong>PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能</strong>。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。</p>
<h2 id="41-pca">4.1. 主成分分析法（PCA）</h2>
<p>使用decomposition库的PCA类选择特征的代码如下：</p>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="c">#主成分分析法，返回降维后的数据</span>
<span class="c">#参数n_components为主成分数目</span>
<span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>


<h2 id="42-lda">4.2. 线性判别分析法（LDA）</h2>
<p>使用LDA进行降维的代码如下：</p>
<div class="hlcode"><pre><span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span> <span class="k">as</span> <span class="n">LDA</span>

<span class="c">#线性判别分析法，返回降维后的数据</span>
<span class="c">#参数n_components为降维后的维数</span>
<span class="n">LDA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>


<h2 id="43">4.3. 降维总结</h2>
<table>
<thead>
<tr>
<th>库</th>
<th align="center">类</th>
<th align="right">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>decomposition</td>
<td align="center">PCA</td>
<td align="right">主成分分析法</td>
</tr>
<tr>
<td>lda</td>
<td align="center">LDA</td>
<td align="right">线性判别分析法</td>
</tr>
</tbody>
</table>
</div>
<div id="renote">
  <HR style=" FILTER: alpha (opacity = 100, finishopacity =0 , style= 3 )" width="80%" color=#987 cb 9 SIZE=3>
  <p>如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多!</p>
  <img src="/Wiki/static/images/pay.jpg" width="25%">
</div>

    </div>
    <div id="footer">
        <span>
            Copyright © 2020 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>

    
</body>
<script>
    function changeImgurl(site_root_url) {
        var images = document.images;
        var site_root = site_root_url;
        for (i = 0, len = images.length; i < len; i++) {
            image = images[i];
            image_src = image.src;
            if (image_src.search("attach") >= 0) {
                re_image_src = image_src.slice(image_src.search("attach"));
                abs_image_src = (site_root.endsWith("/")) ? site_root + re_image_src : site_root + "/" +
                    re_image_src;
                image.src = abs_image_src;
            }
        }
    }
    var site_root_url = "/Wiki";
    changeImgurl(site_root_url);
    let isMathjaxConfig = false; // 防止重复调用Config，造成性能损耗
    const initMathjaxConfig = () => {
        if (!window.MathJax) {
            return;
        }
        window.MathJax.Hub.Config({
            showProcessingMessages: false, //关闭js加载过程信息
            messageStyle: "none", //不显示信息
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
                displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
                skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
            },
            "HTML-CSS": {
                availableFonts: ["STIX", "TeX"], //可选字体
                showMathMenu: false //关闭右击菜单显示
            }
        });
        isMathjaxConfig = true; //
    };
    if (isMathjaxConfig === false) {
        // 如果：没有配置MathJax
        initMathjaxConfig();
    };
</script>

</html>