<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <title>Tensorflow IO 问题 - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
</head>

<body>

    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="/Wiki/">Home</a>&nbsp;»&nbsp;<a href="/Wiki/#Data_Science\Library_Platform\04-Tensorflow 1.x\模块分解笔记">Data_Science\Library_Platform\04-Tensorflow 1.x\模块分解笔记</a>&nbsp;»&nbsp;Tensorflow IO 问题</div>
</div>
<div class="clearfix"></div>
<div id="title">Tensorflow IO 问题</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#1-tensorflow-io">1. Tensorflow 的 I/O 问题</a><ul>
<li><a href="#11-io">1.1. 数据I/O操作方法</a><ul>
<li><a href="#111-constant">1.1.1. Constant 转换为常量存在图中</a><ul>
<li><a href="#1111">1.1.1.1. 描述</a></li>
<li><a href="#1112">1.1.1.2. 优劣及适用情景</a></li>
</ul>
</li>
<li><a href="#112-feeding-dataprovider">1.1.2. Feeding 自建 DataProvider 喂给图</a><ul>
<li><a href="#1121">1.1.2.1. 描述</a></li>
<li><a href="#1122">1.1.2.2. 优劣及适用情景</a></li>
<li><a href="#1123">1.1.2.3. 表现形式</a></li>
</ul>
</li>
<li><a href="#113-queue">1.1.3. Queue 使用队列多线程输入到图中</a><ul>
<li><a href="#1131">1.1.3.1. 描述</a></li>
<li><a href="#1132">1.1.3.2. 优劣及适用情景</a></li>
<li><a href="#1133">1.1.3.3. 表现形式</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#12-io">1.2. I/O数据的存储格式</a><ul>
<li><a href="#121-protocol-buffer">1.2.1. Protocol Buffer</a></li>
<li><a href="#122-json">1.2.2. json</a></li>
</ul>
</li>
<li><a href="#13-tfrecord-io">1.3. TFRecord 格式的数据及其I/O操作</a><ul>
<li><a href="#131-tfrecord">1.3.1. TFRecord 是什么？</a></li>
<li><a href="#132">1.3.2. 什么情景下使用推荐</a></li>
<li><a href="#133-tfrecord-example">1.3.3. TFRecord 核心--Example</a><ul>
<li><a href="#1331">1.3.3.1. 参数说明</a></li>
<li><a href="#1332">1.3.3.2. 方法说明</a></li>
</ul>
</li>
<li><a href="#134-tfrecord">1.3.4. 存储为 TFrecord格式文件</a></li>
<li><a href="#135-tfdatatfrecordtensorflow">1.3.5. 使用tf.data将TFrecord格式文件输入到tensorflow图中</a><ul>
<li><a href="#1351">1.3.5.1. 导入</a></li>
<li><a href="#1352">1.3.5.2. 序列化样本解析</a><ul>
<li><a href="#13521-parse_function">1.3.5.2.1. parse_function 解析函数</a></li>
</ul>
</li>
<li><a href="#1353">1.3.5.3. 创建迭代器</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#14-parse_example">1.4. parse_example</a></li>
<li><a href="#15-tfgfile">1.5. tf.gfile</a></li>
<li><a href="#16">1.6. 队列进阶</a><ul>
<li><a href="#io">利用多进程 I/O 构建流水线</a></li>
</ul>
</li>
<li><a href="#tensorflowio">多线程认为中的TensorFlow/IO</a></li>
</ul>
</li>
<li><a href="#io1">提升I/O性能的方法1</a><ul>
<li><a href="#_1">硬件上任务分离</a><ul>
<li><a href="#cpu">CPU</a></li>
<li><a href="#gpu">GPU</a><ul>
<li><a href="#_2">网络</a></li>
</ul>
</li>
<li><a href="#_3">总结</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_4">参考文献</a></li>
</ul>
</div>
<h1 id="1-tensorflow-io">1. Tensorflow 的 I/O 问题</h1>
<h2 id="11-io">1.1. 数据I/O操作方法</h2>
<p>Tensorflow把数据输送到计算图的过程中，在内存里的数据结构形式主要有三种：</p>
<h3 id="111-constant">1.1.1. Constant 转换为常量存在图中</h3>
<h4 id="1111">1.1.1.1. 描述</h4>
<p>把Dataset中的数据以const的形式存放在tensorflow的计算图中，主要使用的是tf.constant 函数。    </p>
<h4 id="1112">1.1.1.2. 优劣及适用情景</h4>
<p>这种形式主要适用于小数据集，由于数据固化到了计算图中，所以它的数据读取速度是最快的。</p>
<h3 id="112-feeding-dataprovider">1.1.2. Feeding 自建 DataProvider 喂给图</h3>
<h4 id="1121">1.1.2.1. 描述</h4>
<p>磁盘数据-&gt;独立内存空间1-&gt;计算图</p>
<p>在每次session.run 时，把numpy形式的数据输入到feed_dict参数中。这种方式主要包括两种存在的状态。<br />
1. <strong>全部加载到内存</strong>。自己维护一个DataProvider 类，每次都会获取一部分训练数据。需要注意的是training的时候最好把数据集shuffle，test的时最好不shuffle。</p>
<ol>
<li><strong>分批加载到内存</strong>。训练数据无法一次性全部load到内存中时，分批次load数据。自己维护的DataProvider 类要做好队列的管理。这种形式一个小的trick是每次载入的数据使用多次进行训练，这样可以减少重复地读取数据。</li>
</ol>
<h4 id="1122">1.1.2.2. 优劣及适用情景</h4>
<p><strong>优点：</strong><br />
1. 当数据集较小的时候，数据可以全部载入到内存，这时数据的处理速度就会比较快。<br />
2. 训练和测试几乎可以共用一套代码，仅需要把反馈网络去掉，不使用参数更新即可。<br />
3. 在预测的时候，一般数据不会是文件的形式，所以只能使用feeding方式。</p>
<p><strong>缺点：</strong><br />
1. 自己维护DataProvider类相对麻烦，而且自己写的类原生是不支持多进程的（主要是Python API的问题）<br />
2. 单进程读取数据较慢，很多时间花费在数据读取上，所以训练时间相对较长。</p>
<h4 id="1123">1.1.2.3. 表现形式</h4>
<p>以字典{tensor:value}的形式将数据传入到图中，构成feed_dict 系统。</p>
<div class="hlcode"><pre><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">y</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mf">2.0</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">_y</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span><span class="mf">1.2</span><span class="p">})</span>
</pre></div>


<h3 id="113-queue">1.1.3. Queue 使用队列多线程输入到图中</h3>
<p>磁盘数据-&gt;导入队列的内存空间-&gt;队列的内存空间导出-&gt;计算图</p>
<h4 id="1131">1.1.3.1. 描述</h4>
<p>使用Queue Runner形式从文件中读取。tensorflow以一种黑箱的方式读取数据,必要的时候会启动多进程（需要设置，这些代码是用c++封装的，多线程支持的效果比较好）。</p>
<p>队列运行器QueueRunner需要2个东西：<br />
1. 队列<br />
2. 一些队列操作器((you can have multiple enqueue operations for one queue))</p>
<p>支持：</p>
<ol>
<li>
<p>FIFOQueue: A queue implementation that dequeues elements in first-in first-out order.</p>
</li>
<li>
<p>PaddingFIFOQueue: A FIFOQueue that supports batching variable-sized tensors by padding.</p>
</li>
<li>
<p>PriorityQueue: A queue implementation that dequeues elements in prioritized order.</p>
</li>
<li>
<p>QueueBase: Base class for queue implementations.</p>
</li>
<li>
<p>RandomShuffleQueue: A queue implementation that dequeues elements in a random order.</p>
</li>
</ol>
<h4 id="1132">1.1.3.2. 优劣及适用情景</h4>
<ol>
<li>Python不支持多线程（伪多线程，虽可以启用multi-thread，但所有启用线程的处理能力加起来等于一个核的处理能力）</li>
<li>Python多进程如果是任务可分，不用和主线程交互的情况下是可用的，但对于tensorflow的训练来说，肯定需要使用多（线程、进程）与主（线程、进程）交互。</li>
</ol>
<p>所以在数据I/O阶段使用python 解释器对磁盘文件进行操作会影响性能<br />
<strong>优势</strong></p>
<ol>
<li>Tensorflow使用黑箱的方式为数据的解析提供支持，相比于自己写multi-process然后共享变量来说在代码实现上更加友好。</li>
<li>python 写的 Tensorflow 程序在运行阶段是调用底层c++运行，更具有I/O 效率</li>
</ol>
<p><strong>劣势</strong> </p>
<p>过于繁琐 目前推荐 <code>tf.data</code>  更高级别的封装</p>
<h4 id="1133">1.1.3.3. 表现形式</h4>
<p>一句话概括就是：<br />
1. 构建图阶段: 创建队列Queue 和队列执行器QueueRunner<br />
2. 执行图阶段：tf.train.start_queue_runners() 开始填充队列<br />
3. tf.train.Coordinator() 在线程出错时关闭之。</p>
<div class="hlcode"><pre><span class="c">#-*- coding:utf-8 -*-  </span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>  

<span class="c">#创建的图:</span>

<span class="c"># 构建一个size=3的先入先出队列q</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">FIFOQueue</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s">&quot;float&quot;</span><span class="p">)</span>  
<span class="c"># 入队操作  </span>
<span class="n">enqueue_op</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">enqueue_many</span><span class="p">(([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],))</span> 
<span class="c"># 出列 </span>
<span class="n">x</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">dequeue</span><span class="p">()</span>  
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>  
<span class="n">q_inc</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">enqueue</span><span class="p">([</span><span class="n">y</span><span class="p">])</span>  

<span class="c">#开启一个session,session是会话,会话的潜在含义是状态保持,各种tensor的状态保持  </span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>  
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">enqueue_op</span><span class="p">)</span>  

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>  
                <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">q_inc</span><span class="p">)</span>  

        <span class="n">quelen</span> <span class="o">=</span>  <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>  
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">quelen</span><span class="p">):</span>  
                <span class="k">print</span> <span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">dequeue</span><span class="p">()))</span> 
</pre></div>


<div class="hlcode"><pre><span class="n">def</span> <span class="n">read_example</span><span class="p">(</span><span class="n">filename_queue</span><span class="p">)</span><span class="o">:</span>
    <span class="s">&quot;&quot;&quot;Read one example from filename_queue&quot;&quot;&quot;</span>
    <span class="n">reader</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">TFRecordReader</span><span class="p">()</span>
    <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">reader</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">filename_queue</span><span class="p">)</span>
    <span class="n">features</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">parse_single_example</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">features</span><span class="o">=</span><span class="p">{</span><span class="s">&quot;image&quot;</span><span class="o">:</span> <span class="n">tf</span><span class="p">.</span><span class="n">FixedLenFeature</span><span class="p">([],</span> <span class="n">tf</span><span class="p">.</span><span class="n">string</span><span class="p">),</span>
                                                        <span class="s">&quot;label&quot;</span><span class="o">:</span> <span class="n">tf</span><span class="p">.</span><span class="n">FixedLenFeature</span><span class="p">([],</span> <span class="n">tf</span><span class="p">.</span><span class="n">int64</span><span class="p">)})</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">decode_raw</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s">&quot;image&quot;</span><span class="p">],</span> <span class="n">tf</span><span class="p">.</span><span class="n">uint8</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">cast</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s">&quot;label&quot;</span><span class="p">],</span> <span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&quot;__main__&quot;</span><span class="o">:</span>
    <span class="n">queue</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">string_input_producer</span><span class="p">([</span><span class="s">&quot;TFRecords/train.tfrecords&quot;</span><span class="p">],</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">read_example</span><span class="p">(</span><span class="n">queue</span><span class="p">)</span>

    <span class="n">img_batch</span><span class="p">,</span> <span class="n">label_batch</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">shuffle_batch</span><span class="p">([</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">capacity</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
                                                    <span class="n">min_after_dequeue</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">num_threads</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">()</span> <span class="n">as</span> <span class="n">sess</span><span class="o">:</span>
        <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">local_variables_initializer</span><span class="p">())</span>
        <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

        <span class="n">coord</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">Coordinator</span><span class="p">()</span>
        <span class="n">threads</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">start_queue_runners</span><span class="p">(</span><span class="n">sess</span><span class="o">=</span><span class="n">sess</span><span class="p">,</span> <span class="n">coord</span><span class="o">=</span><span class="n">coord</span><span class="p">)</span>

        <span class="nl">try:</span>
            <span class="k">while</span> <span class="n">not</span> <span class="n">coord</span><span class="p">.</span><span class="n">should_stop</span><span class="p">()</span><span class="o">:</span>
                <span class="err">#</span> <span class="n">Run</span> <span class="n">training</span> <span class="n">steps</span> <span class="n">or</span> <span class="n">whatever</span>
                <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">([</span><span class="n">img_batch</span><span class="p">,</span> <span class="n">label_batch</span><span class="p">])</span>
                <span class="n">print</span><span class="p">(</span><span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">labels</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">except</span> <span class="n">tf</span><span class="p">.</span><span class="n">errors</span><span class="p">.</span><span class="n">OutOfRangeError</span><span class="o">:</span>
            <span class="n">print</span><span class="p">(</span><span class="err">&#39;</span><span class="n">Done</span> <span class="n">training</span> <span class="o">--</span> <span class="n">epoch</span> <span class="n">limit</span> <span class="n">reached</span><span class="err">&#39;</span><span class="p">)</span>

        <span class="n">coord</span><span class="p">.</span><span class="n">request_stop</span><span class="p">()</span>
        <span class="n">coord</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">threads</span><span class="p">)</span>
</pre></div>


<h2 id="12-io">1.2. I/O数据的存储格式</h2>
<h3 id="121-protocol-buffer">1.2.1. Protocol Buffer</h3>
<p>Protocol Buffers 是谷歌开放的一种轻便高效的结构化数据存储格式<br />
特性：<br />
1. 处理结构化数据的工具<br />
2. 二进制流<br />
3. 需要先定义数据格式（schema）,才能还原<br />
4. 序列化数据比xml 小3~10倍，解析快20~100倍<br />
5. 文件格式 <code>.proto</code><br />
6. 每一个message代表了一类结构体和的数据</p>
<h3 id="122-json">1.2.2. json</h3>
<h2 id="13-tfrecord-io">1.3. TFRecord 格式的数据及其I/O操作</h2>
<h4 id="131-tfrecord">1.3.1. TFRecord 是什么？</h4>
<blockquote>
<p>TFRecord 是谷歌推荐的一种二进制<code>文件格式</code>，理论上它可以保存任何格式的信息。</p>
</blockquote>
<p>TFRecord文件将数据存储为 &lt;<strong>二进制</strong>&gt; &lt;<strong>字符串</strong>&gt; &lt;<strong>序列</strong>&gt; .</p>
<p>将数据写入TFRecord文件之<strong>前</strong>需要<strong>指定数据的结构</strong>，<br />
Tensorflow为此提供了两个组件：tf.train.Example 和 tf.train.SequenceExample。必须将每个数据样本存储在其中一个组件中，然后对其进行序列化并使用tf.python_io.TFRecordWriter把它写到磁盘上。</p>
<h4 id="132">1.3.2. 什么情景下使用推荐</h4>
<p>从磁盘提取<strong>大量小文件</strong>会显著影响 I/O 性能。推荐的实现最大 I/O 吞吐量的一种方法是将输入数据预处理为更大（约 100MB）的 TFRecord 文件。<br />
1. 对于较小的数据集 (200MB-1GB)，最好的方法通常是将整个数据集加载到内存中<br />
2. 对于较大的数据集。 将输入数据预处理为更大（约 100MB）的 TFRecord 文件。 </p>
<h4 id="133-tfrecord-example">1.3.3. TFRecord 核心--Example</h4>
<p>TFRecord文件本身是 &lt;<strong>二进制</strong>&gt; &lt;<strong>字符串</strong>&gt; &lt;<strong>序列</strong>&gt;，还原数据原本面目的<strong>核心在于说明数据的结构</strong></p>
<h5 id="1331">1.3.3.1. 参数说明</h5>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Example</span><span class="p">(</span><span class="n">features</span><span class="p">:</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Features</span><span class="o">=</span><span class="n">xxx</span><span class="p">)</span>
 <span class="o">|</span><span class="n">__tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Features</span><span class="p">(</span><span class="n">feature</span><span class="p">:</span><span class="nb">dict</span><span class="o">=</span><span class="n">feature_dict</span><span class="p">)</span>
   <span class="o">|</span><span class="n">__feature_dict</span><span class="o">=</span><span class="p">{</span><span class="n">key</span><span class="p">:</span><span class="n">value</span><span class="p">}</span>
      <span class="o">|</span><span class="n">__</span> <span class="n">value</span><span class="p">:</span><span class="n">string</span>
      <span class="o">|</span><span class="n">__</span> <span class="n">key</span><span class="p">:</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Feature</span><span class="p">(</span><span class="n">Int64List</span><span class="o">=</span><span class="n">int_a</span><span class="p">)</span> 
          <span class="o">|</span><span class="n">__int_a</span><span class="p">:</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">BytesList</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="p">[])</span>
</pre></div>


<p>tf.train.Example 是protocol buffer  协议下的<strong>消息体(message)</strong>,不是普通的python类<br />
一个 Example <strong>消息体</strong>包含了 <strong>一系列的feature 属性</strong></p>
<div class="hlcode"><pre><span class="n">example</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Example</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">_features</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span><span class="nb">type</span><span class="p">(</span><span class="n">_features</span><span class="p">)</span> 
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Features</span>
</pre></div>


<p>tf.train.Features是命名特征的集合。它有一个形参 feature ，类型为一个字典，其中key是特征的名称，value是tf.train.Feature</p>
<div class="hlcode"><pre><span class="n">_features</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Features</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="n">feature_dict</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">type</span><span class="p">(</span><span class="n">feature_dict</span><span class="p">)</span>
<span class="nb">dict</span>
</pre></div>


<div class="hlcode"><pre><span class="n">feature_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span><span class="n">value</span><span class="p">,}</span>
<span class="o">&gt;&gt;&gt;</span><span class="nb">type</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="n">string</span>
<span class="o">&gt;&gt;&gt;</span><span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Feature</span>  <span class="c">## 注意没有s</span>

<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Feature</span><span class="p">(</span><span class="n">Int64List</span><span class="p">:</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">BytesList</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">FloatList</span><span class="p">:</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">FloatList</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">BytesList</span><span class="p">:</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Int64List</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="n">value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Feature</span><span class="p">(</span><span class="n">Int64List</span><span class="o">=</span><span class="n">int_a</span><span class="p">)</span>
<span class="n">value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Feature</span><span class="p">(</span><span class="n">FloatList</span><span class="o">=</span><span class="n">float_b</span><span class="p">)</span>
<span class="n">value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Feature</span><span class="p">(</span><span class="n">BytesList</span><span class="o">=</span><span class="n">bytes_c</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span><span class="nb">type</span><span class="p">(</span><span class="n">int_a</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">BytesList</span>
<span class="o">&gt;&gt;&gt;</span><span class="nb">type</span><span class="p">(</span><span class="n">float_b</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">FloatList</span>
<span class="o">&gt;&gt;&gt;</span><span class="nb">type</span><span class="p">(</span><span class="n">bytes_c</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Int64List</span>

<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">BytesList</span><span class="p">(</span><span class="n">value</span><span class="p">:</span><span class="nb">list</span><span class="o">=</span><span class="p">[])</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">FloatList</span><span class="p">(</span><span class="n">value</span><span class="p">:</span><span class="nb">list</span><span class="o">=</span><span class="p">[])</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Int64List</span><span class="p">(</span><span class="n">value</span><span class="p">:</span><span class="nb">list</span><span class="o">=</span><span class="p">[])</span>

<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">BytesList</span><span class="p">(</span><span class="n">value</span><span class="p">:</span><span class="nb">list</span><span class="o">=</span><span class="p">[</span><span class="n">b</span><span class="s">&quot;hello world&quot;</span><span class="p">])</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">FloatList</span><span class="p">(</span><span class="n">value</span><span class="p">:</span><span class="nb">list</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.2</span><span class="p">])</span>
<span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Int64List</span><span class="p">(</span><span class="n">value</span><span class="p">:</span><span class="nb">list</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
</pre></div>


<h5 id="1332">1.3.3.2. 方法说明</h5>
<p>tf.train.Example和tf.train.SequenceExample提供SerializeToString方法,进行结构化数据首先需要序列化</p>
<div class="hlcode"><pre><span class="n">example</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">()</span>
</pre></div>


<h4 id="134-tfrecord">1.3.4. 存储为 TFrecord格式文件</h4>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">python_io</span><span class="o">.</span><span class="n">TFRecordWriter</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="n">float_list</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Int64List</span><span class="p">(</span><span class="n">value</span><span class="p">:</span><span class="nb">list</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="n">feature_value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Feature</span><span class="p">(</span><span class="n">FloatList</span><span class="o">=</span><span class="n">float_list</span><span class="p">)</span>
<span class="n">feature_dict</span><span class="o">=</span><span class="p">{</span><span class="s">&quot;feature_key&quot;</span><span class="p">,</span><span class="n">feature_value</span><span class="p">}</span>
<span class="n">features</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Features</span><span class="p">(</span><span class="n">feature</span><span class="p">:</span><span class="nb">dict</span><span class="o">=</span><span class="n">feature_dict</span><span class="p">)</span>
<span class="n">example</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Example</span><span class="p">(</span><span class="n">features</span><span class="p">:</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Features</span><span class="o">=</span><span class="n">features</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">python_io</span><span class="o">.</span><span class="n">TFRecordWriter</span><span class="p">(</span><span class="s">&#39;customer_1.tfrecord&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
    <span class="n">writer</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">example</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">())</span>
</pre></div>


<h4 id="135-tfdatatfrecordtensorflow">1.3.5. 使用tf.data将TFrecord格式文件输入到tensorflow图中</h4>
<h5 id="1351">1.3.5.1. 导入</h5>
<p>从多个tfrecord文件中导入数据到Dataset类</p>
<div class="hlcode"><pre><span class="n">filenames</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;test.tfrecord&quot;</span><span class="p">,</span> <span class="s">&quot;test2.tfrecord&quot;</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TFRecordDataset</span><span class="p">(</span><span class="n">filenames</span><span class="p">)</span>
</pre></div>


<h5 id="1352">1.3.5.2. 序列化样本解析</h5>
<p>tfrecord文件是序列化样本，所以我们需要对每一个样本进行解析。 具体实现 通过dataset 的map方法，map 解析函数，如下：</p>
<div class="hlcode"><pre><span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">parse_function</span><span class="p">)</span>
<span class="c"># parse_function 解析函数</span>
</pre></div>


<h6 id="13521-parse_function">1.3.5.2.1. parse_function 解析函数</h6>
<p><strong>1. 输入输出</strong><br />
输入：example_proto  也就是序列化后的样本tf_serialized<br />
输出： parsed_example </p>
<div class="hlcode"><pre><span class="k">def</span> <span class="nf">parse_function</span><span class="p">(</span><span class="n">example_proto</span><span class="p">):</span>
    <span class="c"># 只接受一个形参：example_proto，也就是序列化后的样本tf_serialized</span>
    <span class="n">parsed_example</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">parse_single_example</span><span class="p">(</span><span class="n">example_proto</span><span class="p">,</span> <span class="n">feature_dicts</span><span class="p">)</span>
    <span class="c"># feature_dicts 解析字典---feature的解析方式</span>
    <span class="c"># feature_dicts={key:value}  key为feature名，value为feature的解析方式</span>

    <span class="c"># 返回所有feature</span>
    <span class="k">return</span> <span class="n">parsed_example</span>
</pre></div>


<p><strong>2. feature_dicts 解析字典</strong><br />
feature_dicts={key:value}  key为feature名，value为feature的解析方式</p>
<p><strong>3. feature的解析方式</strong></p>
<p>feature的解析方式:</p>
<table>
<thead>
<tr>
<th align="right">编号</th>
<th align="left">解析方式</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">1</td>
<td align="left">定长特征解析</td>
</tr>
<tr>
<td align="right">2</td>
<td align="left">不定长特征解析</td>
</tr>
</tbody>
</table>
<ol>
<li>定长特征解析：<code>tf.FixedLenFeature(shape, dtype, default_value)</code></li>
</ol>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">FixedLenFeature</span><span class="p">(</span><span class="n">shape</span><span class="p">:</span><span class="nb">tuple</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">default_value</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</pre></div>


<table>
<thead>
<tr>
<th align="right">形参</th>
<th align="left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">shape</td>
<td align="left">1. 可当reshape来用，如vector的shape从(3,)改动成了(1,3)。<br>2. 如果写入的feature使用了.tostring() 其shape就是()</td>
</tr>
<tr>
<td align="right">dtype</td>
<td align="left">必须是tf.float32， tf.int64， tf.string中的一种。</td>
</tr>
<tr>
<td align="right">default_value</td>
<td align="left">feature值缺失时所指定的值。</td>
</tr>
</tbody>
</table>
<p>注：</p>
<ol>
<li>不定长特征解析：<code>tf.VarLenFeature(dtype)</code></li>
</ol>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">VarLenFeature</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="c">#注：可以不明确指定shape，但得到的tensor是SparseTensor。</span>
</pre></div>


<p><strong>4. 特殊 feature的 转变特征</strong></p>
<p>得到的parsed_example也是一个字典，其中每个key是对应feature的名字，value是相应的feature解析值。如果使用了下面两种情况，则还需要对这些值进行转变。其他情况则不用。</p>
<p>string类型：tf.decode_raw(parsed_feature, type) 来解码<br />
注：这里type必须要和当初.tostring()化前的一致。如tensor转变前是tf.uint8，这里就需是tf.uint8；转变前是tf.float32，则tf.float32</p>
<p>VarLen解析：由于得到的是SparseTensor，所以视情况需要用tf.sparse_tensor_to_dense(SparseTensor)来转变成DenseTensor</p>
<div class="hlcode"><pre><span class="c"># 解码字符</span>
<span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;tensor&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">decode_raw</span><span class="p">(</span><span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;tensor&#39;</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="c"># 稀疏表示 转为 密集表示</span>
<span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;matrix&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sparse_tensor_to_dense</span><span class="p">(</span><span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;matrix&#39;</span><span class="p">])</span>
</pre></div>


<p><strong>5. 改变形状</strong><br />
到此为止得到的特征都是向量，需要根据之前存储的shape信息对每个feature进行reshape。</p>
<div class="hlcode"><pre><span class="c"># 转变matrix形状</span>
<span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;matrix&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;matrix&#39;</span><span class="p">],</span> <span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;matrix_shape&#39;</span><span class="p">])</span>

<span class="c"># 转变tensor形状</span>
<span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;tensor&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;tensor&#39;</span><span class="p">],</span> <span class="n">parsed_example</span><span class="p">[</span><span class="s">&#39;tensor_shape&#39;</span><span class="p">])</span>
</pre></div>


<p>2.1.3. 执行解析函数</p>
<p>创建好解析函数后，将创建的parse_function送入dataset.map()得到新的数据集</p>
<p>new_dataset = dataset.map(parse_function)</p>
<h5 id="1353">1.3.5.3. 创建迭代器</h5>
<p>后续与其他dataset 处理一致<br />
iterator = dataset.make_one_shot_iterator()</p>
<h2 id="14-parse_example">1.4. parse_example</h2>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">parse_example</span><span class="p">(</span>
    <span class="n">serialized</span><span class="p">,</span>
    <span class="n">features_spec</span><span class="p">,</span>
    <span class="n">example_names</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="bp">None</span>
<span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">parse_example</span><span class="p">()</span>
<span class="n">tf</span><span class="o">.</span><span class="n">parse_example</span><span class="p">()</span>

<span class="c"># serialized:一个batch的序列化的example </span>
<span class="c"># features_spec:解析example的规则 </span>
<span class="c"># name：当前操作的名字 </span>
<span class="c"># example_name:当前解析example的proto名称</span>
</pre></div>


<p>这里重点要说的是第二个参数，也就是features，features是把serialized的example中按照键值映射到三种tensor: 1,VarlenFeature 2, SparseFeature 3,FixedLenFeature <br />
下面对这三种映射方式做一个简要的叙述：</p>
<p>VarlenFeature</p>
<p>是按照键值把example的value映射到SpareTensor对象，假设我们有如下的serialized数据：</p>
<p>serialized = [<br />
    features<br />
      { feature { key: "ft" value { float_list { value: [1.0, 2.0] } } } },<br />
    features<br />
      { feature []},<br />
    features<br />
      { feature { key: "ft" value { float_list { value: [3.0] } } }<br />
  ]</p>
<p>使用VarLenFeatures方法：</p>
<p>features={<br />
    "ft":tf.VarLenFeature(tf.float32)<br />
}<br />
1<br />
2<br />
3<br />
那么我们将得到的是：</p>
<p>{"ft": SparseTensor(indices=[[0, 0], [0, 1], [2, 0]],<br />
                      values=[1.0, 2.0, 3.0],<br />
                      dense_shape=(3, 2)) }<br />
1<br />
2<br />
3<br />
可见，显示的indices是ft值的索引，values是值，dense_shape是indices的shape</p>
<p>FixedLenFeature</p>
<p>而FixedLenFeature是按照键值对将features映射到大小为[serilized.size(),df.shape]的矩阵，这里的FixLenFeature指的是每个键值对应的feature的size是一样的。对于上面的例子，如果使用：</p>
<p>features: {<br />
      "ft": FixedLenFeature([2], dtype=tf.float32, default_value=-1),<br />
  }</p>
<p>那么我们将得到：</p>
<p>{"ft": [[1.0, 2.0], [3.0, -1.0]]}<br />
1<br />
可见返回的值是一个[2,2]的矩阵，如果返回的长度不足给定的长度，那么将会使用默认值去填充。 </p>
<h2 id="15-tfgfile">1.5. tf.gfile</h2>
<p>tensorflow gfile文件操作详解</p>
<p>翻译过来就是 <strong>无线程锁的文件I/O操作包装器</strong></p>
<ol>
<li>提供了一种类似于python文件 I/O操作的API；</li>
<li>提供了一种操作tensorflow C++文件系统的API；</li>
<li>tensorflow c++文件操作接口支持多个文件系统实现，包括本地文件、谷歌云存储(以gs://开头)和HDFS(以HDFS://开头)，tensorflow封装这些接口到tf.gfile，以便我们可以使用这些接口来存储和加载检查点文件、将tensorboard log信息写到文本里以及访问训练数据(在其他用途里)。但是，如果所有文件都放在本地，那么我们直接使用python提供的常规文本操作接口也是一样效果且毫无问题的</li>
</ol>
<p>https://zhuanlan.zhihu.com/p/31536538</p>
<h2 id="16">1.6. 队列进阶</h2>
<div class="hlcode"><pre><span class="k">def</span> <span class="nf">read_and_decode</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="c">#根据文件名生成一个队列</span>
    <span class="n">filename_queue</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">string_input_producer</span><span class="p">([</span><span class="n">filename</span><span class="p">])</span>

    <span class="n">reader</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">TFRecordReader</span><span class="p">()</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">serialized_example</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">filename_queue</span><span class="p">)</span>   <span class="c">#返回文件名和文件</span>
    <span class="n">features</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">parse_single_example</span><span class="p">(</span><span class="n">serialized_example</span><span class="p">,</span>
                                       <span class="n">features</span><span class="o">=</span><span class="p">{</span>
                                           <span class="s">&#39;label&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">FixedLenFeature</span><span class="p">([],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
                                           <span class="s">&#39;img_raw&#39;</span> <span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">FixedLenFeature</span><span class="p">([],</span> <span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">),</span>
                                       <span class="p">})</span>

    <span class="n">img</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">decode_raw</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s">&#39;img_raw&#39;</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">[</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="mi">255</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s">&#39;label&#39;</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">img</span><span class="p">,</span> <span class="n">label</span>
</pre></div>


<div class="hlcode"><pre><span class="n">coord</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Coordinator</span><span class="p">()</span>
<span class="n">threads</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">start_queue_runners</span><span class="p">(</span><span class="n">sess</span><span class="o">=</span><span class="n">sess</span><span class="p">,</span> <span class="n">coord</span><span class="o">=</span><span class="n">coord</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">coord</span><span class="o">.</span><span class="n">should_stop</span><span class="p">():</span>
        <span class="c"># Run training steps or whatever</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_op</span><span class="p">)</span>
<span class="k">except</span> <span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">OutOfRangeError</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;Done training -- epoch limit reached&#39;</span><span class="p">)</span>
<span class="k">finally</span><span class="p">:</span>
    <span class="c"># When done, ask the threads to stop.</span>
    <span class="n">coord</span><span class="o">.</span><span class="n">request_stop</span><span class="p">()</span>
<span class="c"># Wait for threads to finish.</span>
<span class="n">coord</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">threads</span><span class="p">)</span>
<span class="n">sess</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>


<h5 id="io">利用多进程 I/O 构建流水线</h5>
<p>流水线将训练步骤的预处理和模型执行过程重叠到一起。当加速器正在执行第 N 个训练步时，CPU 正在准备第 N+1 步的数据。这样做不仅可以最大限度地缩短训练的单步用时（而不是总用时），而且可以缩短提取和转换数据所需的时间。</p>
<p><img alt="" src="../../../../../attach/images/2019-10-12-10-07-27.png" /></p>
<h2 id="tensorflowio">多线程认为中的TensorFlow/IO</h2>
<p>总结<br />
这两个类是实现TensorFlow pipeline的基础，能够高效地并行处理数据。个人认为在数据较大时，应该避免使用feed_dict。因为，feed_dict是利用python读取数据，python读取数据的时候，tensorflow无法计算，而且会将数据再次拷贝一份。</p>
<h1 id="io1">提升I/O性能的方法<sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup></h1>
<h2 id="_1">硬件上任务分离</h2>
<h3 id="cpu">CPU</h3>
<h3 id="gpu">GPU</h3>
<h4 id="_2">网络</h4>
<p>while the CPU is preparing the data, the accelerator is sitting idle. Conversely, while the accelerator is training the model, the CPU is sitting idle. </p>
<p>The training step time is thus the sum of both CPU pre-processing time and the accelerator training time.</p>
<ul>
<li>
<p>首字节时间：与本地存储相比，从远程存储读取文件的首字节所用时间可能要多出几个数量级。</p>
</li>
<li>
<p>读取吞吐量：虽然远程存储通常可提供较大的聚合带宽，但读取单个文件可能只能利用此带宽的一小部分。</p>
</li>
</ul>
<h3 id="_3">总结</h3>
<p>下面简要介绍了设计输入流水线的最佳做法：</p>
<ol>
<li>
<p>使用 prefetch 转换可将提供方和使用方的工作重叠。我们特别建议将 prefetch(n)（其中 n 是单步训练使用的元素数/批次数）添加到输入流水线的末尾，以便将在 CPU 上执行的转换与在加速器上执行的训练重叠。</p>
</li>
<li>
<p>通过设置 num_parallel_calls 参数并行处理 map 转换。建议您将其值设为可用 CPU 核的数量 core num。</p>
</li>
<li>
<p>如果您使用 batch 转换将预处理元素组合到一个批次中，建议您使用 map_and_batch 混合转换；特别是在您使用的批次较大时。</p>
</li>
<li>
<p>如果您要处理远程存储的数据并/或需要反序列化，建议您使用 parallel_interleave 转换来重叠从不同文件读取（和反序列化）数据的操作。</p>
</li>
<li>
<p>向量化传递给 map 转换的低开销用户定义函数，以分摊与调度和执行相应函数相关的开销。</p>
</li>
<li>
<p>如果内存可以容纳您的数据，请使用 cache 转换在第一个周期中将数据缓存在内存中，以便后续周期可以避免与读取、解析和转换该数据相关的开销。</p>
</li>
<li>
<p>如果预处理操作会增加数据大小，建议您首先应用 interleave、prefetch 和 shuffle（如果可以）以减少内存使用量。</p>
</li>
<li>
<p>建议您在应用 repeat 转换之前先应用 shuffle 转换，最好使用 shuffle_and_repeat 混合转换。</p>
</li>
</ol>
<h1 id="_4">参考文献</h1>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>Better performance with tf.data<br />
https://tensorflow.google.cn/guide/data_performance&#160;<a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
</div>
<div id="renote">
  <HR style=" FILTER: alpha (opacity = 100, finishopacity =0 , style= 3 )" width="80%" color=#987 cb 9 SIZE=3>
  <p>如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多!</p>
  <img src="/Wiki/static/images/pay.jpg" width="25%">
</div>

    </div>
    <div id="footer">
        <span>
            Copyright © 2020 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>

    
</body>
<script>
    function changeImgurl(site_root_url) {
        var images = document.images;
        var site_root = site_root_url;
        for (i = 0, len = images.length; i < len; i++) {
            image = images[i];
            image_src = image.src;
            if (image_src.search("attach") >= 0) {
                re_image_src = image_src.slice(image_src.search("attach"));
                abs_image_src = (site_root.endsWith("/")) ? site_root + re_image_src : site_root + "/" +
                    re_image_src;
                image.src = abs_image_src;
            }
        }
    }
    var site_root_url = "/Wiki";
    changeImgurl(site_root_url);
    let isMathjaxConfig = false; // 防止重复调用Config，造成性能损耗
    const initMathjaxConfig = () => {
        if (!window.MathJax) {
            return;
        }
        window.MathJax.Hub.Config({
            showProcessingMessages: false, //关闭js加载过程信息
            messageStyle: "none", //不显示信息
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
                displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
                skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
            },
            "HTML-CSS": {
                availableFonts: ["STIX", "TeX"], //可选字体
                showMathMenu: false //关闭右击菜单显示
            }
        });
        isMathjaxConfig = true; //
    };
    if (isMathjaxConfig === false) {
        // 如果：没有配置MathJax
        initMathjaxConfig();
    };
</script>

</html>