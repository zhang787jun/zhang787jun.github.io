<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <title>Keras finetune - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
</head>

<body>

    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="/Wiki/">Home</a>&nbsp;»&nbsp;<a href="/Wiki/#Data_Science">Data_Science</a>&nbsp;»&nbsp;<a href="/Wiki/#-Library_Platform">Library_Platform</a>&nbsp;»&nbsp;<a href="/Wiki/#-07-Keras">07-Keras</a>&nbsp;»&nbsp;<a href="/Wiki/#-模块分解">模块分解</a>&nbsp;»&nbsp;Keras finetune</div>
</div>
<div class="clearfix"></div>
<div id="title">Keras finetune</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#-">-----------------------------------构建模型------------------------------------</a><ul>
<li><a href="#include_top">加载预训练权重，输入大小可以设定，include_top表示是否包括顶层的全连接层</a></li>
<li><a href="#get_layeroutputtensor">添加新层，get_layer方法可以根据层名返回该层，output用于返回该层的输出张量tensor</a></li>
<li><a href="#finetune">finetune模型</a></li>
</ul>
</li>
<li><a href="#-_1">-------------------------------------训练新层-----------------------------------</a><ul>
<li><a href="#_1">冻结原始层位，在编译后生效</a></li>
<li><a href="#_2">设定优化方法，并编译</a></li>
<li><a href="#_3">训练</a></li>
</ul>
</li>
<li><a href="#-_2">--------------------------------------全局微调-----------------------------------</a><ul>
<li><a href="#_4">设定各层是否用于训练，编译后生效</a></li>
</ul>
</li>
<li><a href="#_5">设定优化方法，并编译</a><ul>
<li><a href="#_6">训练</a></li>
</ul>
</li>
</ul>
</div>
<ol>
<li>Keras的applications模块中就提供了带有预训练权重的深度学习模型。</li>
</ol>
<p>该模块会根据参数设置，自动检查本地的~/.keras/models/目录下是否含有所需要的权重，没有时会自动下载，在notebook上下载会占用notebook线程资源，不太方便，因此也可以手动wget。</p>
<p>keras应用模块applications，以MobileNet为例说明：</p>
<h1 id="-">-----------------------------------构建模型------------------------------------</h1>
<p>from keras.applications.mobilenet import MobileNet<br />
from keras.layers import Input, Reshape, AvgPool2D,\<br />
        Dropout, Conv2D, Softmax, BatchNormalization, Activation<br />
from keras import Model</p>
<h2 id="include_top">加载预训练权重，输入大小可以设定，include_top表示是否包括顶层的全连接层</h2>
<p>base_model = MobileNet(input_shape=(128,128,3), include_top=False)</p>
<h2 id="get_layeroutputtensor">添加新层，get_layer方法可以根据层名返回该层，output用于返回该层的输出张量tensor</h2>
<p>with tf.name_scope("output"):<br />
    x = base_model.get_layer("conv_dw_6_relu").output<br />
    x = Conv2D(256,kernel_size=(3,3))(x)<br />
    x = Activation("relu")(x)<br />
    x = AvgPool2D(pool_size=(5,5))(x)<br />
    x = Dropout(rate=0.5)(x)<br />
    x = Conv2D(10,kernel_size=(1,1),)(x)<br />
    predictions = Reshape((10,))(x)</p>
<h2 id="finetune">finetune模型</h2>
<p>model = Model(inputs=base_model.input, outputs=predictions)</p>
<h1 id="-_1">-------------------------------------训练新层-----------------------------------</h1>
<h2 id="_1">冻结原始层位，在编译后生效</h2>
<p>for layer in base_model.layers:<br />
    layer.trainable = False</p>
<h2 id="_2">设定优化方法，并编译</h2>
<p>sgd = keras.optimizers.SGD(lr=0.01)<br />
model.compile(optimizer=sgd,loss="categorical_crossentropy")<br />
‘’‘可选：记录模型训练过程、数据写入tensorboard<br />
callback = [keras.callbacks.ModelCheckpoint(filepath="./vibration_keras/checkpoints",monitor="val_loss"),<br />
           keras.callbacks.TensorBoard(log_dir="./vibration_keras/logs",histogram_freq=1,write_grads=True)]<br />
’‘’</p>
<h2 id="_3">训练</h2>
<p>model.fit(...)</p>
<h1 id="-_2">--------------------------------------全局微调-----------------------------------</h1>
<h2 id="_4">设定各层是否用于训练，编译后生效</h2>
<p>for layer in model.layers[:80]:<br />
    layer.trainable = False<br />
for layer in model.layers[80:]:<br />
    layer.trainable = True</p>
<h1 id="_5">设定优化方法，并编译</h1>
<p>sgd = keras.optimizer.SGD(lr=0.0001)<br />
model.compile(optimizer=sgd, loss="categorical_crossentropy")</p>
<h2 id="_6">训练</h2>
<p>model.fit(...)<br />
获取各层名称的方法：</p>
</div>
<div id="renote">
  <HR style=" FILTER: alpha (opacity = 100, finishopacity =0 , style= 3 )" width="80%" color=#987 cb 9 SIZE=3>
  <p>如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多!</p>
  <img src="/Wiki/static/images/pay.jpg" width="25%">
</div>

    </div>
    <div id="footer">
        <span>
            Copyright © 2021 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>

    
</body>
<script>
    function changeImgurl(site_root_url) {
        var images = document.images;
        var site_root = site_root_url;
        for (i = 0, len = images.length; i < len; i++) {
            image = images[i];
            image_src = image.src;
            if (image_src.search("attach") >= 0) {
                re_image_src = image_src.slice(image_src.search("attach"));
                abs_image_src = (site_root.endsWith("/")) ? site_root + re_image_src : site_root + "/" +
                    re_image_src;
                image.src = abs_image_src;
            }
        }
    }
    var site_root_url = "/Wiki";
    changeImgurl(site_root_url);
    let isMathjaxConfig = false; // 防止重复调用Config，造成性能损耗
    const initMathjaxConfig = () => {
        if (!window.MathJax) {
            return;
        }
        window.MathJax.Hub.Config({
            showProcessingMessages: false, //关闭js加载过程信息
            messageStyle: "none", //不显示信息
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
                displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
                skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
            },
            "HTML-CSS": {
                availableFonts: ["STIX", "TeX"], //可选字体
                showMathMenu: false //关闭右击菜单显示
            }
        });
        isMathjaxConfig = true; //
    };
    if (isMathjaxConfig === false) {
        // 如果：没有配置MathJax
        initMathjaxConfig();
    };
</script>

</html>